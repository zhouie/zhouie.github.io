<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    
<!-- Google Analytics -->
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-117762077-1', 'auto');
ga('send', 'pageview');
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<!-- End Google Analytics -->


    
<!-- Tencent Speed -->
<script>var _speedMark = new Date()</script>
<!-- End Tencent Speed -->
<!-- Tencent Analysis -->
<script async src="//tajs.qq.com/stats?sId=65795960"></script>
<!-- End Tencent Analysis -->


    
<!-- Baidu Tongji -->
<script>var _hmt = _hmt || []</script>
<script async src="//hm.baidu.com/hm.js?691056d9723d4c6dc862bc15de1dc5d4"></script>
<!-- End Baidu Tongji -->


 
<!-- Baidu Push -->
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
<!-- End Baidu Push -->

    <meta charset="utf-8">
    <!-- HTTP 1.1 -->  
    <meta http-equiv="pragma" content="no-cache">  
    <!-- HTTP 1.0 -->  
    <meta http-equiv="cache-control" content="no-cache">
    
    
    <meta name="google-site-verification" content="KZ_uFAkIzJs1qHL3SkF_RWRekw1-1d4uoyWOfOFAogo">
    

    
    
    <title>
        【复习】 人工智能入门复习总结 - 北岛向南的小屋
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3366FF">
    
    
    <meta name="keywords" content="XTU,AI">
    <meta name="description" content="写在前面   本文严禁转载，只限于学习交流。 课件分享在这里了。 还有人工智能标准化白皮书(2018版)也一并分享了。  绪论人工智能的定义与发展定义 一般解释：人工智能就是用 人工的方法在 机器（计算机）上实现的智能，或称 机器智能； 人工智能(学科)：从学科的角度来说，人工智能是一门研究如何 构造智能机器或智能系统，使之能模拟、延伸、扩展人类智能的学科； 人工智能(能力)：从智能能力的角度来">
<meta name="keywords" content="XTU,AI">
<meta property="og:type" content="article">
<meta property="og:title" content="【复习】 人工智能入门复习总结">
<meta property="og:url" content="https://zhouie.cn/posts/201806111/index.html">
<meta property="og:site_name" content="北岛向南的小屋">
<meta property="og:description" content="写在前面   本文严禁转载，只限于学习交流。 课件分享在这里了。 还有人工智能标准化白皮书(2018版)也一并分享了。  绪论人工智能的定义与发展定义 一般解释：人工智能就是用 人工的方法在 机器（计算机）上实现的智能，或称 机器智能； 人工智能(学科)：从学科的角度来说，人工智能是一门研究如何 构造智能机器或智能系统，使之能模拟、延伸、扩展人类智能的学科； 人工智能(能力)：从智能能力的角度来">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c7168e330d87.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c71689ea4bc5.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c71698de93c6.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716ae5a04f8.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716ae5ec730.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716ae5c0158.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716ae5f07cb.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d990f22f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d98ec484.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d99498e9.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d9957c2b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d991549a.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d993cab5.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d9947a4f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/23/5c716d9934fbb.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700f35a3f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700dab32a.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700f34e3a.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71701202615.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700dc0e44.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700ee5dbe.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700f755dd.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700f30b16.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700ee9580.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71700ee7a7f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a4622da.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a47248c.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a481b87.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a3de085.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a41b712.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a45cf31.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7171a455766.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7173002e915.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71730072035.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7173006871b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71730066f42.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717300bd402.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717300d1b44.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717300df62b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71730075bb3.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71730073a87.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717300b3a3f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71746ca090b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71746d0baad.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71746cc7305.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717cc8eb13c.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d04126ba.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d041d888.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d042113c.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d04140b2.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d043ac6a.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d76c6501.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d9ecfa10.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717d9ee3711.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717deb739dc.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717e07cc3e0.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717e07d34eb.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717e5e3cbf8.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717e8a1c5bd.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717e8a29f94.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717eb0efe4c.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717eb0f1776.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717ecf19ec3.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717ecf3a49b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717ecf55dcc.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717ecf6090f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f0d881df.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f557f98f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f5581682.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f55892f1.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f55bdd0b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f55c706d.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717f55ced12.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717fc28c821.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717fc292138.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c717fc2e39b5.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718058bf8bc.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71811825fda.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7181181ddcc.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7181178fc99.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71811791800.jpg)，bias取常数1，设初始值随机取成(0.75,0.5,-0.6">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c71811823ac3.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718118556a9.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197adb27.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197b3bbf.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7182479a4b4.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197b9052.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197bbddb.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197d1e7f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197cc03b.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197cdd40.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197d633d.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197e8aa7.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c718197f149f.jpg">
<meta property="og:image" content="https://i.loli.net/2019/02/24/5c7182479d604.jpg">
<meta property="og:updated_time" content="2019-11-25T01:25:36.883Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【复习】 人工智能入门复习总结">
<meta name="twitter:description" content="写在前面   本文严禁转载，只限于学习交流。 课件分享在这里了。 还有人工智能标准化白皮书(2018版)也一并分享了。  绪论人工智能的定义与发展定义 一般解释：人工智能就是用 人工的方法在 机器（计算机）上实现的智能，或称 机器智能； 人工智能(学科)：从学科的角度来说，人工智能是一门研究如何 构造智能机器或智能系统，使之能模拟、延伸、扩展人类智能的学科； 人工智能(能力)：从智能能力的角度来">
<meta name="twitter:image" content="https://i.loli.net/2019/02/23/5c7168e330d87.jpg">
    
        <link rel="alternate" type="application/atom+xml" title="北岛向南的小屋" href="/atom.xml">
    
    <link rel="shortcut icon" href="/assets/images/favicon.ico">
    <link rel="stylesheet" href="/assets/styles/style.css?v=1.7.1">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

</head>
<body>
    <div id="loading" class="active"></div>
    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(https://i.loli.net/2019/02/22/5c6f885a1f9fa.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">zhouie</h5>
          <a href="mailto:nanzhouie@qq.com" title="nanzhouie@qq.com" class="mail">nanzhouie@qq.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                主页
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                归档
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                标签
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                分类
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/books"  >
                <i class="icon icon-lg icon-book"></i>
                读书
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/movies"  >
                <i class="icon icon-lg icon-film"></i>
                电影
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/musics"  >
                <i class="icon icon-lg icon-music"></i>
                音乐
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/works"  >
                <i class="icon icon-lg icon-laptop"></i>
                作品
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/about"  >
                <i class="icon icon-lg icon-user-secret"></i>
                关于
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>
    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">【复习】 人工智能入门复习总结</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">【复习】 人工智能入门复习总结</h1>
        <h5 class="subtitle">
            
                <time datetime="2018-06-11T13:03:51.000Z" itemprop="datePublished" class="page-time">
  2018-06-11
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/复习/">复习</a></li></ul>

            
        </h5>
    </div>

    

</header>

<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap" id="post-toc">
            <h4>目录</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#绪论"><span class="post-toc-number">1.</span> <span class="post-toc-text">绪论</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#人工智能的定义与发展"><span class="post-toc-number">1.1.</span> <span class="post-toc-text">人工智能的定义与发展</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#定义"><span class="post-toc-number">1.1.1.</span> <span class="post-toc-text">定义</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#起源与发展"><span class="post-toc-number">1.1.2.</span> <span class="post-toc-text">起源与发展</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#各种认知观"><span class="post-toc-number">1.2.</span> <span class="post-toc-text">各种认知观</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#目前人工智能主要有以下三个学派："><span class="post-toc-number">1.2.1.</span> <span class="post-toc-text">目前人工智能主要有以下三个学派：</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#几种学派各自不足之处："><span class="post-toc-number">1.2.2.</span> <span class="post-toc-text">几种学派各自不足之处：</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#研究目标与内容"><span class="post-toc-number">1.3.</span> <span class="post-toc-text">研究目标与内容</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#研究目标"><span class="post-toc-number">1.3.1.</span> <span class="post-toc-text">研究目标</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#研究的基本内容"><span class="post-toc-number">1.3.2.</span> <span class="post-toc-text">研究的基本内容</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#应用领域"><span class="post-toc-number">1.4.</span> <span class="post-toc-text">应用领域</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#课后习题"><span class="post-toc-number">1.5.</span> <span class="post-toc-text">课后习题</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#知识表示与推理"><span class="post-toc-number">2.</span> <span class="post-toc-text">知识表示与推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#知识表示方法"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">知识表示方法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#状态空间法"><span class="post-toc-number">2.1.1.</span> <span class="post-toc-text">状态空间法</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#问题归约法"><span class="post-toc-number">2.1.2.</span> <span class="post-toc-text">问题归约法</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#谓词逻辑法"><span class="post-toc-number">2.1.3.</span> <span class="post-toc-text">谓词逻辑法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#谓词演算"><span class="post-toc-number">2.1.3.1.</span> <span class="post-toc-text">谓词演算</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#置换与合一"><span class="post-toc-number">2.1.3.2.</span> <span class="post-toc-text">置换与合一</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#语义网路法"><span class="post-toc-number">2.1.4.</span> <span class="post-toc-text">语义网路法</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#其他方法"><span class="post-toc-number">2.1.5.</span> <span class="post-toc-text">其他方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#确定性推理"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">确定性推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#推理的基本概念"><span class="post-toc-number">2.2.1.</span> <span class="post-toc-text">推理的基本概念</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#搜索策略"><span class="post-toc-number">2.2.2.</span> <span class="post-toc-text">搜索策略</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#状态空间的搜索策略"><span class="post-toc-number">2.2.2.1.</span> <span class="post-toc-text">状态空间的搜索策略</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#与-或树的搜索策略-ppt-9-10"><span class="post-toc-number">2.2.2.2.</span> <span class="post-toc-text">与/或树的搜索策略(ppt-9~10)</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#自然演绎推理"><span class="post-toc-number">2.2.3.</span> <span class="post-toc-text">自然演绎推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#注意避免以下两类错误："><span class="post-toc-number">2.2.3.1.</span> <span class="post-toc-text">注意避免以下两类错误：</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#自然演绎推理的优缺点"><span class="post-toc-number">2.2.3.2.</span> <span class="post-toc-text">自然演绎推理的优缺点</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#消解演绎推理"><span class="post-toc-number">2.2.4.</span> <span class="post-toc-text">消解演绎推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#子句集及其化简"><span class="post-toc-number">2.2.4.1.</span> <span class="post-toc-text">子句集及其化简</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#鲁滨逊消解原理"><span class="post-toc-number">2.2.4.2.</span> <span class="post-toc-text">鲁滨逊消解原理</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#消解反演推理的消解策略"><span class="post-toc-number">2.2.4.3.</span> <span class="post-toc-text">消解反演推理的消解策略</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#用消解反演求取问题的答案"><span class="post-toc-number">2.2.4.4.</span> <span class="post-toc-text">用消解反演求取问题的答案</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#消解演绎推理的优缺点："><span class="post-toc-number">2.2.4.5.</span> <span class="post-toc-text">消解演绎推理的优缺点：</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#基于规则的演绎推理"><span class="post-toc-number">2.2.5.</span> <span class="post-toc-text">基于规则的演绎推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#规则正向演绎系统"><span class="post-toc-number">2.2.5.1.</span> <span class="post-toc-text">规则正向演绎系统</span></a></li></ol></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#产生式系统"><span class="post-toc-number">2.2.6.</span> <span class="post-toc-text">产生式系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#基本结构"><span class="post-toc-number">2.2.6.1.</span> <span class="post-toc-text">基本结构</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#产生式系统的推理"><span class="post-toc-number">2.2.6.2.</span> <span class="post-toc-text">产生式系统的推理</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#主要优缺点"><span class="post-toc-number">2.2.6.3.</span> <span class="post-toc-text">主要优缺点</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#非经典推理"><span class="post-toc-number">3.</span> <span class="post-toc-text">非经典推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#经典推理和非经典推理"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">经典推理和非经典推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#非经典推理-1"><span class="post-toc-number">3.1.1.</span> <span class="post-toc-text">非经典推理</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#非经典逻辑推理与经典逻辑推理的区别"><span class="post-toc-number">3.1.2.</span> <span class="post-toc-text">非经典逻辑推理与经典逻辑推理的区别</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#不确定性推理"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">不确定性推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#不确定性推理的概念"><span class="post-toc-number">3.2.1.</span> <span class="post-toc-text">不确定性推理的概念</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#为什么要采用不确定性推理"><span class="post-toc-number">3.2.2.</span> <span class="post-toc-text">为什么要采用不确定性推理</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#不确定性推理的基本问题"><span class="post-toc-number">3.2.3.</span> <span class="post-toc-text">不确定性推理的基本问题</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#知识的不确定性的表示"><span class="post-toc-number">3.2.4.</span> <span class="post-toc-text">知识的不确定性的表示</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#证据的非精确性表示"><span class="post-toc-number">3.2.5.</span> <span class="post-toc-text">证据的非精确性表示</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#不确定性的匹配"><span class="post-toc-number">3.2.6.</span> <span class="post-toc-text">不确定性的匹配</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#组合证据不确定性的计算"><span class="post-toc-number">3.2.7.</span> <span class="post-toc-text">组合证据不确定性的计算</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#不确定性的更新"><span class="post-toc-number">3.2.8.</span> <span class="post-toc-text">不确定性的更新</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#不确定性结论的合成"><span class="post-toc-number">3.2.9.</span> <span class="post-toc-text">不确定性结论的合成</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#概率推理"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">概率推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#概率论基础回顾"><span class="post-toc-number">3.3.1.</span> <span class="post-toc-text">概率论基础回顾</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#概率推理方法"><span class="post-toc-number">3.3.2.</span> <span class="post-toc-text">概率推理方法</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#概率推理方法的特点"><span class="post-toc-number">3.3.3.</span> <span class="post-toc-text">概率推理方法的特点</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#贝叶斯推理（主观贝叶斯方法）"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">贝叶斯推理（主观贝叶斯方法）</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#主观贝叶斯方法-ppt-24"><span class="post-toc-number">3.4.1.</span> <span class="post-toc-text">主观贝叶斯方法(ppt-24)</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#可信度方法"><span class="post-toc-number">3.5.</span> <span class="post-toc-text">可信度方法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#证据理论-ppt-25-21"><span class="post-toc-number">3.6.</span> <span class="post-toc-text">证据理论(ppt-25-21)</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#专家系统"><span class="post-toc-number">4.</span> <span class="post-toc-text">专家系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#专家系统概述"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">专家系统概述</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#专家系统的特点"><span class="post-toc-number">4.1.1.</span> <span class="post-toc-text">专家系统的特点</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#专家系统的优点"><span class="post-toc-number">4.1.2.</span> <span class="post-toc-text">专家系统的优点</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#专家系统的结构"><span class="post-toc-number">4.1.3.</span> <span class="post-toc-text">专家系统的结构</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#专家系统的建造步骤"><span class="post-toc-number">4.1.4.</span> <span class="post-toc-text">专家系统的建造步骤</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基于规则的专家系统基于框架的专家系统"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">基于规则的专家系统基于框架的专家系统</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基于框架的专家系统"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">基于框架的专家系统</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基于模型的专家系统"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">基于模型的专家系统</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#基于Web的专家系统"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">基于Web的专家系统</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#新型专家系统"><span class="post-toc-number">4.6.</span> <span class="post-toc-text">新型专家系统</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模糊逻辑系统"><span class="post-toc-number">5.</span> <span class="post-toc-text">模糊逻辑系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊逻辑原理"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">模糊逻辑原理</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊集"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">模糊集</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#集合及其特征函数"><span class="post-toc-number">5.2.1.</span> <span class="post-toc-text">集合及其特征函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊集与隶属函数"><span class="post-toc-number">5.2.2.</span> <span class="post-toc-text">模糊集与隶属函数</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊集的表示方法"><span class="post-toc-number">5.2.3.</span> <span class="post-toc-text">模糊集的表示方法</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊集的运算"><span class="post-toc-number">5.2.4.</span> <span class="post-toc-text">模糊集的运算</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊关系"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">模糊关系</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊关系的定义"><span class="post-toc-number">5.3.1.</span> <span class="post-toc-text">模糊关系的定义</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#模糊关系的合成"><span class="post-toc-number">5.3.1.1.</span> <span class="post-toc-text">模糊关系的合成</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊变换"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">模糊变换</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊变换的概念"><span class="post-toc-number">5.4.1.</span> <span class="post-toc-text">模糊变换的概念</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#用模糊变换可进行模糊推理"><span class="post-toc-number">5.4.2.</span> <span class="post-toc-text">用模糊变换可进行模糊推理</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊推理"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">模糊推理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#简单模糊推理-扎德法"><span class="post-toc-number">5.5.1.</span> <span class="post-toc-text">简单模糊推理(扎德法)</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#模糊计算的流程"><span class="post-toc-number">5.6.</span> <span class="post-toc-text">模糊计算的流程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊计算适用"><span class="post-toc-number">5.6.1.</span> <span class="post-toc-text">模糊计算适用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊计算的过程"><span class="post-toc-number">5.6.2.</span> <span class="post-toc-text">模糊计算的过程</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#模糊计算的一般流程："><span class="post-toc-number">5.6.3.</span> <span class="post-toc-text">模糊计算的一般流程：</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#神经网络系统"><span class="post-toc-number">6.</span> <span class="post-toc-text">神经网络系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#人工神经网络概述"><span class="post-toc-number">6.1.</span> <span class="post-toc-text">人工神经网络概述</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#人工神经网络的特性"><span class="post-toc-number">6.2.</span> <span class="post-toc-text">人工神经网络的特性</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#人工神经元模型"><span class="post-toc-number">6.3.</span> <span class="post-toc-text">人工神经元模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#人工神经网络的分类"><span class="post-toc-number">6.4.</span> <span class="post-toc-text">人工神经网络的分类</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#单层前馈网络"><span class="post-toc-number">6.4.1.</span> <span class="post-toc-text">单层前馈网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#多层前馈网络"><span class="post-toc-number">6.4.2.</span> <span class="post-toc-text">多层前馈网络</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#前馈网络与反馈网络的区别"><span class="post-toc-number">6.4.3.</span> <span class="post-toc-text">前馈网络与反馈网络的区别</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#传统的感知器模型"><span class="post-toc-number">6.5.</span> <span class="post-toc-text">传统的感知器模型</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#单层感知器"><span class="post-toc-number">6.5.1.</span> <span class="post-toc-text">单层感知器</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#多层感知器"><span class="post-toc-number">6.5.2.</span> <span class="post-toc-text">多层感知器</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#BP网络模型"><span class="post-toc-number">6.6.</span> <span class="post-toc-text">BP网络模型</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#BP网络模型-1"><span class="post-toc-number">6.7.</span> <span class="post-toc-text">BP网络模型</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#机器学习系统"><span class="post-toc-number">7.</span> <span class="post-toc-text">机器学习系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#机器学习的基本概念"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">机器学习的基本概念</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#机器学习与深度学习的关系"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">机器学习与深度学习的关系</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#机器学习策略与基本结构"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">机器学习策略与基本结构</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#机器学习的主要策略"><span class="post-toc-number">7.3.1.</span> <span class="post-toc-text">机器学习的主要策略</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#学习系统的基本结构"><span class="post-toc-number">7.3.2.</span> <span class="post-toc-text">学习系统的基本结构</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#归纳学习"><span class="post-toc-number">7.4.</span> <span class="post-toc-text">归纳学习</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#类比学习"><span class="post-toc-number">7.5.</span> <span class="post-toc-text">类比学习</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#解释学习"><span class="post-toc-number">7.6.</span> <span class="post-toc-text">解释学习</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#神经网络学习-ppt-21-32"><span class="post-toc-number">7.7.</span> <span class="post-toc-text">神经网络学习(ppt-21-32)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Hebb学习"><span class="post-toc-number">7.7.1.</span> <span class="post-toc-text">Hebb学习</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#纠错学习"><span class="post-toc-number">7.7.2.</span> <span class="post-toc-text">纠错学习</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#竞争学习及随机学习"><span class="post-toc-number">7.7.3.</span> <span class="post-toc-text">竞争学习及随机学习</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#感知器学习"><span class="post-toc-number">7.7.4.</span> <span class="post-toc-text">感知器学习</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#BP网络学习"><span class="post-toc-number">7.7.5.</span> <span class="post-toc-text">BP网络学习</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#Hopfield网络学习"><span class="post-toc-number">7.7.6.</span> <span class="post-toc-text">Hopfield网络学习</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#其他机器学习方法"><span class="post-toc-number">7.8.</span> <span class="post-toc-text">其他机器学习方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#仿生进化系统-GA"><span class="post-toc-number">8.</span> <span class="post-toc-text">仿生进化系统(GA)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#遗传算法的定义"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">遗传算法的定义</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#遗传算法的基本思想"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">遗传算法的基本思想</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#遗传算法的基本过程"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">遗传算法的基本过程</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#遗传算法的优势"><span class="post-toc-number">8.4.</span> <span class="post-toc-text">遗传算法的优势</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#群智能系统"><span class="post-toc-number">9.</span> <span class="post-toc-text">群智能系统</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#蚁群算法-Ant-Colony-Optimization-ACO"><span class="post-toc-number">9.1.</span> <span class="post-toc-text">蚁群算法(Ant Colony Optimization,ACO)</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ACO基本要素"><span class="post-toc-number">9.1.1.</span> <span class="post-toc-text">ACO基本要素</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#粒子群优化算法"><span class="post-toc-number">9.2.</span> <span class="post-toc-text">粒子群优化算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#粒子群算法的特点"><span class="post-toc-number">9.2.1.</span> <span class="post-toc-text">粒子群算法的特点</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#其他计算智能方法"><span class="post-toc-number">9.3.</span> <span class="post-toc-text">其他计算智能方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#多真体及自然语言理解"><span class="post-toc-number">10.</span> <span class="post-toc-text">多真体及自然语言理解 *</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#Agent的定义和译法"><span class="post-toc-number">10.1.</span> <span class="post-toc-text">Agent的定义和译法</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#真体的要素和特性"><span class="post-toc-number">10.2.</span> <span class="post-toc-text">真体的要素和特性</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#自然语言理解"><span class="post-toc-number">10.3.</span> <span class="post-toc-text">自然语言理解</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#存在问题"><span class="post-toc-number">10.4.</span> <span class="post-toc-text">存在问题</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#如何看待语言理解"><span class="post-toc-number">10.5.</span> <span class="post-toc-text">如何看待语言理解</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#自然语言理解的研究领域和方向"><span class="post-toc-number">10.6.</span> <span class="post-toc-text">自然语言理解的研究领域和方向</span></a></li></ol></li></ol>
        </nav>
    </aside>
    
<article id="post-2018-06-11-【复习】 人工智能入门复习总结"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">【复习】 人工智能入门复习总结</h1>
        <div class="post-meta">
            <time class="post-time" title="2018-06-11 13:03:51" datetime="2018-06-11T13:03:51.000Z"  itemprop="datePublished">2018-06-11</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/复习/">复习</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


            
    <span>
        <i class="icon icon-clock-o"></i>
        257 min.
    </span>
    
        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <blockquote>
<p>写在前面</p>
</blockquote>
<ul>
<li>本文严禁转载，只限于学习<a href="mailto:nanzhouieATqq.com" target="_blank" rel="noopener">交流</a>。</li>
<li>课件分享在<a href="https://download.csdn.net/download/jave_f/10470736" target="_blank" rel="noopener">这里</a>了。</li>
<li>还有<a href="https://download.csdn.net/download/jave_f/10464580" target="_blank" rel="noopener">人工智能标准化白皮书(2018版)</a>也一并分享了。</li>
</ul>
<h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><h3 id="人工智能的定义与发展"><a href="#人工智能的定义与发展" class="headerlink" title="人工智能的定义与发展"></a>人工智能的定义与发展</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ol>
<li><em>一般解释</em>：人工智能就是用 <strong>人工</strong>的方法在 <strong>机器（计算机）</strong>上实现的智能，或称 <strong>机器智能</strong>；</li>
<li><em>人工智能(学科)</em>：从学科的角度来说，人工智能是一门研究如何 <strong>构造智能机器或智能系统</strong>，使之能模拟、延伸、扩展人类智能的学科；</li>
<li><em>人工智能(能力)</em>：从智能能力的角度来说，人工智能是智能机器所执行的通常 <strong>与人类智能有关的智能行为</strong>，如判断、推理、证明、识别、感知、理解、通信、设计、思考、规划、学习和问题求解等思维活动。</li>
</ol>
<p>【<strong>补充</strong>】 2018年1月发布的<a href="https://download.csdn.net/download/jave_f/10464580" target="_blank" rel="noopener">人工智能标准化白皮书</a>上关于“人工智能的概念”有如下一段详尽描述（仅供参考）：</p>
<blockquote>
<p>2.1.2 人工智能的概念</p>
<p>人工智能作为一门前沿交叉学科，其定义一直存有不同的观点：<strong>《人工智能——一种现代方法》</strong>中将已有的一些人工智能定义分为四类：像人一样思考的系统、像人一样行动的系统、理性地思考的系统、理性地行动的系统。<strong>维基百科</strong>上定义“人工智能就是机器展现出的智能”，即只要是某种机器，具有某种或某些“智能”的特征或表现，都应该算作“人工智能”。<strong>大英百科全书</strong>则限定人工智能是数字计算机或者数字计算机控制的机器人在执行智能生物体才有的一些任务上的能力。<strong>百度百科</strong>定义人工智能是“研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学”，将其视为计算机科学的一个分支，指出其研究包括机器人、语言识别、图像识别、自然语言处理和专家系统等。</p>
<p>本白皮书认为，<strong>人工智能是利用数字计算机或者数字计算机控制的机器模拟、延伸和扩展人的智能，感知环境、获取知识并使用知识获得最佳结果的理论、方法、技术及应用系统。</strong></p>
<p>人工智能的定义对人工智能学科的基本思想和内容作出了解释，即围绕智能活动而构造的人工系统。人工智能是知识的工程，是机器模仿人类利用知识完成一定行为的过程。根据人工智能是否能真正实现推理、思考和解决问题，可以将人工智能分为弱人工智能和强人工智能。</p>
<p><strong>弱人工智能</strong>是指不能真正实现推理和解决问题的智能机器，这些机器表面看像是智能的，但是并不真正拥有智能，也不会有自主意识。迄今为止的人工智能系统都还是实现特定功能的专用智能，而不是像人类智能那样能够不断适应复杂的新环境并不断涌现出新的功能，因此都还是弱人工智能。目前的主流研究仍然集中于弱人工智能，并取得了显著进步，如语音识别、图像处理和物体分割、机器翻译等方面取得了重大突破，甚至可以接近或超越人类水平。</p>
<p><strong>强人工智能</strong>是指真正能思维的智能机器，并且认为这样的机器是有知觉的和自我意识的，这类机器可分为类人（机器的思考和推理类似人的思维）与非类人（机器产生了和人完全不一样的知觉和意识，使用和人完全不一样的推理方式）两大类。从一般意义来说，达到人类水平的、能够自适应地应对外界环境挑战的、具有自我意识的人工智能称为“通用人工智能”、“强人工智能”或“类人智能”。强人工智能不仅在哲学上存在巨大争论（涉及到思维与意识等根本问题的讨论），在技术上的研究也具有极大的挑战性。强人工智能当前鲜有进展，美国私营部门的专家及国家科技委员会比较支持的观点是，至少在未来几十年内难以实现。</p>
<p>靠符号主义、连接主义、行为主义和统计主义这四个流派的经典路线就能设计制造出强人工智能吗？其中一个主流看法是：即使有更高性能的计算平台和更大规模的大数据助力，也还只是量变，不是质变，人类对自身智能的认识还处在初级阶段，在人类真正理解智能机理之前，不可能制造出强人工智能。理解大脑产生智能的机理是脑科学的终极性问题，绝大多数脑科学专家都认为这是一个数百年乃至数千年甚至永远都解决不了的问题。</p>
<p>通向强人工智能还有一条“新”路线，这里称为“仿真主义”。这条新路线通过制造先进的大脑探测工具从结构上解析大脑，再利用工程技术手段构造出模仿大脑神经网络基元及结构的仿脑装置，最后通过环境刺激和交互训练仿真大脑实现类人智能，简言之，“先结构，后功能”。虽然这项工程也十分困难，但都是有可能在数十年内解决的工程技术问题，而不像“理解大脑”这个科学问题那样遥不可及。</p>
<p>仿真主义可以说是符号主义、连接主义、行为主义和统计主义之后的第五个流派，和前四个流派有着千丝万缕的联系，也是前四个流派通向强人工智能的关键一环。经典计算机是数理逻辑的开关电路实现，采用冯•诺依曼体系结构，可以作为逻辑推理等专用智能的实现载体。但要靠经典计算机不可能实现强人工智能。要按仿真主义的路线“仿脑”，就必须设计制造全新的软硬件系统，这就是“类脑计算机”，或者更准确地称为“仿脑机”。“仿脑机”是“仿真工程”的标志性成果，也是“仿脑工程”通向强人工智能之路的重要里程碑。</p>
</blockquote>
<h4 id="起源与发展"><a href="#起源与发展" class="headerlink" title="起源与发展"></a>起源与发展</h4><p>人工智能始于20世纪50年代，50多年来，人工智能走过了一条起伏和曲折的发展道路。回顾历史，可以按照不同时期的主要特征，将其产生与发展过程分为5个阶段。</p>
<p>1、孕育期（1956年前）</p>
<p>2、形成期（1956-1970年）</p>
<p>1956年夏，麦卡锡 (J.McCarthy，数学家、计算机专家)、明斯基(M.L.Minsky，哈佛大学数学家、神经学家)、洛切斯特(N.Lochester，IBM公司信息中心负责人)、香农(C.E.Shannon，贝尔实验室信息部数学家和信息学家)<br>邀请莫尔(T.more)、塞缪尔(A.L.Samuel) 、塞尔夫里奇(O.Selfridge)、索罗蒙夫(R.Solomonff)、纽厄尔(A.Newell)、西蒙(H.A.Simon)在 <strong>美国达特茅斯(Dartmouth)大学</strong>举办了长达历时两个月的研讨会。会上，麦卡锡正式使用“<strong>人工智能AI</strong>”这一术语。这是人类历史上首次第一次人工智能研讨会，<strong>标志着人工智能学科的诞生</strong>。</p>
<p>3、暗淡期（1966-1974年）</p>
<p><strong>失败的预言</strong>给人工智能的声誉造成重大伤害<br>60年代初，西蒙预言：10年内计算机将成为世界冠军、将证明一个未发现的数学定理、将能谱写出具有优秀作曲家水平的乐曲、大多数心理学理论将在计算机上形成。</p>
<p><strong>挫折和困境</strong></p>
<ul>
<li>在博弈方面：塞缪尔的下棋程序在与世界冠军对弈时，5局败了4局。</li>
<li>在定理证明方面：发现鲁宾逊归结法的能力有限。当用归结原理证明两个连续函数之和还是连续函数时，推了10万步也没证出结果。</li>
<li>在问题求解方面：对于不良结构，会产生 <strong>组合爆炸</strong>问题。</li>
<li>在机器翻译方面：发现并不那么简单，甚至会闹出笑话。例如，把“心有余而力不足”的英语句子翻译成俄语，再翻译回来时竟变成了“酒是好的，肉变质了”</li>
<li>在神经生理学方面：研究发现人脑有1011-12以上的神经元，在现有技术条件下用机器从结构上模拟人脑是根本不可能的。</li>
<li>在其它方面：人工智能也遇到了不少问题。在英国，剑桥大学的詹姆教授指责“人工智能研究不是骗局，也是庸人自扰” 。从此，形势急转直下，在全世界范围内人工智能研究陷入困境、落入低谷。</li>
</ul>
<p><strong>Minsky的批评</strong><br>1969年 M. Minsky 和 S.Papert 在《感知机》一书中指出了感知机无法解决异或（XOR）问题的缺陷，并表示出对这方面研究的悲观态度，使得神经网络的研究从兴起期进入了停滞期。<br>该批评对人工智能的发展造成了重要的影响</p>
<ul>
<li>在以后的二十年，感知机的研究方向被忽视</li>
<li>基于符号的知识表示成为主流</li>
<li>基于逻辑的推理成为主要研究方向</li>
</ul>
<p><strong>当时的人工智能存在三个方面的局限性</strong></p>
<ul>
<li>知识局限性：早期开发的人工智能程序中包含了太少的主题知识，甚至没有知识，而且只采用简单的句法处理。</li>
<li>解法局限性：求解方法和步骤的局限性使得设计的人工智能程序在实际上无法求得问题的解答，或者只能得到简单问题的解答，而这种简单问题并不需要人工智能的参与。</li>
<li>结构局限性：用于产生智能行为的人工智能系统或程序在一些基本结构上严重局限，如没有考虑不良结构，无法处理组合爆炸问题，因而只能用于解决比较简单的问题，影响到人工智能系统的推广应用。</li>
</ul>
<p>4、知识应用期（ 1970-1988年）</p>
<p>5、集成发展期（1986年至今）</p>
<p>人工智能具体的发展历程图示如下：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c7168e330d87.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>这两年人工智能得到了突飞猛进的发展，实现这种发展的基本条件有三个：</p>
<ul>
<li><strong>大数据的积累</strong></li>
<li><strong>计算能力的提升</strong></li>
<li><strong>理论算法的革新</strong></li>
</ul>
<h3 id="各种认知观"><a href="#各种认知观" class="headerlink" title="各种认知观"></a>各种认知观</h3><h4 id="目前人工智能主要有以下三个学派："><a href="#目前人工智能主要有以下三个学派：" class="headerlink" title="目前人工智能主要有以下三个学派："></a>目前人工智能主要有以下三个学派：</h4><ul>
<li><strong>符号主义</strong>（Symbolicism）: 基于物理符号系统假设和有限合理性原理（逻辑）</li>
</ul>
<p>符号主义观点认为：<strong>智能的基础是知识</strong>，其核心是知识表示和知识推理；知识可用符号表示，也可用符号进行推理，因而可以建立基于知识的人类智能和机器智能的统一的理论体系。</p>
<ul>
<li><strong>连接主义</strong>（Connectionism）: 基于神经网络及其间的连接机制与学习算法（仿生）</li>
</ul>
<p>连接主义观点认为：<strong>思维的基元是神经元</strong>，而不是符号；思维过程是神经元的联结活动过程，而不是符号运算过程；反对符号主义关于物理符号系统的假设。</p>
<ul>
<li><strong>行为主义</strong>（Actionism）: 基于控制论及感知—动作型控制系统（进化）</li>
</ul>
<p>行为主义观点认为：<strong>智能取决于感知和行动</strong>，提出了智能行为的“感知—动作”模型；<strong>智能不需要知识、不需要表示、不需要推理</strong>；人工智能可以像人类智能那样逐步进化。</p>
<p>此外，还有一种由<a href="https://baike.baidu.com/item/%E9%92%9F%E4%B9%89%E4%BF%A1/7371623?fr=aladdin" target="_blank" rel="noopener">钟义信</a>院士提出的一种认知学派：</p>
<ul>
<li><strong>机制主义</strong>（mechanism）：结构（连接）主义、功能（符号）主义、行为主义的和谐统一。</li>
</ul>
<h4 id="几种学派各自不足之处："><a href="#几种学派各自不足之处：" class="headerlink" title="几种学派各自不足之处："></a>几种学派各自不足之处：</h4><ul>
<li><p><strong>符号主义的不足</strong>(功能模拟法/认知学观点)</p>
<ul>
<li>在用符号表示知识的概念时，有效性很大程度上取决于符号表示的正确性和准确性</li>
<li>将知识概念转换成符号时，可能丢失一些重要信息</li>
<li>难于对含噪信息、不确定性信息和不完全性信息进行处理</li>
</ul>
</li>
<li><p><strong>连接主义的不足</strong>(结构模拟法/生物学观点)</p>
<ul>
<li>由于大脑的生理结构和工作机理还远未搞清楚，因而现在只能对人脑的局部进行模拟或进行近似模拟</li>
<li>不适合模拟人的逻辑思维过程</li>
<li>受大规模人工神经网络制造的制约</li>
<li>尚不能满足人脑完全模拟的要求</li>
</ul>
</li>
<li><p><strong>行为主义的不足</strong></p>
<ul>
<li>难以获得高级智能控制行为</li>
</ul>
</li>
</ul>
<h3 id="研究目标与内容"><a href="#研究目标与内容" class="headerlink" title="研究目标与内容"></a>研究目标与内容</h3><h4 id="研究目标"><a href="#研究目标" class="headerlink" title="研究目标"></a>研究目标</h4><p>远期目标：构造出可以实现人类思维活动和智力功能的智能系统。<br>近期目标：使现有的计算机更聪明更有用，使它不仅能够进行一般的数值计算和非数值信息的处理，而且能够运用知识去处理问题，能够模拟人类的智能行为。</p>
<h4 id="研究的基本内容"><a href="#研究的基本内容" class="headerlink" title="研究的基本内容"></a>研究的基本内容</h4><p>1、认知建模<br>认知：可一般地认为是和情感、动机、意志相对应的理智或认识过程，或者是为了一定的目的，在一定的心理结构中进行的信息加工过程。</p>
<p>2、知识表示：基础</p>
<p>3、知识推理：实现问题求解</p>
<p>4、知识应用：目的</p>
<blockquote>
<p>知识表示、知识推理、知识应用是传统人工智能的三大核心研究内容。</p>
</blockquote>
<p>5、<strong>机器感知</strong>：就是要让计算机具有类似于人的感知能力，如视觉、听觉、触觉、嗅觉、味觉……，是机器获取外部信息的基本途径</p>
<ul>
<li>相当于智能系统的输入部分</li>
<li>机器视觉（或叫计算机视觉）：就是给计算机配上能看的视觉器官，如摄像机等，使它可以识别并理解文字、图像、景物等</li>
<li>机器听觉（或叫计算机听觉）：就是给计算配上能听的听觉器官，如话筒等，使计算机能够识别并理解语言、声音等。</li>
<li>模式识别：对客体的识别与分类</li>
<li>自然语言理解：实现人机对话</li>
<li>机器翻译</li>
</ul>
<p>6、<strong>机器思维</strong>：<br>机器思维是让计算机能够对感知到的外界信息和自己产生的内部信息进行思维性加工，包括逻辑思维、形象思维和灵感思维，涉及信息的表示，组织，积累，管理，搜索，推理等过程。</p>
<p>7、<strong>机器学习</strong>：</p>
<ul>
<li>让计算机能够像人那样自动地获取新知识，并在实践中不断地完善自我和增强能力。</li>
<li>是机器获取智能的途径</li>
<li>学习是一个有特定目的的知识获取过程，学习的本质是对信息的理解与应用</li>
</ul>
<p>8、<strong>机器行为</strong>：</p>
<ul>
<li>让计算机能够具有像人那样地行动和表达能力，如走、跑、拿、说、唱、写画等。</li>
<li>相当于智能系统的输出部分</li>
</ul>
<p>9、<strong>智能系统构建</strong></p>
<ul>
<li>无论是人工智能的近期目标还是远期目标，都需要建立智能系统或构造智能机器</li>
<li>需要开展对系统模型、构造技术、构造工具及语言环境等研究</li>
</ul>
<h3 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h3><p>问题求解、机器学习、自然语言理解、专家系统、模式识别、计算机视觉、机器人学、博弈、计算智能、人工生命、自动定理证明、自动程序设计、智能控制、智能检索、智能调度与指挥、智能决策支持系统、人工神经网络、数据挖掘与知识发现……</p>
<h3 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h3><p>1-1 什么是人工智能？是从科学与能力两方面加以说明。<br>1-3 在人工智能的发展过程中，有哪些思想和思潮起到了重要作用？<br>1-5 人工智能有哪些学派？他们的认知观是什么？现在这些学派的关系如何？<br>1-9 人工智能的基本研究方法有哪些类？<br>1-10 人工智能的主要研究和应用领域是什么？其中，哪些是新的研究热点？</p>
<h2 id="知识表示与推理"><a href="#知识表示与推理" class="headerlink" title="知识表示与推理"></a>知识表示与推理</h2><h3 id="知识表示方法"><a href="#知识表示方法" class="headerlink" title="知识表示方法"></a>知识表示方法</h3><blockquote>
<p>知识的一般概念</p>
</blockquote>
<p>知识是人们在改造客观世界的实践中积累起来的 <strong>认识</strong>和 <strong>经验</strong>。<br>其中，<strong>认识</strong>与 <strong>经验</strong>可以这样定义：</p>
<ul>
<li>认识：包括对事物现象、本质、属性、状态、联系等的认识</li>
<li>经验：包括解决问题的微观方法和宏观方法<ul>
<li>微观方法：如步骤、操作、规则、过程、技巧等</li>
<li>宏观方法：如战略、战术、计谋、策略等</li>
</ul>
</li>
</ul>
<blockquote>
<p>人工智能系统中的知识</p>
</blockquote>
<p>一个智能程序高水平的运行需要有关的 <strong>事实知识</strong>、 <strong>规则知识</strong>、 <strong>控制知识</strong>和 <strong>元知识</strong>。</p>
<ul>
<li>事实知识 ：是有关问题环境的一些事物的知识，常以“…是…”的形式出现。<ul>
<li>如事物的分类、属性、事物间关系、科学事实、客观事实等。</li>
<li>事实是静态的为人们共享的可公开获得的公认的知识，在知识库中属低层的知识，如：雪是白色的、鸟有翅膀、张三李四是好朋友、这辆车是张三的……</li>
</ul>
</li>
<li>规则知识 ：是有关问题中与事物的行动、动作相联系的因果关系知识，是动态的，常以“如果…那么…” 形式出现。</li>
<li>控制知识 ：是有关问题的求解步骤、技巧的知识，告诉人们怎么做一件事，也包括当有多个动作同时被激活时应选哪一个动作来执行的知识。控制知识常与程序结合在一起出现，如一个问题求解的算法可以看做是一种知识表示。</li>
<li>元知识 ：是有关知识的知识，是知识库中的高层知识。包括怎样使用规则、解释规则、校验规则、解释程序结构等知识。</li>
</ul>
<p>元知识与控制知识是有重迭的，对一个大的程序来说，以元知识或说元规则形式体现控制知识更为方便，因为元知识存于知识库中，而控制知识常与程序结合在一起出现，从而不容易修改。</p>
<blockquote>
<p>知识表示</p>
</blockquote>
<p>研究用机器表示知识的可行性、有效性的一般方法，是一种数据结构与控制结构的统一体，既考虑知识的存储又考虑知识的使用。</p>
<p>知识表示的要求：</p>
<ul>
<li>表示能力：能否正确、有效地表示问题。包括：表范围的广泛性、领域知识表示的高效性、对非确定性知识表示的支持程度；</li>
<li>可利用性：可利用这些知识进行有效推理。包括：对推理的适应性，对高效算法的支持程度；</li>
<li>可实现性：要便于计算机直接对其进行处理；</li>
<li>可组织性：可以按某种方式把知识组织成某种知识结构；</li>
<li>可维护性：便于对知识的增、删、改等操作；</li>
<li>自然性：符合人们的日常习惯；</li>
<li>可理解性：知识应易读、易懂、易获取等。</li>
</ul>
<h4 id="状态空间法"><a href="#状态空间法" class="headerlink" title="状态空间法"></a>状态空间法</h4><p>状态空间法是一种 <strong>基于解答空间的问题表示和求解方法</strong>，它是以“<strong>状态</strong>（state）”和“<strong>算符</strong>（operator）”为基础的，它是人工智能中最基本的 <strong>形式化方法</strong>。<br>由于状态空间法需要扩展过多的节点，容易出现“<strong>组合爆炸</strong>”，因而 <strong>只适用于表示比较简单的问题</strong>。</p>
<blockquote>
<p>状态空间法的三要素：</p>
</blockquote>
<ol>
<li><strong>状态</strong>（state）：描述某类不同事物间的差别而引入的一组最少变量 <code>q0，q1，…，qn</code>的有序集合，是表示问题解法中每一步问题状况的数据结构。有序集合中每个元素qi（i= 0,1,…,n）为集合的分量，称为状态变量。给定每个分量的一组值就得到一个具体的状态。</li>
<li><strong>算符</strong>（operator）：使问题从一种状态变化为另一种状态的手段称为操作符或算符。</li>
<li><strong>问题的状态空间**</strong>：即解答空间，也就是一个表示该问题全部可能状态及其关系的图。它是以状态和算符为基础来表示和求解问题的。它包含三种说明的集合，即S：所有可能的问题初始状态集合、F：操作符集合、G：目标状态集合。可将状态空间记为三元状态<code>（S，F，G）</code>。</li>
</ol>
<hr>
<p>【例子】<br><strong>猴子和香蕉问题</strong>：在一个房间内有一只猴子、一个箱子和一束香蕉。香蕉挂在天花板下方，但猴子的高度不足以碰到它。那么这只猴子怎样才能摘到香蕉呢?<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c71689ea4bc5.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p><strong>解题过程</strong><br>用一个四元表列<code>(W，x，Y，z)</code>来表示这个问题状态空间<br>其中W：猴子的水平位置；x：当猴子在箱子顶上时取1；否则取0；Y：箱子的水平位置；z：当猴子摘到香蕉时取1；否则取0。<br>则可见初始状态为<code>(a,0,b,0)</code>，目标状态为<code>(c,1,c,1)</code></p>
<p>这个问题的算符如下：<br><code>goto(U)</code>表示猴子走到水平位置U；<code>pushbox(V)</code>表示猴子把箱子推到水平位置V；<code>climbbox</code>表示猴子爬上箱顶；<code>grasp</code>表示猴子摘到香蕉。</p>
<p>由初始状态变换为目标状态的操作序列为：<br>    Step1: <code>goto(b)</code><br>    Step2: <code>pushbox(c)</code><br>    Step3: <code>climbbox</code><br>    Step4: <code>grasp</code><br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c71698de93c6.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<h4 id="问题归约法"><a href="#问题归约法" class="headerlink" title="问题归约法"></a>问题归约法</h4><blockquote>
<p>问题规约（Problem Reduction）：</p>
</blockquote>
<ul>
<li>另外一种 <strong>基于状态空间</strong>的问题描述与求解方法；</li>
<li>已知问题的描述，通过一系列 <strong>变换</strong> 把此问题变为一个 <strong>子问题集合</strong>；</li>
<li>这些子问题的解可以直接得到（<strong>本原问题</strong>），从而解决了初始问题。</li>
</ul>
<blockquote>
<p>问题归约的组成部分：</p>
</blockquote>
<ul>
<li>一个初始问题描述；</li>
<li>一套把问题变换为子问题的 <strong>操作符</strong>；</li>
<li>一套本原问题描述。(本原问题:不能再分解或变换且直接可解的子问题)。</li>
</ul>
<blockquote>
<p>问题归约的 <strong>实质</strong>：</p>
</blockquote>
<ul>
<li>从目标（要解决的问题）出发 <strong>逆向推理</strong>，建立子问题以及子问题的子问题，直到 <strong>最后把初始问题归约为一个本原问题集合</strong>。</li>
</ul>
<blockquote>
<p>问题归约法举例：</p>
</blockquote>
<hr>
<p>【例子】<br><strong>汉诺塔问题</strong>(Hanoi):规定每次移动一个盘子、且总个过程中大盘在下小盘在上、目标是将盘子从柱子1移到柱子3。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716ae5a04f8.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p><strong>解题过程</strong></p>
<p>原始问题可以归约为下列3个子问题：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716ae5ec730.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>规约过程：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716ae5c0158.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>归约图示：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716ae5f07cb.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<blockquote>
<p>与或图表示</p>
</blockquote>
<p>用一个类似于图的结构来表示,把问题归约为后继问题的替换集合。</p>
<ul>
<li><p>与图：把一个复杂问题分解为若干个较为简单的子问题，形成“与”树。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d990f22f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
<li><p>或图：把原问题变换为若干个较为容易求解的新问题，形成“或”树。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d98ec484.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<h4 id="谓词逻辑法"><a href="#谓词逻辑法" class="headerlink" title="谓词逻辑法"></a>谓词逻辑法</h4><p>谓词逻辑法采用谓词合式公式和一阶谓词演算将要解决的问题变成一个有待证明的问题，然后利用消解定理和消解反演来证明一个新语句是从已知的正确语句中导出的，从而证明这个新语句也是正确的。<br>谓词逻辑是一种 <strong>形式语言</strong>，能够将数学中的逻辑论证符号化，谓词逻辑经常与其他表示方法混合使用，<strong>可以表示比较复杂的问题</strong>。</p>
<h5 id="谓词演算"><a href="#谓词演算" class="headerlink" title="谓词演算"></a>谓词演算</h5><blockquote>
<p>语法和语义</p>
</blockquote>
<ul>
<li>基本符号：谓词符号、变量符号、函数符号、常量符号、括号和逗号</li>
<li>原子公式由若干谓词符号和项组成</li>
</ul>
<blockquote>
<p>连词和量词</p>
</blockquote>
<ul>
<li><p>连词<br>合取、析取、蕴涵、非、双条件</p>
</li>
<li><p>量词<br>全称量词、存在量词</p>
</li>
</ul>
<blockquote>
<p>谓词公式</p>
</blockquote>
<ul>
<li><p>原子谓词公式<br>由谓词符号和若干项组成的谓词演算</p>
</li>
<li><p>分子谓词公式<br>可以用 <strong>连词</strong>把原子谓词公式组成复合谓词公式，并把它叫做分子谓词公式<br>通常把合式公式叫做谓词公式。在谓词演算中合式公式的递归定义如下：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d99498e9.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<blockquote>
<p>合式公式的性质</p>
</blockquote>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d9957c2b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<h5 id="置换与合一"><a href="#置换与合一" class="headerlink" title="置换与合一"></a>置换与合一</h5><blockquote>
<p>置换</p>
</blockquote>
<p>置换是用变元、常量、函数来替换变元，使该变元不在公式中出现，形如<code>{t1/x1, t2/x2,...，tn/xn}</code>的有限集合，其中：</p>
<ul>
<li><code>t1，t2，...，tn</code>是项；</li>
<li><code>x1，x2，...，xn</code>是互不相同的变元；</li>
<li><code>ti/xi</code>表示用ti项替换变元<code>xi</code>，不允许<code>ti</code>和<code>xi</code>相同，也不允许变元<code>xi</code>循环地出现在另一个tj中。</li>
</ul>
<p>推理规则：用合式公式的集合产生新的合式公式<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d991549a.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>置换是 <strong>可结合的</strong>；<br>用<code>s1s2</code>表示两个置换s1和s2的合成，L表示一个表达式，则有<code>(Ls1)s2 = L(s1s2)</code>以及<code>(s1s2)s3 = s1(s2s3)</code>，即用s1和s2相继作用于表达式L是与用<code>s1s2</code>作用于L一样的。<br>一般说来，置换是 <strong>不可交换的</strong>，即<code>s1s2 ≠ s2s1</code>。</p>
<blockquote>
<p>合一</p>
</blockquote>
<p>寻找项对变量的置换，以使两表达式一致，叫做合一。<br>如果一个置换s作用于表达式集合<code>{Ei}</code>的每个元素，则用<code>{Ei}s</code>来表示置换的集。称表达式{Ei}是可合一的，如果存在一个置换s使得：<code>E1s = E2s =  E3s =……</code>，那么，称此s为<code>{Ei}</code>的合一者，因为s的作用是使集合<code>{Ei}</code>成为单一形式。<br>例如：设有公式集<code>E={ P( x, y, f(y)),  P( a, g(x), z) }</code>，则<code>s={a/x, g(a)/y, f(g(a))/z}</code>是它的一个合一。</p>
<h4 id="语义网路法"><a href="#语义网路法" class="headerlink" title="语义网路法"></a>语义网路法</h4><p>语义网络是通过概念及其语义关系来表达知识一种网络图，是一种 <strong>结构化表示方法</strong>。<br>从图论的观点看，语义网络是一个“带标识的有向图”，它由 <strong>节点</strong>和 <strong>弧线或链线</strong>组成。<strong>节点代表实体、概念、情况等</strong>，<strong>弧线代表节点间的关系</strong>，必须带标识。<br>语义网络的解答是一个经过推理和匹配而得到的具有明确结果的新的语义网路，扩展后可以表示更复杂的问题。</p>
<p>语义网络中最基本的语义单元称为语义基元，可用三元组表示为：(结点1，弧，结点2)。</p>
<blockquote>
<p>二元语义网络的表示</p>
</blockquote>
<p>例如：用语义网络表示：李新的汽车的款式是“捷达”、银灰色；王红的汽车的款式是“凯越”、红色；李新和王红的汽车均属于具体概念,可增加“汽车” 这个抽象概念。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d993cab5.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<blockquote>
<p>多元语义网络的表示</p>
</blockquote>
<ul>
<li>增加情况和动作节点；</li>
<li>增加事件节点；</li>
<li>连接词和量词的表示；</li>
<li>……</li>
</ul>
<h4 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h4><ul>
<li>框架表示法<br>这是一种 <strong>结构化方法</strong>；<br>框架理论是明斯基于1975年作为理解视觉、自然语言对话及其它复杂行为的一种基础提出来的；<br>框架理论认为，人们对现实世界中各种事物的认识都是以一种类似于框架的结构存储在记忆中的。当遇到一个新事物时，就从记忆中找出一个合适的框架，并根据新的情况对其细节加以修改、补充，从而形成对这个新事物的认识。</li>
</ul>
<p>框架网络：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d9947a4f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>框架结构：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/23/5c716d9934fbb.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<ul>
<li>每个框架都有框架名，代表某一类对象</li>
<li>一个框架由若干个槽（项目）组成，用于表示对象的某个方面的属性</li>
<li><p>有时一个槽（属性）还要从不同的侧面来描述，每个侧面可具有一个或多个值。<br>注意：框架中的槽与侧面可任意定义，也可以是另一框架，形成框架网络系统。</p>
</li>
<li><p>剧本表示法(ppt-6)</p>
</li>
<li>过程表示法(ppt-6)</li>
<li>……</li>
</ul>
<h3 id="确定性推理"><a href="#确定性推理" class="headerlink" title="确定性推理"></a>确定性推理</h3><h4 id="推理的基本概念"><a href="#推理的基本概念" class="headerlink" title="推理的基本概念"></a>推理的基本概念</h4><blockquote>
<p>推理方法及其分类</p>
</blockquote>
<ol>
<li>按推理的逻辑基础分：演绎推理，归纳推理，类比归纳推理</li>
<li>按推理过程所用知识的确定性分：确定性推理、 不确定性推理</li>
<li>按推理过程推出的结论是否单调增加分：单调推理、非单调推理</li>
<li>按推理过程是否利用问题的启发性知识分：启发式推理、非启发式推理</li>
</ol>
<blockquote>
<p>推理的控制策略及其分类</p>
</blockquote>
<p>推理的控制策略是指如何使用领域知识使推理过程尽快达到目标的策略。</p>
<ul>
<li>推理策略<ul>
<li><strong>推理方向控制策略</strong>可分为<ul>
<li>正向推理</li>
<li>逆向推理</li>
<li>混合推理</li>
<li>双向推理</li>
</ul>
</li>
<li><strong>求解策略</strong>：是指仅求一个解，还是求所有解或最优解等。</li>
<li><strong>限制策略</strong>：是指对推理的深度、宽度、时间、空间等进行的限制。</li>
<li><strong>冲突消解策略</strong>：是指当推理过程有多条知识可用时，如何从这多条可用知识中选出一条最佳知识用于推理的策略。</li>
</ul>
</li>
<li>搜索策略（下面会详述）</li>
</ul>
<h4 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h4><ul>
<li>按是否使用启发式信息：<ul>
<li>盲目搜索：按预定的控制策略进行搜索，在搜索过程中获得的中间信息并不改变控制策略。</li>
<li>启发式搜索：在搜索中加入了与问题有关的启发性信息，用于指导搜索朝着最有希望的方向前进，加速问题的求解过程并找到最优解。</li>
</ul>
</li>
<li>按问题的表示方式：<ul>
<li>状态空间搜索：用指用状态空间法来表示问题所进行的搜索</li>
<li>与或树搜索：用指用问题归约法来表示问题所进行的搜索</li>
</ul>
</li>
</ul>
<h5 id="状态空间的搜索策略"><a href="#状态空间的搜索策略" class="headerlink" title="状态空间的搜索策略"></a>状态空间的搜索策略</h5><ul>
<li>状态空间的盲目搜索<ul>
<li>广度优先搜索</li>
<li>深度优先搜索</li>
<li>代价树搜索</li>
</ul>
</li>
<li>状态空间的启发式搜索<ul>
<li>启发性信息和估价函数</li>
<li>A算法和A*算法</li>
</ul>
</li>
<li>基本思想<ul>
<li>先把问题的初始状态作为当前扩展节点对其进行扩展，生成一组子节点。</li>
<li>然后检查问题的目标状态是否出现在这些子节点中。若出现，则搜索成功，找到了问题的解；若没出现，则再 <strong>按照某种搜索策略</strong>从已生成的子节点中选择一个节点作为当前扩展节点。</li>
<li>重复上述过程，直到目标状态出现在子节点中或者没有可供操作的节点为止。</li>
<li>所谓对一个节点进行“扩展”是指对该节点用某个可用操作进行作用，生成该节点的一组子节点。</li>
</ul>
</li>
<li>数据结构和符号约定<ul>
<li><strong>OPEN表</strong>：未扩展节点表，用于存放刚生成节点</li>
<li><strong>CLOSED表</strong>：已扩展节点表，用于存放已经扩展或将要扩展的节点</li>
<li>S：用表示问题的初始状态</li>
<li>G：表示搜索过程所得到的搜索图</li>
<li>M：表示当前扩展节点新生成的且不为自己先辈的子节点集</li>
</ul>
</li>
</ul>
<p><em>各种搜索策略的主要区别在于对OPEN表中节点的排列顺序不同。</em>例如，广度优先搜索把先生成的子节点排在前面，而深度优先搜索则把后生成的子节点排在前面。</p>
<blockquote>
<p>广度优先搜索算法流程：</p>
</blockquote>
<ol>
<li>把初始节点S放入OPEN表中；</li>
<li>如果OPEN表为空，则问题无解，失败退出；</li>
<li>把OPEN表的第一个节点取出放入CLOSED表，并记该节点为n；</li>
<li>考察节点n是否为目标节点。若是，则得到问题的解，成功退出；</li>
<li>若节点n不可扩展，则转第(2)步；</li>
<li>扩展节点n，将其子节点放入OPEN表的 <strong>尾部</strong>，并为每一个子节点设置指向父节点的指针，然后转第(2)步。</li>
</ol>
<p>以八数码问题为例，得到下面这个广度优先搜索树：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700f35a3f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>在上述广度优先算法中需要注意两个问题：</p>
<ul>
<li>对于任意一个可扩展的节点，总是按照固定的操作符的顺序对其进行扩展（空格左移、上移、右移、下移）。</li>
<li>在对任一节点进行扩展的时候，如果所得的某个子节点（状态）前面已经出现过，则立即将其放弃，不再重复画出（不送入OPEN表）。<br>因此，广度优先搜索的本质是，以初始节点为根节点，在状态空间图中按照广度优先的原则，生成一棵搜索树。</li>
</ul>
<p>广度优先搜索的优缺点：</p>
<ul>
<li>优点<ul>
<li>只要问题有解，用广度优先搜索总可以得到解，而且得到的是路径最短的解。</li>
</ul>
</li>
<li>缺点<ul>
<li>广度优先搜索盲目性较大，当目标节点距初始节点较远时将会产生许多无用节点，搜索效率低。</li>
</ul>
</li>
</ul>
<blockquote>
<p>深度优先搜索算法流程：</p>
</blockquote>
<ol>
<li>把初始节点S放入OPEN表中；</li>
<li>如果OPEN表为空，则问题无解 ，失败退出；</li>
<li>把OPEN表的第一个节点取出放入CLOSED表，并记该节点为n；</li>
<li>考察节点n是否为目标节点。若是，则得到问题的解，成功退出；</li>
<li>若节点n不可扩展，则转第(2)步；</li>
<li>扩展节点n，将其子节点放入OPEN表的 <strong>首部</strong>，并为每一个子节点设置 指向父节点的指针，然后转第(2)步。</li>
</ol>
<p>在深度优先搜索中，搜索一旦进入某个分支，就将沿着该分支一直向下搜索。如果目标节点恰好在此分支上，则可较快地得到解。但是，如果目标节点不在此分支上，而该分支又是一个无穷分支，则就不可能得到解。所以深度优先搜索是不完备的，即使问题有解，它也不一定能求得解。<br>因此，为了防止搜索过程沿着无益的路径扩展下去，往往给出一个节点扩展的最大深度，即 <strong>深度界限</strong>。当搜索深度达到了深度界限而仍未出现目标节点时，就换一个分支进行搜索。</p>
<p>有界深度优先搜索的特点：</p>
<ul>
<li>从某种意义上讲，有界深度优先搜索具有一定的启发性；</li>
<li>如果问题有解，且其路径长度≤dm，则上述搜索过程一定能求得解。</li>
<li>但是若解的路径长度&gt; dm,则上述搜索过程就得不到解。。</li>
<li>这说明在有界深度优先搜索中，深度界限的选择是很重要的，但是要恰当地给出dm的值是比较困难的。</li>
<li>即使能求出解，它也不一定是最优解。</li>
</ul>
<blockquote>
<p>代价树搜索</p>
</blockquote>
<p>考虑边的代价的搜索方法，代价树搜索的目的是为了找到一条代价最小的解路径。代价树搜索方法包括：</p>
<ul>
<li>代价树的广度优先搜索</li>
<li>代价树的深度优先搜索</li>
</ul>
<blockquote>
<p>启发式信息与代价函数</p>
</blockquote>
<p>采用问题自身的特性信息，以指导搜索朝着最有希望的方向前进。<br>启发性信息是指那种与具体问题求解过程有关的，并可指导搜索过程朝着最有希望方向前进的控制信息。启发信息的启发能力越强，扩展的无用结点越少。</p>
<p>启发性信息的种类</p>
<ul>
<li>有效地帮助确定扩展节点的信息；</li>
<li>有效的帮助决定哪些后继节点应被生成的信息；</li>
<li>能决定在扩展一个节点时哪些节点应从搜索树上删除的信息。</li>
</ul>
<p>估价函数的一般形式为：<code>f(x) = g(x)+h(x)</code>，其中<code>g(x)</code>表示从初始节点S0到节点x的代价；<code>h(x)</code>是从节点x到目标节点Sg的最优路径的代价的估计，它体现了问题的启发性信息，<code>h(x)</code>称为启发函数。</p>
<blockquote>
<p>A算法与A*算法</p>
</blockquote>
<p>A算法：在图搜索算法中，如果能在搜索的每一步都利用估价函数<code>f(n)=g(n)+h(n)</code>对OPEN表中的节点进行排序，则该搜索算法为A算法。</p>
<p>A算法的类型<br>可根据搜索过程中选择扩展节点的范围，将启发式搜索算法分为：</p>
<ul>
<li>全局择优搜索算法： 从OPEN表的所有节点中选择一个估价函数值最小的一个进行扩展。</li>
<li>局部择优搜索算法：仅从刚生成的子节点中选择一个估价函数值最小的一个进行扩展。</li>
</ul>
<p>A*算法是对A算法的估价函数<code>f(n)=g(n)+h(n)</code>加上某些限制后得到的一种启发式搜索算法。</p>
<p>假设<code>f*(n)</code>是从初始节点出发经过节点n达到目标节点的最小代价，估价函数<code>f(n)</code>是对<code>f*(n)</code>的 <strong>估计值</strong>。且<code>f*(n)=g*(n)+h*(n)</code>，<code>g*(n)</code>是从初始节点S0到节点n的最小代价。<code>h*(n)</code>是从节点n到目标节点的最小代价，<strong>若有多个目标节点，则为其中最小的一个</strong>。</p>
<p>A*算法对A算法（全局择优的启发式搜索算法）中的<code>g(n)</code>和<code>h(n)</code>分别提出如下限制：</p>
<ul>
<li>第一，<code>g(n)</code>是对最小代价<code>g*(n)</code>的估计，且<code>g(n)&gt;0</code>；</li>
<li>第二，<code>h(n)</code>是最小代价<code>h*(n)</code>的下界，即对任意节点n均有<code>h(n)≤h*(n)</code>。</li>
</ul>
<p>即：满足上述两条限制的A算法称为A*算法。</p>
<h5 id="与-或树的搜索策略-ppt-9-10"><a href="#与-或树的搜索策略-ppt-9-10" class="headerlink" title="与/或树的搜索策略(ppt-9~10)"></a>与/或树的搜索策略(ppt-9~10)</h5><p>1、 与/或树的一般搜索过程</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>) 把原始问题作为初始节点S0，并把它作为当前节点；</span><br><span class="line">(<span class="number">2</span>) 应用分解或等价变换操作对当前节点进行扩展；</span><br><span class="line">(<span class="number">3</span>) 为每个子节点设置指向父节点的指针；</span><br><span class="line">(<span class="number">4</span>) 选择合适的子节点作为当前节点，反复执行第(<span class="number">2</span>)步和第(<span class="number">3</span>)步，在此期间需要多次调用可解标记过程或不可解标记过程，直到初始节点被标记为可解节点或不可解节点为止。</span><br></pre></td></tr></table></figure>
<p>2、 与/或树的广度优先搜索</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)把初始节点S0放入OPEN表中；</span><br><span class="line">(<span class="number">2</span>)把OPEN表的第一个节点取出放入CLOSED表，并记该节点为n；</span><br><span class="line">(<span class="number">3</span>)如果节点n可扩展，则做下列工作：</span><br><span class="line">    ① 扩展节点n，将其子节点放入OPEN表的尾部，并为每一个子节点设置指向父节点的指针；</span><br><span class="line">    ② 考察这些子节点中有否终止节点。若有，则标记这些终止节点为可解节点，并用可解标记过程对其父节点及先辈节点中的可解解节点进行标记。如果初始解节点S0能够被标记为可解节点，就得到了解树，搜索成功，退出搜索过程；如果不能确定S0为可解节点，则从OPEN表中删去具有可解先辈的节点。</span><br><span class="line">    ③ 转第(<span class="number">2</span>)步。</span><br><span class="line">(<span class="number">4</span>) 如果节点n不可扩展，则作下列工作：</span><br><span class="line">    ① 标记节点n为不可解节点；</span><br><span class="line">    ② 应用不可解标记过程对节点n的先辈中不可解解的节点进行标记。如果初始解节点S0也被标记为不可解节点，则搜索失败，表明原始问题无解，退出搜索过程；如果不能确定S0为不可解节点，则从Open表中删去具有不可解先辈的节点。</span><br><span class="line">    ③ 转第(<span class="number">2</span>)步。</span><br></pre></td></tr></table></figure>
<hr>
<p>【例子】 设有下图所示的与/或树，节点按标注顺序进行扩展，其中标有t1、t2、t3的节点是终止节点，A、B、C为不可解的端节点。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700dab32a.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>本例中与/或树的广度优先搜索过程：</p>
<p>(1) 先扩展1号节点，生成2号节点和3号节点。<br>(2) 扩展2号节点，生成A节点和4号节点。<br>(3) 扩展3号节点，生成t1节点和5号节点。由于t1为终止节点，则标记它为可解节点，并应用可解标记过程，不能确定3号节点是否可节。<br>(4)  扩展节点A，由于A是端节点，因此不可扩展。调用不可解标记过程。<br>(5) 扩展4号节点，生成t2节点和B节点。由于t2为终止节点，标记为可解节点，应用可解标记过程，可标记2号节点为可解，但不能标记1号节点为可解。<br>(6) 扩展5号节点，生成t3节点和C节点。由于t3为终止节点，标记它为可解节点，应用可解标记过程，可标记1号节点为可解节点。<br>(7) 搜索成功，得到由1、2、3、4、5号节点和t1、t2、t3节点构成的解树。</p>
<hr>
<p>3、 与/或树的深度优先搜索</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>)把初始节点S0放入OPEN表中；</span><br><span class="line">(<span class="number">2</span>)把OPEN表第一个节点取出放入CLOSED表，并记该节点为n；</span><br><span class="line">与/或树的深度优先搜索算法如下：</span><br><span class="line">(<span class="number">3</span>)如果节点n的深度等于dm，则转第(<span class="number">5</span>)步的第①点；</span><br><span class="line">(<span class="number">4</span>)如果节点n可扩展，则做下列工作：</span><br><span class="line">    ① 扩展节点n，将其子节点放入OPEN表的首部，并为每一个子节点设置指向父节点的指针；</span><br><span class="line">    ② 考察这些子节点中是否有终止节点。若有，则标记这些终止节点为可解节点，并用可解标记过程对其父节点及先辈节点中的可解解节点进行标记。如果初始解节点S0能够被标记为可解节点，就得到了解树，搜索成功；如果不能确定S0为可解节点，则从OPEN表中删去具有可解先辈的节点。</span><br><span class="line">    ③ 转第(<span class="number">2</span>)步。</span><br><span class="line">(<span class="number">5</span>)如果节点n不可扩展，则作下列工作：</span><br><span class="line">    ① 标记节点n为不可解节点；</span><br><span class="line">    ② 应用不可解标记过程对节点n的先辈中不可解解的节点进行标记。如果初始解节点S0也被标记为不可解节点，则搜索失败，表明原始问题无解，退出搜索过程；如果不能确定S0为不可解节点，则从Open表中删去具有不可解先辈的节点。</span><br><span class="line">    ③ 转第(<span class="number">2</span>)步。</span><br></pre></td></tr></table></figure>
<p>4、 与/或树的启发式搜索</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span>) 把初始节点<span class="symbol">S0</span>放入OPEN表中；</span><br><span class="line">(<span class="number">2</span>) 求出希望树<span class="built_in">T</span>，即根据当前搜索树中节点的代价h求出以<span class="symbol">S0</span>为根的希望树<span class="built_in">T</span>；</span><br><span class="line">(<span class="number">3</span>) 依次在OPEN表中取出<span class="built_in">T</span>的端节点放入CLOSED表，并记该节点为<span class="built_in">n</span>；节点<span class="built_in">n</span>有三种不同情况：</span><br><span class="line">    ①<span class="built_in">n</span>为终止节点，</span><br><span class="line">    ②<span class="built_in">n</span>不是终止节点，但可扩展，</span><br><span class="line">    ③<span class="built_in">n</span>不是终止节点，且不可扩展，</span><br><span class="line">对三种情况分别进行步骤(<span class="number">4</span>) (<span class="number">5</span>) (<span class="number">6</span>)的操作过程；</span><br><span class="line">(<span class="number">4</span>)如果节点<span class="built_in">n</span>为终止节点，则：</span><br><span class="line">    ① 标记节点<span class="built_in">n</span>为可解节点；</span><br><span class="line">    ② 在<span class="built_in">T</span>上应用可解标记过程，对<span class="built_in">n</span>的先辈节点中的所有可解解节点进行标记；</span><br><span class="line">    ③ 如果初始解节点<span class="symbol">S0</span>能够被标记为可解节点，则<span class="built_in">T</span>就是最优解树，成功退出；</span><br><span class="line">    ④ 否则，从OPEN表中删去具有可解先辈的所有节点。</span><br><span class="line">    ⑤ 转第(<span class="number">2</span>)步。</span><br><span class="line">(<span class="number">5</span>) 如果节点<span class="built_in">n</span>不是终止节点，但可扩展，则：</span><br><span class="line">    ① 扩展节点<span class="built_in">n</span>，生成<span class="built_in">n</span>的所有子节点；</span><br><span class="line">    ② 把这些子节点都放入OPEN表中，并为每一个子节点设置指向父节点<span class="built_in">n</span>的指针；</span><br><span class="line">    ③ 计算这些子节点及其先辈节点的h值；</span><br><span class="line">    ④ 转第(<span class="number">2</span>)步。</span><br><span class="line">(<span class="number">6</span>) 如果节点<span class="built_in">n</span>不是终止节点，且不可扩展，则：</span><br><span class="line">    ① 标记节点<span class="built_in">n</span>为不可解节点；</span><br><span class="line">    ② 在<span class="built_in">T</span>上应用不可解标记过程，对<span class="built_in">n</span>的先辈节点中的所有不可解解节点进行标记；</span><br><span class="line">    ③ 如果初始解节点<span class="symbol">S0</span>能够被标记为不可解节点，则问题无解，失败退出；</span><br><span class="line">    ④ 否则，从OPEN表中删去具有不可解先辈的所有节点。</span><br><span class="line">    ⑤ 转第(<span class="number">2</span>)步。</span><br></pre></td></tr></table></figure>
<p>5、 博弈树的启发式搜索<br>6、 α-β剪枝技术</p>
<blockquote>
<p>搜索的完备性与效率</p>
</blockquote>
<p><strong>完备性</strong></p>
<ul>
<li>对于一类 <strong>可解的问题</strong>和一个搜索过程，如果运用该搜索过程一定能求得该类问题的解，则称该搜索过程为 <strong>完备</strong>的，否则为不完备的。</li>
<li>完备的搜索过程称为“搜索算法”。不完备的搜索过程不是算法，称为“过程”。</li>
<li>广度优先搜索、代价树的广度优先搜索、改进后的有界深度优先搜索以及A*算法都是完备的搜索过程，其它搜索过程都是不完备的。</li>
</ul>
<p><strong>搜索效率</strong></p>
<ul>
<li>一个搜索过程的搜索效率不仅取决于过程自身的启发能力，而且还与被解问题的有关属性等多种因素有关。</li>
<li>为了比较求解同一问题的不同搜索方法的效率，常用以下两种指标来衡量：<ul>
<li><strong>外显率</strong></li>
<li><strong>有效分支因数</strong></li>
</ul>
</li>
</ul>
<p>其中，外显率定义为：<code>P=L/T</code>；L为从初始节点到目标节点的路径长度；T为整个搜索过程中所生成的节点总数。<br>外显率反映了搜索过程中从初始节点向目标节点前进时 <strong>搜索区域的宽度</strong>。当<code>T=L</code>时，<code>P=1</code>，表示搜索过程中每次只生成一个节点，它恰好是解路径上的节点，搜索效率最高。P越小表示搜索时产生的无用节点愈多，搜索效率愈低。</p>
<p>有效分枝因数B定义为：<code>B+B^2+…+B^L=T</code>；B是有效分枝因数，它表示在整个搜索过程中 <strong>每个节点平均生成的子节点数目</strong>；L为从初始节点到目标节点的路径长度；T为整个搜索过程中所生成的节点总数。当<code>B＝1</code>时，<code>L=T</code>，此时所生成的节点数最少，搜索效率最高。</p>
<h4 id="自然演绎推理"><a href="#自然演绎推理" class="headerlink" title="自然演绎推理"></a>自然演绎推理</h4><p>从一组已知为真的事实出发，直接运用经典逻辑中的推理规则推出结论的过程称为自然演绎推理。<br>自然演绎推理最基本的推理规则是三段论推理，它包括：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700f34e3a.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<p>【例子】 设已知如下事实：<br>(1) 只要是需要编程序的课，王程都喜欢。<br>(2) 所有的程序设计语言课都是需要编程序的课。<br>(3) C是一门程序设计语言课。<br>求证：王程喜欢C这门课。</p>
<p>证明：<br>第一步，首先定义谓词<br>    <code>Prog(x)</code>：x是需要编程序的课。<br>    <code>Like(x, y)</code>: x喜欢y。<br>    <code>Lang(x)</code>: x是一门程序设计语言课<br>第二步，把已知事实及待求解问题用谓词公式表示如下：<br>    <code>Prog(x)→Like(Wang , x)</code><br>    <code>(∀x)( Lang(x)→Prog(x))</code><br>    <code>Lang(C)</code><br>第三步，应用推理规则进行推理：<br>    <code>Lang(y)→Prog(y)</code>                                    全称固化<br>    <code>Lang(C)，Lang(y)→Prog(y) ⇒ Prog(C)</code>                 假言推理 {C/y}<br>    <code>Prog(C),  Prog(x)→Like(Wang , x) ⇒ Like(Wang , C)</code>  假言推理  {C/x}<br>因此，王程喜欢C这门课。</p>
<hr>
<h5 id="注意避免以下两类错误："><a href="#注意避免以下两类错误：" class="headerlink" title="注意避免以下两类错误："></a>注意避免以下两类错误：</h5><ul>
<li>肯定后件的错误：当<code>P→Q</code>为真时，希望通过肯定后件Q为真来推出前件P为真，这是不允许的。</li>
<li>否定前件的错误：当<code>P→Q</code>为真时，希望通过否定前件P来推出后件Q为假，这也是不允许的。</li>
</ul>
<h5 id="自然演绎推理的优缺点"><a href="#自然演绎推理的优缺点" class="headerlink" title="自然演绎推理的优缺点"></a>自然演绎推理的优缺点</h5><ul>
<li>优点<ul>
<li>定理证明过程自然，易于理解，并且有丰富的推理规则可用。</li>
</ul>
</li>
<li>缺点<ul>
<li>是容易产生知识爆炸，推理过程中得到的中间结论一般按指数规律递增，对于复杂问题的推理不利，甚至难以实现。</li>
</ul>
</li>
</ul>
<h4 id="消解演绎推理"><a href="#消解演绎推理" class="headerlink" title="消解演绎推理"></a>消解演绎推理</h4><p>一种基于 <strong>鲁滨逊（Robinson）消解原理</strong>的机器推理技术。鲁滨逊消解原理亦称为消解原理，是鲁滨逊于1965年在海伯伦（Herbrand）理论的基础上提出的一种基于逻辑的“反证法”。</p>
<blockquote>
<p>在人工智能中，几乎所有的问题都可以转化为一个定理证明问题。定理证明的实质，就是要对前提P和结论Q，证明<code>P→Q</code>永真。<br>而要证明<code>P→Q</code>永真，就是要证明<code>P→Q</code>在任何一个非空的个体域上都是永真的。这将是非常困难的，甚至是不可实现的。</p>
</blockquote>
<p><strong>鲁滨逊消解原理</strong>把永真性的证明转化为关于 <strong>不可满足性</strong>的证明。即：要证明<code>P→Q</code>永真，只需证明<code>P∧¬Q</code>不可满足。（<code>¬(P→Q) ⇔ ¬(¬P∨Q) ⇔ P∧¬Q</code>）</p>
<h5 id="子句集及其化简"><a href="#子句集及其化简" class="headerlink" title="子句集及其化简"></a>子句集及其化简</h5><blockquote>
<p>鲁滨逊消解原理是在子句集的基础上讨论问题的。因此，讨论消解演绎推理之前，需要先讨论子句集的有关概念。</p>
</blockquote>
<ul>
<li>原子谓词公式及其否定统称为 <strong>文字</strong>。例如: P(x)、Q(x)、¬ P(x)、 ¬ Q(x)等都是文字。</li>
<li>任何文字的析取式称为 <strong>子句</strong>。例如，P(x)∨Q(x)，P(x，f(x))∨Q(x，g(x))都是子句。</li>
<li>不含任何文字的子句称为 <strong>空子句</strong>。<ul>
<li>由于空子句不含有任何文字，也就不能被任何解释所满足，因此 <strong>空子句是永假的</strong>，不可满足的。</li>
<li>空子句一般被记为NIL。</li>
</ul>
</li>
<li>由子句或空子句所构成的集合称为 <strong>子句集</strong>。<ul>
<li>在子句集中，子句之间是 <strong>合取关系</strong>；</li>
<li>子句集中的变元受 <strong>全称量词</strong>的约束；</li>
<li>任何谓词公式都可通过等价关系及推理规则化为相应的子句集。</li>
</ul>
</li>
</ul>
<p>把谓词公式化成子句集的步骤<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71701202615.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>子句集的意义<br>在上述化简过程中，由于在消去存在量词时所用的Skolem函数可以不同，因此化简后的标准子句集是不唯一的。因此，当原谓词公式为非永假时，它与其标准子句集并不等价。但当原谓词公式为永假（或不可满足）时，其标准子句集则一定是永假的，即Skolem化并不影响原谓词公式的永假性。</p>
<p>不可满足性<br>对于任意论域中的任意一个解释，S中的子句不能同时取得真值T。</p>
<blockquote>
<p>定理：设有谓词公式F，其子句集为S，则F不可满足的充要条件是S不可满足。</p>
</blockquote>
<ul>
<li>由此定理可知，要证明一个谓词公式是不可满足的，只要证明其相应的标准子句集是不可满足的就可以了。</li>
<li>由于子句集中的子句之间是合取关系，<strong>子句集中只要有一个子句为不可满足，则整个子句集就是不可满足的</strong>。</li>
<li>空子句是不可满足的。因此，<strong>一个子句集中如果包含有空子句，则此子句集就一定是不可满足的</strong>。</li>
<li>这个定理是 <strong>鲁滨逊消解原理的主要依据</strong>。</li>
</ul>
<h5 id="鲁滨逊消解原理"><a href="#鲁滨逊消解原理" class="headerlink" title="鲁滨逊消解原理"></a>鲁滨逊消解原理</h5><p>鲁滨逊消解原理的基本思想</p>
<ul>
<li>首先把欲证明问题的 <strong>结论否定</strong>，并加入子句集，得到一个扩充的子句集S’；</li>
<li>然后设法检验子句集S’是否含有空子句，若含有空子句，则表明S’是不可满足的；若不含有空子句，则继续使用消解法，在子句集中选择合适的子句进行消解，<strong>直至导出空子句或不能继续消解为止</strong>。</li>
</ul>
<p>鲁滨逊消解原理包括</p>
<ul>
<li>命题逻辑的消解</li>
<li>谓词逻辑的消解</li>
</ul>
<blockquote>
<p>命题逻辑的消解</p>
</blockquote>
<p>消解推理的核心是求两个子句的 <strong>消解式</strong>。</p>
<ul>
<li>设C1和C2是子句集中的任意两个子句，如果C1中的文字L1与C2中的文字L2 <strong>互补</strong>，那么可从C1和C2中分别消去L1和L2，并将C1和C2中余下的部分按析取关系构成一个新的子句C12，则称这一过程为 <strong>消解</strong>，称C12为C1和C2的 <strong>消解式</strong>，称C1和C2为C12的 <strong>亲本子句</strong>。</li>
</ul>
<hr>
<p>【例子】</p>
<ol>
<li>设C1=¬Q，C2=Q，则C1和C2的消解式C12 = NIL .</li>
<li>设C1=P∨Q∨R，C2=¬P∨S，则C1和C2的消解式C12 = Q∨R∨S .</li>
<li>设C1 =¬P ∨ Q ，C2=¬Q，C3=P，则C1、C2、C3的消解式C123 = NIL .</li>
</ol>
<hr>
<ul>
<li>很显然，可以得出定理：<strong>消解式C12是其亲本子句C1和C2的逻辑结论。</strong>根据该定理，可以得到以下推论：<ul>
<li>推论1：设C1和C2是子句集S中的两个子句，C12是C1和C2的消解式，若用C12代替C1和C2后得到新的子句集S1，则由S1的不可满足性可以推出原子句集S的不可满足性。即：<br><code>S1的不可满足性⇔S的不可满足性</code></li>
<li>推论2：设C1和C2是子句集S中的两个子句，C12是C1和C2的消解式，若把C12加入S中得到新的子句集S2，则S与S2的不可满足性是等价的。即：<br><code>S2的不可满足性⇔S的不可满足性</code></li>
</ul>
</li>
</ul>
<p>上述两个推论说明，<em>为证明子句集S的不可满足性，只要对其中可进行消解得子句进行消解，<strong>并把消解式加入到子句集S中，或者用消解式代替他的亲本子句</strong>，然后对新的子句集证明其不可满足性就可以了</em>。<br>如果经消解能得到空子句，根据空子句的不可满足性，即可得到原子句集S是不可满足的结论。<br>在命题逻辑中，对不可满足的子句集S，其消解原理是完备的。即：<strong>子句集S是不可满足的，当且仅当存在一个从S到空子句的消解过程。</strong></p>
<p>应用消解原理证明定理的过程称为 <strong>消解反演</strong>。</p>
<blockquote>
<p>命题逻辑的消解反演：</p>
</blockquote>
<p>在命题逻辑中，已知F，证明G为真的消解反演过程如下：</p>
<ol>
<li>否定目标公式G，得¬G;</li>
<li>把¬G并入到公式集F中，得到{F，¬G}；</li>
<li>把{F，¬G}化为子句集S；</li>
<li>应用消解原理对子句集S中的子句进行消解，并把每次得到的消解式并入S中。如此反复进行，若 <strong>出现空子句</strong>，则停止消解，此时就证明了G为真。</li>
</ol>
<hr>
<p>【例子】 设已知的公式集为<code>{P,(P∧Q)→R,(S∨T)→Q,T}</code>，求证：R为真。</p>
<p>解：假设结论R为假, 将¬R加入公式集，并化为子句集：<br>    <code>S={P,¬P∨¬Q∨R,¬S∨Q,¬T∨Q,T,¬R}</code><br>其消解过程如下图的消解演绎树所示。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700dc0e44.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>该树根为空子句NIL，则子句集S不可满足，即假设有误，于是证得R为真。</p>
<hr>
<blockquote>
<p>谓词逻辑的消解</p>
</blockquote>
<ul>
<li>在谓词逻辑中，由于子句集中的谓词一般都含有变元，因此不能象命题逻辑那样直接消去互补文字。</li>
<li>对于谓词逻辑，需要先用一个最一般合一对变元进行置换，然后才能进行消解。</li>
</ul>
<p>设C1和C2是两个没有公共变元的子句，L1和L2分别是C1和C2中的文字。如果 σ 是L1和¬ L2存在的<code>最一般合一</code>，则称：<br>    <code>C12=({C1σ}-{ L1σ})∪({ C2σ}-{ L2σ})</code><br>为C1和C2的二元消解式，L1和L2为消解式上的文字。</p>
<p><strong>注意</strong>：在谓词逻辑的消解过程中，要注意以下几个问题：</p>
<ol>
<li>若C1和C2有相同的变元x，需要将其中一个变元更名。（例2）</li>
<li>求消解式不能同时消去两个互补对，消去这两个互补文字所得的结果不是两个亲本子句的逻辑结论。(例3)</li>
<li>对参加消解的某个子句，若其内部有可合一的文字，则在进行消解之前应先对这些文字进行合一，以实现这些子句内部的化简。(例4)</li>
</ol>
<hr>
<p>【例子】<br>例1、设<code>C1=P(a)∨R(x)</code>，<code>C2=¬P(y)∨Q(b)</code>，求 C12。<br>解：取<code>L1= P(a)</code>, <code>L2=¬P(y)</code>，则L1和¬L2的最一般合一是<code>σ={a/y}</code>。因此：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C12 = ( &#123;C1σ&#125;-&#123;L1σ&#125;) ∪ (&#123;C2σ&#125;-&#123;L2σ&#125;)</span><br><span class="line">    = (&#123;<span class="constructor">P(<span class="params">a</span>)</span>, <span class="constructor">R(<span class="params">x</span>)</span>&#125;-&#123;<span class="constructor">P(<span class="params">a</span>)</span>&#125;)∪(&#123;¬<span class="constructor">P(<span class="params">a</span>)</span>, <span class="constructor">Q(<span class="params">b</span>)</span>&#125;-&#123;¬<span class="constructor">P(<span class="params">a</span>)</span>&#125;)</span><br><span class="line">    = (&#123;<span class="constructor">R(<span class="params">x</span>)</span>&#125;)∪(&#123;<span class="constructor">Q(<span class="params">b</span>)</span>&#125;)</span><br><span class="line">    = &#123; <span class="constructor">R(<span class="params">x</span>)</span>, <span class="constructor">Q(<span class="params">b</span>)</span> &#125;</span><br><span class="line">    = <span class="constructor">R(<span class="params">x</span>)</span>∨<span class="constructor">Q(<span class="params">b</span>)</span></span><br></pre></td></tr></table></figure>
<p>例2、设<code>C1=P(x)∨Q(a)</code>，<code>C2=¬P(b)∨R(x)</code>，求 C12。<br>解：由于C1和C2有相同的变元x，不符合定义的要求。为了进行消解，需要修改C2中变元的名字。令<code>C2=¬P(b)∨R(y)</code>，此时<code>L1= P(x)</code>, <code>L2 =¬P(b)</code>，L1和¬L2的最一般合一是 <code>σ={b/x}</code>。则有:</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C12 = (&#123;C1σ&#125;-&#123;L1σ&#125;)∪ (&#123;C2σ&#125;-&#123;L2σ&#125;)</span><br><span class="line">    = (&#123;<span class="constructor">P(<span class="params">b</span>)</span>, <span class="constructor">Q(<span class="params">a</span>)</span>&#125;-&#123;<span class="constructor">P(<span class="params">b</span>)</span>&#125;) ∪ (&#123;¬<span class="constructor">P(<span class="params">b</span>)</span>, <span class="constructor">R(<span class="params">y</span>)</span>&#125;-&#123;¬<span class="constructor">P(<span class="params">b</span>)</span>&#125;)</span><br><span class="line">    = (&#123;<span class="constructor">Q(<span class="params">a</span>)</span>&#125;) ∪ (&#123;<span class="constructor">R(<span class="params">y</span>)</span>&#125;)</span><br><span class="line">    = &#123;<span class="constructor">Q(<span class="params">a</span>)</span>, <span class="constructor">R(<span class="params">y</span>)</span>&#125;</span><br><span class="line">    = <span class="constructor">Q(<span class="params">a</span>)</span>∨<span class="constructor">R(<span class="params">y</span>)</span></span><br></pre></td></tr></table></figure>
<p>例3、设 <code>C1=P(a)∨¬Q(x)</code>，<code>C2=¬P(y)∨Q(b)</code>，求C12。<br>解：对C1和C2通过最一般合一（<code>σ={b/x, a/y}</code>）的作用，便得到空子句NIL的结论，从而得出C1、C2互相矛盾的结论，而事实上C1、C2并无矛盾。</p>
<p>例4、设 <code>C1=P(x)∨P(f(a))∨Q(x)</code> ，<code>C2=¬P(y)∨R(b)</code>，求C12。<br>解：本例的C1中有可合一的文字P(x)与P(f(a))，用它们的最一般合一<code>σ={f(a)/x}</code>进行代换，可得到 ：<br><code>C1σ=P(f(a))∨Q(f(a))</code><br>此时对C1σ与C2进行消解。选<code>L1= P(f(a))</code>, <code>L2 =¬P(y)</code>，L1和L2的最一般合一是<code>σ={f(a)/y}</code>，则可得到C1和C2的二元消解式为：<br><code>C12=R(b)∨Q(f(a))</code></p>
<p>例5、设 <code>C1=P(y)∨P(f(x))∨Q(g(x))</code>、<code>C2=¬P(f(g(a)))∨Q(b)</code>，求C12。<br>解：对C1，取最一般合一<code>σ={f(x)/y}</code>，得C1的因子<code>C1σ=P(f(x))∨Q(g(x))</code>，对C1的因子和C2消解（<code>σ={g(a)/x }</code>），可得：<code>C12=Q(g(g(a)))∨Q(b)</code></p>
<hr>
<blockquote>
<p>谓词逻辑的消解反演：</p>
</blockquote>
<p>在谓词逻辑中，已知F，证明G是F的结论的消解反演过程如下：</p>
<ol>
<li>否定目标公式G，得¬G;</li>
<li>把¬G并入到公式集F中，得到{F，¬G}；</li>
<li>把{F，¬G}化为子句集S；</li>
<li>应用消解原理对子句集S中的子句进行消解，并把每次得到的消解式并入S中。如此反复进行，若出现空子句，则停止消解，此时就证明了G为真。</li>
</ol>
<p>与命题逻辑的消解反演过程比较一下</p>
<ul>
<li>步骤基本相同，但每步的处理对象不同。</li>
<li>在步骤(3)化简子句集时，谓词逻辑需要把由谓词构成的公式集化为子句集。</li>
<li>在步骤(4)按消解原理进行消解时，谓词逻辑的消解原理需要考虑两个亲本子句的最一般合一。</li>
</ul>
<hr>
<p>【例子】<br>例1、已知<code>F:(∀x)((∃y)(A(x, y)∧B(y))→(∃y)(C(y)∧D(x, y)))</code>、<code>G:¬(∃x)C(x)→(∀x)(∀y)(A(x, y)→¬B(y))</code>，求证G是F的逻辑结论。</p>
<p>证明：<br>第一步，先把G否定，并放入F中，得到的<code>{F,¬G}</code>：<br>    <code>{(∀ x)((∃ y)(A(x,y)∧B(y))→(∃ y)(C(y)∧D(x,y)))</code>，<code>¬(¬(∃ x)C(x)→(∀ x)(∀ y)(A(x,y)→¬ B(y)))}</code><br>第二步，把{F,¬G}化成子句集，得到<br>    (1) <code>¬A(x,y)∨¬B(y)∨C(f(x))</code><br>    (2) <code>¬A(u,v)∨¬B(v)∨D(u,f(u))</code><br>    (3) <code>¬C(z)</code><br>    (4) <code>A(m,n)</code><br>    (5) <code>B(k)</code><br>第三步，应用谓词逻辑的消解原理对上述子句集进行消解，其过程为：<br>    (6) <code>¬ A(x,y)∨¬ B(y)</code>    <em>(1)和(3)消解，取σ={f(x)/z}</em><br>    (7) <code>¬ B(n)</code>             <em>(4)和(6)消解，取σ={m/x,n/y}</em><br>    (8) <code>NIL</code>                  <em>(5)和(7)消解，取σ={n/k}</em><br>最后，“G是F的逻辑结论”得证。</p>
<p>上述消解过程可用如下消解树来表示<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700ee5dbe.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>例2、<strong>“快乐学生”问题</strong><br>假设：任何通过计算机考试并获奖的人都是快乐的，任何肯学习或幸运的人都可以通过所有考试，张不肯学习但他是幸运的，任何幸运的人都能获奖。<br>求证：张是快乐的。</p>
<p>证明：(完整的解题过程)<br>第一步，先定义谓词：<br>    Pass(x, y)：x可以通过y考试<br>    Win(x, prize)：x能获得奖励<br>    Study(x) ：x肯学习<br>    Happy(x)：x是快乐的<br>    Lucky(x) ：x是幸运的<br>第二步，将已知条件以及结论的否定用谓词表示如下：<br>    “任何通过计算机考试并奖的人都是快乐的”<br>            <code>(∀x)(Pass(x, computer)∧Win(x, prize)→Happy(x))</code><br>    “任何肯学习或幸运的人都可以通过所有考试”<br>            <code>(∀x)(∀y)(Study(x)∨Lucky(x)→Pass(x, y))</code><br>    “张不肯学习但他是幸运的”<br>            <code>¬Study(zhang)∧Lucky(zhang)</code><br>    “任何幸运的人都能获奖”<br>            <code>(∀x)(Lucky(x)→Win(x, prize))</code><br>    结论“张是快乐的”的否定<br>            <code>¬Happy(zhang)</code><br>第三步，将上述谓词公式转化为子句集如下：</p>
<pre><code>1. `¬Pass(x, computer)∨¬Win(x, prize)∨Happy(x)`
2. `¬Study(y)∨Pass(y, z)`
3. `¬Lucky(u)∨Pass(u, v)`
4. `¬Study(zhang)`
5. `Lucky(zhang)`
6. `¬Lucky(w)∨Win(w, prize)`
7. `¬ Happy(zhang)`    (结论的否定)
</code></pre><p>第四步，按消解原理进行消解，消解树如下：<br>    <img src="https://i.loli.net/2019/02/24/5c71700f755dd.jpg" alt></p>
<p>最后，“张是快乐的”得证。</p>
<hr>
<h5 id="消解反演推理的消解策略"><a href="#消解反演推理的消解策略" class="headerlink" title="消解反演推理的消解策略"></a>消解反演推理的消解策略</h5><blockquote>
<p>在消解演绎推理中，由于事先并不知道哪些子句对可进行消解，更不知道通过对哪些子句对的消解能尽快得到空子句，因此就需要对子句集中的所有子句逐对进行比较，直到得出空子句为止。这种盲目的全面进行消解的方法，不仅会产生许多无用的消解式，更严重的是会产生组核爆炸问题。因此，需要研究有效的消解策略来解决这些问题。</p>
</blockquote>
<p>常用的消解策略可分为两大类：</p>
<ul>
<li>限制策略：通过限制参加消解的子句减少盲目性</li>
<li>删除策略：通过删除某些无用的子句缩小消解范围</li>
</ul>
<h5 id="用消解反演求取问题的答案"><a href="#用消解反演求取问题的答案" class="headerlink" title="用消解反演求取问题的答案"></a>用消解反演求取问题的答案</h5><p>消解原理除了可用于 <strong>定理证明</strong>外，还可用来 <strong>求取问题答案</strong>，其思想与定理证明相似。<br>其一般步骤为：</p>
<ol>
<li>把问题的已知条件用谓词公式表示出来，并化为子句集；</li>
<li>把问题的目标的否定用谓词公式表示出来，并化为子句集；</li>
<li>对目标否定子句集中的每个子句，构造该子句的重言式（即把该目标否定子句和此目标否定子句的否定之间再进行析取所得到的子句），用这些重言式代替相应的目标否定子句式，并把这些重言式加入到前提子句集中，得到一个新的子句集；</li>
<li>对这个新的子句集，应用消解原理求出其证明树，这时证明树的根子句不为空，称这个证明树为修改的证明树；</li>
<li>用修改证明树的根子句作为回答语句，则答案就在此根子句中。</li>
</ol>
<hr>
<p>【例子】<br>例1、已知：“张和李是同班同学，如果x和y是同班同学，则x的教室也是y的教室，现在张在302教室上课。”<br>问：“现在李在哪个教室上课？”</p>
<p>解：第一步，首先定义谓词<br>    <code>C(x, y)</code>：x和y是同班同学<br>    <code>At(x, u)</code>：x在u教室上课。<br>第二步，把已知前提用谓词公式表示如下：<br>    <code>C(zhang, li)</code><br>    <code>(∀x)(∀y)(∀u)(C(x, y)∧At(x, u)→At(y,u))</code><br>    <code>At(zhang, 302)</code><br>把目标的否定用谓词公式表示如下：<br>    <code>¬(∃v)At(li, v)</code><br>第三步，把上述表示前提的谓词公式化为子句集：<br>    <code>C(zhang, li)</code><br>    <code>¬C(x, y)∨¬At(x, u)∨At(y, u)</code><br>    <code>At(zhang, 302)</code><br>把目标的否定化成子句式，并用下面的 <strong>重言式</strong>代替：<br>         <code>¬At(li,v) ∨At(li,v)</code><br>第四步，把此 <strong>重言式</strong>加入前提子句集中，得到一个新的子句集，对这个新的子句集，应用消解原理求出其证明树。<br>求解过程如下图所示。该证明树的根子句就是所求的答案，即“李明在302教室”。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71700f30b16.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>例2、已知：A,B,C三人中有人从不说真话，也有人从不说假话。某人向这三人分别提出同一个问题：谁是说谎者？<br>    A答：“B和C都是说谎者”；<br>    B答：“A和C都是说谎者”；<br>    C答：“A和B中至少有一个是说谎者”。<br>问：求谁是老实人，谁是说谎者？</p>
<p>解：第一步，首先定义谓词<br><code>T(x)</code>：表示x说真话<br>第二步，把已知前提用谓词公式表示如下：<br>    有人从不说真话：<code>¬T(C)∨¬T(A)∨¬T(B)</code><br>    有人从不说假话：<code>T(C)∨T(A)∨T(B)</code><br>根据“A答：B和C都是说谎者”，则<br>    若A说真话：<code>T(A)→¬T(B)∧¬T(C)</code><br>    若A说假话： <code>¬T(A)→T(B)∨T(C)</code><br>同理，根据“B答：A和C都是说谎者”，则<br>    <code>T(B)→¬T(A)∧¬T(C)</code><br>    <code>¬T(B)→T(A)∨T(C)</code><br>根据“C答：A和B中至少有一个是说谎者”，则<br>    <code>T(C)→¬T(A)∨¬T(B)</code><br>    <code>¬T(C)→T(A)∧T(B)</code><br>第三步，把上述公式化成子句集，得到前提子句集S：<br>    <code>¬T(A)∨¬T(B)</code><br>    <code>¬T(A)∨¬T(C)</code><br>    <code>T(C)∨T(A)∨T(B)</code><br>    <code>¬T(B)∨¬T(C)</code><br>    <code>¬T(C)∨¬T(A)∨¬T(B)</code><br>    <code>T(A)∨T(C)</code><br>    <code>T(B)∨T(C)</code><br>第四步，先求谁是老实人，结论的否定为：<code>¬(∃x)T(x)</code>，把目标的否定化成子句式，并用下面的重言式代替：<br>    <code>¬T(x)∨T(x)</code><br>把此重言式加入前提子句集S，得到一个新子句集。<br>第五步，对这个新的子句集，应用消解原理求出其证明树。<br>    <img src="https://i.loli.net/2019/02/24/5c71700ee9580.jpg" alt></p>
<p>第六步，同理证明A不是老实人，结论的否定为： ¬T(A)，将结论的否定¬(¬T(A)) 加入并入前提子句集S中，应用消解原理对新的子句集进行消解：<br>    <img src="https://i.loli.net/2019/02/24/5c71700ee7a7f.jpg" alt></p>
<hr>
<h5 id="消解演绎推理的优缺点："><a href="#消解演绎推理的优缺点：" class="headerlink" title="消解演绎推理的优缺点："></a>消解演绎推理的优缺点：</h5><ul>
<li>优点：<ul>
<li>简单，便于在计算机上实现。</li>
</ul>
</li>
<li>缺点：<ul>
<li>必须把逻辑公式化成子句集。</li>
<li>不便于阅读与理解：¬P(x)∨Q(x)没有P(x)→Q(x)直观。</li>
<li>可能丢失控制信息，如下列逻辑公式，化成子句后都是: A∨B∨C<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">(¬A∧¬B)</span>→C    ¬A→<span class="comment">(B∨C)</span></span><br><span class="line"><span class="comment">(¬A∧¬C)</span>→B    ¬B→<span class="comment">(A∨C)</span></span><br><span class="line"><span class="comment">(¬C∧¬B)</span>→A    ¬C→<span class="comment">(B∨A)</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="基于规则的演绎推理"><a href="#基于规则的演绎推理" class="headerlink" title="基于规则的演绎推理"></a>基于规则的演绎推理</h4><blockquote>
<p>在消解演绎推理中，需要把谓词公式化为子句形，这使得原来蕴含在谓词公式中的一些重要信息却会在求取子句形的过程中被丢失。<br>在不少情况下人们多希望使用接近于问题原始描述的形式来进行求解，而不希望把问题描述化为子句集。</p>
</blockquote>
<p>基于规则的演绎推理又称为与/或形演绎推理，不再把有关知识转化为子句集，而是把领域知识及已知事实分别用蕴含式及与/或形表示出来，然后通过运用蕴含式进行演绎推理，从而证明某个目标公式。</p>
<p>规则是一种比较接近于人们习惯的问题描述方式，按照 <strong>蕴含式</strong>（“If →Then”规则）这种问题描述方式进行求解的系统称为基于规则的系统，或者叫做 <strong>规则演绎系统</strong>。</p>
<p>规则演绎系统按照推理方式可分为：</p>
<ul>
<li>规则正向演绎系统</li>
<li>规则逆向演绎系统(ppt-14)</li>
<li>规则双向演绎系统(ppt-14)</li>
</ul>
<h5 id="规则正向演绎系统"><a href="#规则正向演绎系统" class="headerlink" title="规则正向演绎系统"></a>规则正向演绎系统</h5><p>首先说明一下，在规则正向演绎系统中，对已知事实和规则都有一定的要求，如果不是所要求的形式，需要进行变换。</p>
<p>事实表达式的与或形变换</p>
<ul>
<li>在基于规则的正向演绎系统中，把事实表示为非蕴含形式的与或形，作为系统的总数据库；</li>
<li>把一个公式化为与或形的步骤与化为子句集类似，只是不必把公式化为子句的合取形式，也不能消去公式中的合取。</li>
</ul>
<p>详细来说，把事实表达式化为非蕴含形式的与/或形的步骤如下：</p>
<ol>
<li>利用 “P→Q⇔﹁P∨Q”，消去蕴含符号；</li>
<li>利用狄.摩根定律及量词转换率把“﹁”移到紧靠谓词的位置，直到否定符号的辖域最多只含一个谓词为止；</li>
<li>重新命名变元，使不同量词约束的变元有不同的名字；</li>
<li>对存在量词量化的变量用skolem函数代替；</li>
<li>消去全称量词，且使各主要合取式中的变元具有不同的变量名。</li>
</ol>
<hr>
<p>【例子】<br>有如下表达式<br>        <code>(∃x) (∀y)(Q(y, x)∧﹁((R(y)∨P(y))∧S(x, y)))</code><br>可把它转化为：<br>        <code>Q(z, a)∧(  ( ﹁R(y)∧﹁P(y) )∨﹁S(a, y)  )</code><br>这就是 <strong>与/或形表示</strong>，也可用一棵与/或图表示出来。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7171a4622da.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<blockquote>
<p>关于 <strong>与/或图</strong>说明以下几点：</p>
</blockquote>
<ul>
<li>当某表达式为k个子表达式的析取：<code>E1∨E2∨…∨Ek</code>，其中每个子表达式Ei均被表示为<code>E1∨E2∨…∨Ek</code>的后继节点，并由一个k线连接符（即图中的半圆弧）将这些后继节点都连接到其父节点，即表示成与的关系。</li>
<li>当某表达式为k个子表达式的合取：<code>E1∧E2∧…∧Ek</code>，其中的每个子表达式Ei均被表示为<code>E1∧E2∧…∧Ek</code>的一个单一的后继节点，无需用连接符连接，即表示成或的关系。</li>
<li><strong>这样，与/或图的根节点就是整个事实表达式，叶节点均为事实表达式中的一个文字</strong>。</li>
</ul>
<blockquote>
<p>有了与/或图的表示，就可以求出其解树（结束于文字节点上的子树）集。可以发现，事实表达式的子句集与解树集之间存在着一一对应关系，即 <strong>解树集中的每个解树都对应着子句集中的一个子句</strong>。<br><strong>解树集中每个解树的端节点上的文字的析取就是子句集中的一个子句。</strong></p>
</blockquote>
<p>上面那个与/或图有3个解树，分别对应这以下3个子句：<br>    <code>Q(z, a)</code>、<code>﹁R(y)∨ ﹁ S(a, y)</code>、<code>﹁P(y)∨ ﹁ S(a, y)</code></p>
<blockquote>
<p>还需要注意以下两点：</p>
<ul>
<li>这里的与/或图是作为综合数据库的一种表示，其中的变量受全称量词的约束。</li>
<li>在之前 问题归约表示 中所描述的 <strong>与/或图表示方法</strong>与这里 <strong>与/或形的与/或图表示</strong>有着不同的目的和含义，因此应用时应加以 <strong>区分</strong>。</li>
</ul>
</blockquote>
<blockquote>
<p>规则的表示</p>
</blockquote>
<p>为简化演绎过程，通常要求规则具有如下形式：<code>L→W</code>，其中，L为单文字，W为与/或形公式。<br>(之所以限制前件L为单文字，是因为在进行正向演绎推理时要用规则作用于表示事实的与/或树，而该与/或树的叶节点都是单文字，这样就可用规则的前件与叶节点进行简单匹配。对非单文字情况，若形式为L1∨L2→W，则可将其转换成与之等价的两个规则L1→W与 L2→W进行处理。)</p>
<ul>
<li>假定出现在蕴含式中的任何变量全都受全称量词的约束，并且这些变量已经被换名，使得他们与事实公式和其他规则中的变量不同。</li>
<li>如果领域知识的规则表示形式与上述要求不同，则应将它转换成要求的形式。</li>
</ul>
<blockquote>
<p>将规则转换为要求形式的步骤：</p>
</blockquote>
<p>1、 暂时消去蕴含符号“→”。设有如下公式：<br>        <code>(∀x)(((∃y) (∀ z)P(x, y,z))→(∀u)Q(x, u))</code><br>运用等价关系“P→Q⇔﹁P∨Q”，可将上式变为：<br>        <code>(∀x)(﹁((∃ y) (∀z)P(x, y,z))∨(∀u)Q(x, u))</code><br>2、 把否定符号“﹁”移到紧靠谓词的位置上，使其作用域仅限于单个谓词。通过使用狄.摩根定律及量词转换律可把上式转换为：<br>        <code>(∀ x)( (∀y) (∃z)﹁P(x, y,z))∨ (∀u)Q(x, u))</code><br>3、 引入Skolem函数，消去存在量词。消去存在量词后，上式可变为：<br>        <code>(∀ x)( (∀y) (﹁P(x, y,f(x,y)))∨(∀u)Q(x, u))</code><br>4、 把所有全称量词移至前面化成前束式，消去全部全称量词。消去全称量词后，上式变为：<br>        <code>﹁P(x, y,f(x,y))∨Q(x, u)</code><br>此公式中的变元都被视为受全称量词约束的变元。<br>5、 恢复蕴含式表示。利用等价关系“<code>﹁P∨Q⇔P→Q</code>”将上式变为：<br>        <code>P(x, y,f(x,y))→Q(x, u)</code></p>
<blockquote>
<p>目标公式的表示形式</p>
</blockquote>
<ul>
<li>与/或树正向演绎系统要求目标公式用子句形表示。如果目标公式不是子句形，则需要化成子句形。</li>
</ul>
<blockquote>
<p>推理过程</p>
</blockquote>
<p>规则正向演绎推理过程是从已知事实出发，不断运用规则，推出欲证明目标公式的过程。<br>先用与/或树把已知事实表示出来，然后再用规则的前件和与/或树的叶节点进行匹配，并通过一个匹配弧把匹配成功的规则加入到与/或树中，依此使用规则，直到产生一个含有以目标节点为终止节点的解树为止。</p>
<blockquote>
<p>下面分命题逻辑和谓词逻辑两种情况来讨论规则正向演绎过程。</p>
</blockquote>
<p><strong>命题逻辑的规则正向演绎过程</strong><br>已知事实：<code>A∨B</code><br>规则：<code>r1: A→C∧D</code>，<code>r2:  B→E∧G</code><br>目标公式：<code>C∨G</code></p>
<p>证明：<br>1）先将已知事实用与/或树表示出来；<br>2）然后再用匹配弧把r1和r2分别连接到事实与/或树中与r1和r2 的前件匹配的两个不同端节点；<br>3) 由于出现了以目标节点为终节点的解树，故推理过程结束。这一证明过程可用下图表示。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7171a47248c.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p><strong>谓词逻辑的规则正向演绎过程</strong><br>已知事实的与/或形表示：P(x, y)∨(Q(x)∧R(v, y))<br>规则：P(u, v)→(S(u)∨N(v))<br>目标公式：S(a)∨N(b)∨Q(c)</p>
<p>证明：<br>在谓词逻辑情况下，由于事实、规则及目标中均含有变元，因此，其规则演绎过程还需要用最一般合一对变进行置换。证明过程可用下图表示。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7171a481b87.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<h4 id="产生式系统"><a href="#产生式系统" class="headerlink" title="产生式系统"></a>产生式系统</h4><h5 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h5><p>产生式系统的 <strong>基本结构</strong>由 <strong>数据库</strong>、<strong>产生式规则</strong>和 <strong>控制策略</strong>三部分构成。</p>
<ul>
<li>总数据库：存放求解问题的各种当前信息，如：问题的初始状态，输入的事实，中间结论及最终结论等。<ul>
<li>推理过程中，当规则库中某条规则的前提可以和总数据库的已知事实匹配时，该规则被激活，由它推出的结论将被作为新的事实放入总数据库，成为后面推理的已知事实。</li>
</ul>
</li>
<li>产生式规则：是一个规则库，也称知识库 。用于存放与求解问题有关的所有规则的集合。</li>
<li>控制策略：亦称推理机，用于控制整个产生式系统的运行，决定问题求解过程的推理线路。<ul>
<li>控制系统的主要任务包括： 选择匹配、 冲突消解、 执行操作、 终止推理、 路径解释…</li>
</ul>
</li>
</ul>
<h5 id="产生式系统的推理"><a href="#产生式系统的推理" class="headerlink" title="产生式系统的推理"></a>产生式系统的推理</h5><p>产生式系统的推理分为 <strong>正向推理</strong>、<strong>逆向推理</strong>和 <strong>双向推理</strong>三种形式。</p>
<h5 id="主要优缺点"><a href="#主要优缺点" class="headerlink" title="主要优缺点"></a>主要优缺点</h5><p>产生式系统的主要 <strong>优缺点</strong></p>
<ul>
<li>优点：<ul>
<li>自然性：采用“如果……，则……”的形式，人类的判断性知识基本一致。</li>
<li>模块性：规则是规则库中最基本的知识单元，各规则之间只能通过总数据库发生联系，而不能相互调用，从而增加了规则的模块性。</li>
<li>有效性：产生式知识表示法既可以表示确定性知识，又可以表示不确定性知识，既有利于表示启发性知识，又有利于表示过程性知识。</li>
</ul>
</li>
<li>缺点：<ul>
<li>效率较低：各规则之间的联系必须以总数据库为媒介。并且，其求解过程是一种反复进行的“匹配—冲突消解—执行”过程。这样的执行方式将导致执行的低效率。</li>
<li>不便于表示结构性知识：由于产生式表示中的知识具有一致格式，且规则之间不能相互调用，因此那种具有结构关系或层次关系的知识则很难以自然的方式来表示。</li>
</ul>
</li>
</ul>
<h2 id="非经典推理"><a href="#非经典推理" class="headerlink" title="非经典推理"></a>非经典推理</h2><h3 id="经典推理和非经典推理"><a href="#经典推理和非经典推理" class="headerlink" title="经典推理和非经典推理"></a>经典推理和非经典推理</h3><h4 id="非经典推理-1"><a href="#非经典推理-1" class="headerlink" title="非经典推理"></a>非经典推理</h4><p>现实世界中的大多数问题存在随机性、模糊性、不完全性和不精确性。对于这些问题，若采用前面所讨论的精确性推理方法显然是无法解决的。<br>为此，出现了一些新的逻辑学派，称为非经典逻辑，相应的推理方法称为 <strong>非经典推理</strong>。包括非单调性推理、不确定性推理、概率推理和贝叶斯推理等。</p>
<h4 id="非经典逻辑推理与经典逻辑推理的区别"><a href="#非经典逻辑推理与经典逻辑推理的区别" class="headerlink" title="非经典逻辑推理与经典逻辑推理的区别"></a>非经典逻辑推理与经典逻辑推理的区别</h4><ul>
<li>在推理方法上，经典逻辑采用演绎逻辑推理，非经典逻辑采用归纳推理。</li>
<li>在辖域取值上，经典逻辑是二值逻辑，非经典逻辑是多值逻辑。</li>
<li>在运算法则上，两者大不相同。</li>
<li>在逻辑运算符上，非经典逻辑有更多的逻辑运算符。</li>
<li>在单调性上，经典逻辑是单调的，即已知事实均为充分可信的，不会随着新事实的出现而使原有事实变为假。非经典逻辑是非单调的。</li>
</ul>
<h3 id="不确定性推理"><a href="#不确定性推理" class="headerlink" title="不确定性推理"></a>不确定性推理</h3><h4 id="不确定性推理的概念"><a href="#不确定性推理的概念" class="headerlink" title="不确定性推理的概念"></a>不确定性推理的概念</h4><ul>
<li>不确定性推理是建立在非经典逻辑基础上的一种推理，它是对不确定性知识的运用与处理。</li>
<li>不确定性推理泛指除精确推理以外的其它各种推理问题。包括不完备、不精确知识的推理，模糊知识的推理，非单调性推理等。</li>
<li>不确定性推理从不确定性的初始证据（即事实）出发，通过运用不确定性的知识，最终推出具有一定程度不确定性的结论。</li>
</ul>
<h4 id="为什么要采用不确定性推理"><a href="#为什么要采用不确定性推理" class="headerlink" title="为什么要采用不确定性推理"></a>为什么要采用不确定性推理</h4><ul>
<li>所需知识不完备或问题的背景知识不足</li>
<li>所需知识描述不精确或模糊</li>
<li>多种原因导致同一结论或解题方案不唯一</li>
</ul>
<h4 id="不确定性推理的基本问题"><a href="#不确定性推理的基本问题" class="headerlink" title="不确定性推理的基本问题"></a>不确定性推理的基本问题</h4><ol>
<li>不确定性的表示</li>
<li>不确定性的匹配</li>
<li>组合证据的不确定性的计算</li>
<li>不确定性的更新</li>
<li>不确定性结论的合成</li>
</ol>
<h4 id="知识的不确定性的表示"><a href="#知识的不确定性的表示" class="headerlink" title="知识的不确定性的表示"></a>知识的不确定性的表示</h4><ul>
<li>考虑因素：1. 问题描述能力; 2. 推理中不确定性的计算</li>
<li>含义：知识的确定性程度，或静态强度</li>
<li>表示：<ul>
<li>概率，[0,1]，0接近于假，1接近于真</li>
<li>可信度，[-1,1]，大于0接近于真，小于0接近于假</li>
</ul>
</li>
</ul>
<h4 id="证据的非精确性表示"><a href="#证据的非精确性表示" class="headerlink" title="证据的非精确性表示"></a>证据的非精确性表示</h4><ul>
<li>证据来源：初始证据，中间结论</li>
<li>表示：用概率或可信度</li>
</ul>
<h4 id="不确定性的匹配"><a href="#不确定性的匹配" class="headerlink" title="不确定性的匹配"></a>不确定性的匹配</h4><ul>
<li>含义：不确定的前提条件与不确定的事实匹配</li>
<li>问题：前提是不确定的，事实也是不确定的</li>
<li>方法：设计一个计算相似程度的算法，给出相似的限度</li>
<li>标志：相似度落在规定限度内为匹配，否则为不匹配</li>
</ul>
<h4 id="组合证据不确定性的计算"><a href="#组合证据不确定性的计算" class="headerlink" title="组合证据不确定性的计算"></a>组合证据不确定性的计算</h4><ul>
<li>含义：知识的前提条件是多个证据的组合</li>
<li>方法：<strong>T(E)表示证据E为真的程度</strong><ul>
<li>最大最小法：<br><code>T(E1 AND E2)=min{T(E1),T(E2)}</code><br><code>T(E1 OR E2)=max{T(E1),T(E2)}</code></li>
<li>概率法：在事件之间完全独立时使用<br><code>T(E1 AND E2)=T(E1)T(E2)</code><br><code>T(E1 OR E2)=T(E1)＋T(E2)－T(E1)T(E2)</code></li>
<li>有界法：<br><code>T(E1 AND E2)=max{0,T(E1)＋T(E2)－1}</code><br><code>T(E1 OR E2)=min{1,T(E1)＋T(E2)}</code></li>
</ul>
</li>
</ul>
<h4 id="不确定性的更新"><a href="#不确定性的更新" class="headerlink" title="不确定性的更新"></a>不确定性的更新</h4><ul>
<li>主要问题：解决不确定性知识在推理的过程中，知识不确定性的累积和传递。</li>
<li>解决方法<ul>
<li>已知规则前提证据E的不确定性<code>T(E)</code>和规则的强度<code>F(E,H)</code>，则结论H的不确定性：<code>T(H) = g1[T(E),F(E,H)]</code></li>
<li>证据合取： <code>T(E1 AND E2) = g2[T(E1), T (E2)]</code></li>
<li>证据析取： <code>T(E1   OR  E2) = g3[T(E1), T (E2)]</code></li>
</ul>
</li>
</ul>
<h4 id="不确定性结论的合成"><a href="#不确定性结论的合成" class="headerlink" title="不确定性结论的合成"></a>不确定性结论的合成</h4><ul>
<li>主要问题：多个不同知识推出同一结论，且不确定性程度不同</li>
<li>解决方法：<ul>
<li>并行规则算法：根据独立证据E1和E2分别求得结论H的不确定性为<code>T1(H)</code>和<code>T2(H)</code>，则证据E1和E2的组合导致结论H的不确定性：<code>T(H)=g[T1(H), T2(H)]</code></li>
<li>函数g视不同推理方法而定</li>
</ul>
</li>
</ul>
<h3 id="概率推理"><a href="#概率推理" class="headerlink" title="概率推理"></a>概率推理</h3><h4 id="概率论基础回顾"><a href="#概率论基础回顾" class="headerlink" title="概率论基础回顾"></a>概率论基础回顾</h4><ul>
<li><a href="https://baike.baidu.com/item/%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F" target="_blank" rel="noopener">全概率公式</a></li>
<li><a href="https://baike.baidu.com/item/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/9683982" target="_blank" rel="noopener">贝叶斯(Bayes)公式</a></li>
</ul>
<h4 id="概率推理方法"><a href="#概率推理方法" class="headerlink" title="概率推理方法"></a>概率推理方法</h4><ul>
<li>设有如下产生式规则：IF  E   THEN   H<ul>
<li>其中，E为前提条件，H为结论。</li>
<li>条件概率P(H|E)可以作为在证据E出现时结论H的确定性程度，即规则的静态强度。</li>
</ul>
</li>
<li><p>把贝叶斯方法用于不精确推理的思想</p>
<ul>
<li>已知前提E的概率P(E)和结论H的先验概率P(H)</li>
<li>已知H成立时E出现的条件概率P(E|H)</li>
<li><p>利用规则推出H在E出现的条件下的后验概率：</p>
<p><img src="https://i.loli.net/2019/02/24/5c7171a3de085.jpg" alt></p>
</li>
</ul>
</li>
<li><p>对于一组产生式规则：IF  E   THEN  Hi</p>
<ul>
<li>一个前提条件E支持多个结论H1, H2, …,Hn</li>
<li><p>同样有后验概率如下（ Hi 确定性的程度，或规则的静态强度）：</p>
<p><img src="https://i.loli.net/2019/02/24/5c7171a41b712.jpg" alt></p>
</li>
</ul>
</li>
<li><p>对于有多个证据E1, E2, …, Em和多个结论H1, H2, …, Hn,并且每个证据都以一定程度支持结论的情况，上面的式子可进一步扩展为:</p>
<p>  <img src="https://i.loli.net/2019/02/24/5c7171a45cf31.jpg" alt></p>
</li>
</ul>
<hr>
<p>【例子】<br>设H1,H2,H3分别是三个结论，E是支持这些结论的证据。已知：<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">P</span><span class="params">(H1)</span></span>=<span class="number">0.3</span></span><br><span class="line"><span class="function"><span class="title">P</span><span class="params">(H2)</span></span>=<span class="number">0.4</span></span><br><span class="line"><span class="function"><span class="title">P</span><span class="params">(H3)</span></span>=<span class="number">0.5</span></span><br><span class="line"><span class="function"><span class="title">P</span><span class="params">(E|H1)</span></span>=<span class="number">0.5</span></span><br><span class="line"><span class="function"><span class="title">P</span><span class="params">(E|H2)</span></span>=<span class="number">0.3</span></span><br><span class="line"><span class="function"><span class="title">P</span><span class="params">(E|H3)</span></span>=<span class="number">0.4</span></span><br></pre></td></tr></table></figure></p>
<p>求P(H1|E),P(H2|E)及P(H3|E)的值各是多少？</p>
<p>解：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7171a455766.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>同理可得：<br>P(H2|E)=0.26<br>P(H3|E)=0.43<br>观察：( 注：P(E)=0.47 )<br>P(H1)=0.3 , P(E|H1)=0.5     P(H1|E)=0.32<br>P(H2)=0.4 , P(E|H2)=0.3     P(H2|E)=0.26<br>P(H3)=0.5 , P(E|H3)=0.4     P(H3|E)=0.43<br>结论：由于E的出现，H1成立的可能性增加，H2和H3成立的可能性不同程度的下降。</p>
<hr>
<h4 id="概率推理方法的特点"><a href="#概率推理方法的特点" class="headerlink" title="概率推理方法的特点"></a>概率推理方法的特点</h4><ul>
<li>优点：<ul>
<li>概率推理方法有较强的理论背景和良好的数学特性，当证据彼此独立时计算的复杂度比较低。</li>
</ul>
</li>
<li>缺点：<ul>
<li>概率推理方法要求给出结论Hi的先验概率P(Hi)及条件概率 P(Ej|Hi)。</li>
</ul>
</li>
</ul>
<h3 id="贝叶斯推理（主观贝叶斯方法）"><a href="#贝叶斯推理（主观贝叶斯方法）" class="headerlink" title="贝叶斯推理（主观贝叶斯方法）"></a>贝叶斯推理（主观贝叶斯方法）</h3><blockquote>
<p>使用概率推理方法求结论Hi在存在证据E时的条件概率P(Hi|E) ，需要给出结论Hi的先验概率P(Hi)及证据E的条件概率 P(E|Hi)。这对于实际应用是不容易做到的。<br>Duda 和 Hart 等人在贝叶斯公式的基础上，于1976年提出主观贝叶斯方法，建立了不精确推理的模型，并把它成功地应用于PROSPECTOR专家系统（PROSPECTOR是国际上著名的一个用于勘察固体矿的专家系统）。</p>
</blockquote>
<h4 id="主观贝叶斯方法-ppt-24"><a href="#主观贝叶斯方法-ppt-24" class="headerlink" title="主观贝叶斯方法(ppt-24)"></a>主观贝叶斯方法(ppt-24)</h4><ul>
<li><p>知识不确定性的表示</p>
<ul>
<li>在主观Bayes方法中，知识是用产生式表示的，其形式为：<br><code>IF  E  THEN  (LS, LN)   H</code></li>
<li>E表示规则前提条件，它既可以是一个简单条件，也可以是用AND或OR把多个简单条件连接起来的复合条件。</li>
<li><p>H是结论，用P(H)表示H的先验概率，它指出没有任何专门证据的情况下结论H为真的概率，其值由领域专家根据以往的实践经验给出。</p>
</li>
<li><p>LS是规则的充分性度量。用于指出E对H的支持程度，取值范围为[0,+∞)，其定义为：</p>
<p><img src="https://i.loli.net/2019/02/24/5c7173002e915.jpg" alt></p>
</li>
<li><p>LN是规则的必要性度量。用于指出E对H为真的必要程度，即﹁E对对H的支持程度。取值范围为[0,+∞)，其定义为：</p>
<p><img src="https://i.loli.net/2019/02/24/5c71730072035.jpg" alt></p>
</li>
</ul>
</li>
<li><p>证据不确定性的表示</p>
</li>
<li>组合证据不确定性的计算</li>
<li>不确定性的更新</li>
<li>主观贝叶斯方法的推理过程</li>
</ul>
<h3 id="可信度方法"><a href="#可信度方法" class="headerlink" title="可信度方法"></a>可信度方法</h3><blockquote>
<p>什么是可信度</p>
</blockquote>
<ul>
<li>可信度是指人们根据以往经验对某个事物或现象为真的程度的一个判断，或者说是人们对某个事物或现象为真的相信程度。</li>
<li>在可信度方法中，由专家给出规则或知识的可信度，从而 <strong>避免对先验概率、条件概率的要求</strong>。</li>
<li>可信度方法是肖特里菲（Shortliffe）等人在确定性理论基础上结合概率论等理论提出的一种不精确推理模型。</li>
</ul>
<p>该方法 <strong>直观</strong>、<strong>简单</strong>而且 <strong>效果好</strong>，在专家系统等领域获得了较为广泛的应用。</p>
<blockquote>
<p>C-F模型</p>
</blockquote>
<p>C-F模型：基于可信度表示的不确定性推理的基本方法，其他可信度方法都是基于此发展而来。</p>
<blockquote>
<p>知识的不确定性表示</p>
</blockquote>
<p>知识的不确定性表示：在C-F模型中，知识是用产生式规则表示的，其一般形式为：<br>                 <code>IF   E   THEN   H (CF(H, E))</code><br><code>E</code>：知识的前提条件，可以是单一或复合条件；<br><code>H</code>：知识的结论，可以是单一结论或多个结论；<br><code>CF(H, E)</code>：知识的可信度，称为 <strong>可信度因子</strong>(Certainty Factor)或规则强度。</p>
<p>一般情况下，CF(H, E)的取值为[-1, 1]，表示当证据E为真时，对结论H的支持程度。其值越大，表示支持程度越大。</p>
<ul>
<li>CF(H,E)&gt;0对应于P(H|E)&gt;P(H)；</li>
<li>CF(H,E)=0对应于P(H|E)=P(H)；</li>
<li>CF(H,E)&lt;0对应于P(H|E)&lt;P(H)。</li>
</ul>
<p>例如：<code>IF   发烧  AND  流鼻涕   THEN   感冒(0.7)</code>，表示当某人确实有“发烧”及“流鼻涕”症状时，则有七成的把握是患了感冒。</p>
<blockquote>
<p>CF(H,E)的定义:</p>
</blockquote>
<p><code>CF(H,E) = MB(H,E) - MD(H,E)</code></p>
<ul>
<li><p>MB ( Measure Belief ) 称为信任增长度，反映了证据对结论有利的一面。MB(H, E)定义为：<br>*</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7173006871b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
<li><p>MD ( Measure Disbelief ) 称为不信任增长度，MD反映了证据对结论不利的一面。MD(H,E)定义为：<br>*</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71730066f42.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<blockquote>
<p>MB和MD的关系：</p>
</blockquote>
<ul>
<li>当<code>P(H|E)&gt;P(H)</code>时： E的出现增加了H的概率<br><code>MB(H,E)&gt;0，MD(H,E)=0</code></li>
<li>当<code>P(H|E)&lt;P(H)</code>时： E的出现降低了H的概率<br><code>MB(H,E) =0，MD(H,E)&gt;0</code><br>因此，CF(H, E)的计算公式：</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717300bd402.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<blockquote>
<p>可信度的性质(ppt-25)</p>
</blockquote>
<ul>
<li>互斥性：对同一证据，不可能既增加对H的信任程度，又同时增加对H的不信任程度，即MB与MD是互斥的<ul>
<li>当MB(H, E)&gt;0时，MD(H, E)=0</li>
<li>当MD(H, E)&gt;0时，MB(H, E)=0</li>
</ul>
</li>
<li>值域：MB(H, E) ∈ [0,1]; MD(H, E) ∈ [0,1]; CF(H, E) ∈[-1,1],<ul>
<li>当且仅当P(H|E)=1时,  CF(H,E)=1</li>
<li>当且仅当P(H|E)=0时,  CF(H,E)=-1</li>
<li>CF(H,E)定性地反映了P(H|E)的大小，因此可以用CF(H,E)近似表示P(H|E) ，描述规则的可信度。</li>
</ul>
</li>
<li>对H的信任增长度等于对非H的不信任增长度<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717300d1b44.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>再根据CF的定义和MB、MD的互斥性有<br><code>CF(H,E)+CF(﹁H,E)=0</code></li>
<li>对前提E，若支持若干个不同的结论Hi(i=1,2,…,n)，则</li>
<li>因此，如果发现专家给出的知识有如下情况<br><code>CF(H1, E)=0.7,  CF(H2, E)=0.4</code></li>
<li>则因0.7+0.4=1.1&gt;1为非法，应进行调整或规范化。</li>
</ul>
<blockquote>
<p>证据不确定性的表示</p>
</blockquote>
<ul>
<li>证据的E不确定性也用可信度因子CF(E)表示</li>
<li>CF(E)的取值范围：[-1，+1]。<ul>
<li>CF(E)=1，证据E肯定它为真</li>
<li>CF(E)=-1，证据E肯定它为假</li>
<li>CF(E)=0，对证据E一无所知</li>
<li>0&lt;CF(E)&lt;1，证据E以CF(E)程度为真</li>
<li>-1&lt;CF(E)&lt;0，证据E以CF(E)程度为假</li>
</ul>
</li>
</ul>
<blockquote>
<p>否定证据的不确定性计算</p>
</blockquote>
<p><code>CF(¬E)=－CF(E)</code></p>
<blockquote>
<p>组合证据的不确定性计算</p>
</blockquote>
<ul>
<li>可采用最大最小法<ul>
<li>当组合证据E是多个单一证据的合取时，若已知<code>CF(E1), …, CF(En)</code>，则:<br><code>CF(E)=min{CF(E1), CF(E2), … ,CF(En)}</code></li>
<li>当组合证据E是多个单一证据的析取时，若已知<code>CF(E1), …, CF(En)</code>，则:<br><code>CF(E)=max{CF(E1), CF(E2), … ,CF(En)}</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>不确定性的更新</p>
</blockquote>
<p><code>IF   E   THEN   H  (CF(H, E))</code></p>
<blockquote>
<p>结论H的可信度由下式计算：</p>
</blockquote>
<p><code>CF(H)=CF(H,E)×max{0,CF(E)}</code></p>
<ul>
<li>CF(H)的取值范围：[-1，+1]。<ul>
<li>CF(H)=0: CF(E)&lt;0,即该模型没考虑E为假对H的影响</li>
<li>CF(H)&gt;0: 表示结论以某种程度为真</li>
<li>CF(H)&lt;0: 表示结论以某种程度为假</li>
</ul>
</li>
</ul>
<blockquote>
<p>结论不确定性的合成</p>
</blockquote>
<ul>
<li>若由多条不同知识推出了相同的结论，但可信度不同，则用合成算法求出综合可信度。设有知识：<br><code>IF  E1   THEN   H  (CF(H, E1))</code><br><code>IF  E2   THEN   H  (CF(H, E2))</code></li>
<li>则结论H 的综合可信度可分以下两步计算：<br>(1)、分别对每条知识求出其CF(H)。即<br><code>CF1(H)=CF(H, E1) ×max{0, CF(E1)}</code><br><code>CF2(H)=CF(H, E2) ×max{0, CF(E2)}</code></li>
</ul>
<p>(2)、用如下公式求E1与E2对H的综合可信度</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717300df62b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<hr>
<p>【例子】<br>设有如下一组知识：<br><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">r1</span>：<span class="selector-tag">IF</span>  <span class="selector-tag">E1</span>  <span class="selector-tag">THEN</span>  <span class="selector-tag">H</span>  (<span class="number">0.9</span>)</span><br><span class="line"><span class="selector-tag">r2</span>：<span class="selector-tag">IF</span>  <span class="selector-tag">E2</span>  <span class="selector-tag">THEN</span>  <span class="selector-tag">H</span>  (<span class="number">0.6</span>)</span><br><span class="line"><span class="selector-tag">r3</span>：<span class="selector-tag">IF</span>  <span class="selector-tag">E3</span>  <span class="selector-tag">THEN</span>  <span class="selector-tag">H</span>  (-<span class="number">0.5</span>)</span><br><span class="line"><span class="selector-tag">r4</span>：<span class="selector-tag">IF</span>  <span class="selector-tag">E4</span>  <span class="selector-tag">AND</span>  ( E5  OR  E6)  <span class="selector-tag">THEN</span>  <span class="selector-tag">E1</span>  (<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71730075bb3.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>已知：<code>CF(E2)=0.8</code>，<code>CF(E3)=0.6</code>，<code>CF(E4)=0.5</code>，<code>CF(E5)=0.6</code>,<code>CF(E6)=0.8</code>，求：<code>CF(H)</code>。</p>
<p>解：由r4得到：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"> CF(E1) = <span class="number">0.8</span>×max&#123;<span class="number">0</span>, CF(E4  AND  (E5  OR   E6))&#125;</span><br><span class="line">        = <span class="number">0.8</span>×max&#123;<span class="number">0</span>, min&#123;CF(E4),  CF(E5  OR   E6)&#125;&#125;</span><br><span class="line">        = <span class="number">0.8</span>×max&#123;<span class="number">0</span>, min&#123;CF(E4),  max&#123;CF(E5),  CF(E6)&#125;&#125;&#125;</span><br><span class="line">        = <span class="number">0.8</span>×max&#123;<span class="number">0</span>, min&#123;CF(E4),  max&#123;<span class="number">0.6</span>,  <span class="number">0.8</span>&#125;&#125;&#125;</span><br><span class="line">        = <span class="number">0.8</span>×max&#123;<span class="number">0</span>, min&#123;<span class="number">0.5</span>,  <span class="number">0.8</span>&#125;&#125;</span><br><span class="line">        = <span class="number">0.8</span>×max&#123;<span class="number">0</span>,  <span class="number">0.5</span>&#125;</span><br><span class="line">        = <span class="number">0.4</span></span><br><span class="line"> 由r1得到：</span><br><span class="line">    CF1(H) = CF(H, E1)×max&#123;<span class="number">0</span>,  CF(E1)&#125;</span><br><span class="line">           = <span class="number">0.9</span>×max&#123;<span class="number">0</span>,  <span class="number">0.4</span>&#125;</span><br><span class="line">           = <span class="number">0.36</span></span><br><span class="line"></span><br><span class="line"> 由r2得到：</span><br><span class="line">    CF2(H) = CF(H, E2)×max&#123; <span class="number">0</span>,  CF(E2) &#125;</span><br><span class="line">      　   = <span class="number">0.6</span>×max&#123; <span class="number">0</span>,  <span class="number">0.8</span> &#125;</span><br><span class="line">           = <span class="number">0.48</span></span><br><span class="line">由r3得到：</span><br><span class="line">    CF3(H) = CF(H, E3)×max&#123; <span class="number">0</span>,  CF(E3) &#125;</span><br><span class="line">      　   = <span class="number">-0.5</span>×max&#123; <span class="number">0</span>,  <span class="number">0.6</span> &#125;</span><br><span class="line">      　   = <span class="number">-0.3</span></span><br></pre></td></tr></table></figure></p>
<p>根据结论不精确性的合成算法，CF1(H)和CF2(H)同号，有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71730073a87.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>CF12(H)和CF3(H)异号，有：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717300b3a3f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>综合可信度为CF(H)=0.53。</p>
<hr>
<h3 id="证据理论-ppt-25-21"><a href="#证据理论-ppt-25-21" class="headerlink" title="证据理论(ppt-25-21)"></a>证据理论(ppt-25-21)</h3><h2 id="专家系统"><a href="#专家系统" class="headerlink" title="专家系统"></a>专家系统</h2><blockquote>
<p>专家系统的先行者费根鲍姆（Feigenbaum）曾把专家系统定义为一个应用知识和推理过程来求解那些需要大量的人类专家解决难题经验的智能计算机程序。<br>专家系统主要指的是一个智能计算机程序系统，其内部含有大量的某个领域专家水平的知识与经验，能够利用人类专家的知识和解决问题的经验方法来处理该领域的高水平难题。</p>
</blockquote>
<h3 id="专家系统概述"><a href="#专家系统概述" class="headerlink" title="专家系统概述"></a>专家系统概述</h3><ul>
<li>专家系统是一个具有大量的专门知识与经验的程序系统，它应用人工智能技术和计算机技术，根据某领域一个或多个专家提供的知识和经验，进行推理和判断，模拟人类专家的决策过程，以便解决那些需要人类专家才能处理好的复杂问题。简而言之，专家系统是一种模拟人类专家解决领域问题的计算机程序系统。</li>
<li>专家系统的基本功能取决于它所含有的知识，因此，有时也把专家系统称为基于知识的系统（knowledge-based system）。</li>
</ul>
<h4 id="专家系统的特点"><a href="#专家系统的特点" class="headerlink" title="专家系统的特点"></a>专家系统的特点</h4><ul>
<li>启发性<ul>
<li>专家系统要解决的问题，其结构往往是不合理的，其问题求解（problem-solving）知识不仅包括理论知识和常识，而且包括专家本人的启发知识。</li>
<li>能运用专家的知识和经验进行推理、判断和决策。</li>
</ul>
</li>
<li>透明性<ul>
<li>专家系统能够解释本身的推理过程和回答用户提出的问题，以便让用户了解推理过程，提高对专家系统的信赖感。</li>
<li>问题求解过程中知识应用的合理性可由检验专家系统的解释推理路径来验证。</li>
</ul>
</li>
<li>灵活性<ul>
<li>专家系统的灵活性是指它的扩展和丰富知识库的能力，以及改善非编程状态下的系统性能，即自学习能力。</li>
<li>专家系统能不断增长知识，修改原有知识，不断更新</li>
</ul>
</li>
</ul>
<h4 id="专家系统的优点"><a href="#专家系统的优点" class="headerlink" title="专家系统的优点"></a>专家系统的优点</h4><ul>
<li>专家系统能够高效率、准确、周到、迅速和不知疲倦地进行工作。</li>
<li>专家系统解决实际问题时不受周围环境的影响，也不可能遗漏和忘记。</li>
<li>可以使专家的专长不受时间和空间的限制，以便推广珍贵和稀缺的专家知识与经验。</li>
<li>专家系统能促进各领域的发展，使各领域专家的专业知识和经验得到总结和精炼，能够广泛有力地传播专家的知识、经验和能力。</li>
<li>专家系统能汇集多领域专家的知识和经验以及他们协作解决重大问题的能力，它拥有更渊博的知识、更丰富的经验和更强的工作能力。</li>
<li>军事专家系统的水平是一个国家国防现代化的重要标志之一。</li>
<li>专家系统的研制和应用，具有巨大的经济效益和社会效益。</li>
<li>研究专家系统能够促进整个科学技术的发展。专家系统对人工智能各个领域的发展起了很大的促进作用，并将对科技、经济、国防、教育、社会和人民生活产生极其深远的影响。</li>
</ul>
<h4 id="专家系统的结构"><a href="#专家系统的结构" class="headerlink" title="专家系统的结构"></a>专家系统的结构</h4><ul>
<li>专家系统简化结构图</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71746ca090b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>理想专家系统的结构图</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71746d0baad.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>专家系统的主要组成部分：<ul>
<li>知识库</li>
<li>综合数据库</li>
<li>推理机</li>
<li>解释器</li>
<li>接口</li>
</ul>
</li>
</ul>
<h4 id="专家系统的建造步骤"><a href="#专家系统的建造步骤" class="headerlink" title="专家系统的建造步骤"></a>专家系统的建造步骤</h4><ul>
<li>设计初始知识库<ul>
<li>问题知识化</li>
<li>知识概念化</li>
<li>概念形式化</li>
<li>形式规则化</li>
<li>规则合法化</li>
</ul>
</li>
<li>原型机（prototype）的开发与实验</li>
<li>知识库的改进与归纳</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71746cc7305.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<h3 id="基于规则的专家系统基于框架的专家系统"><a href="#基于规则的专家系统基于框架的专家系统" class="headerlink" title="基于规则的专家系统基于框架的专家系统"></a>基于规则的专家系统基于框架的专家系统</h3><p>一个基于规则的专家系统采用下列模块来建立产生式系统的模型：</p>
<ol>
<li>知识库：以一套规则建立人的长期存储器模型。</li>
<li>工作存储器：建立人的短期存储器模型，存放问题事实和由规则激发而推断出的新事实。</li>
<li>推理机：借助于把存放在工作存储器内的问题事实和存放在知识库内的规则结合起来，建立人的推理模型，以推断出新的信息 。</li>
</ol>
<h3 id="基于框架的专家系统"><a href="#基于框架的专家系统" class="headerlink" title="基于框架的专家系统"></a>基于框架的专家系统</h3><ul>
<li>基于框架的专家系统是一个计算机程序，该程序使用一组包含在知识库内的框架对工作存储器内的具体问题信息进行处理，通过推理机推断出新的信息。</li>
<li>基于框架的专家系统是建立在框架的基础之上的。一般概念存放在框架内，而该概念的一些特例则被表示在其他框架内并含有实际的特征值。</li>
<li>基于框架的专家系统能够提供基于规则专家系统所没有的特征，如继承、侧面、信息通信和模式匹配规则等，因而，基于框架的专家系统比基于规则的专家系统拥有更强的功能，适用于解决更复杂的问题。</li>
</ul>
<h3 id="基于模型的专家系统"><a href="#基于模型的专家系统" class="headerlink" title="基于模型的专家系统"></a>基于模型的专家系统</h3><ul>
<li>关于人工智能的一个观点:  认为人工智能是对各种定性模型（物理的、感知的、认识的和社会的系统模型）的获得、表达及使用的计算方法进行研究的学问。一个知识系统中的知识库是由各种模型综合而成的。</li>
<li>模型类型：基于逻辑的心理模型、定性的物理模型、神经元网络模型、可视知识模型等等。</li>
<li>综合各种模型的专家系统比基于逻辑心理模型的系统具有更强的功能，从而有可能显著改进专家系统的设计</li>
<li>在诸多模型中，人工神经网络模型的应用最为广泛</li>
</ul>
<h3 id="基于Web的专家系统"><a href="#基于Web的专家系统" class="headerlink" title="基于Web的专家系统"></a>基于Web的专家系统</h3><h3 id="新型专家系统"><a href="#新型专家系统" class="headerlink" title="新型专家系统"></a>新型专家系统</h3><p>以上部分专家系统就不详叙了。</p>
<hr>
<h2 id="模糊逻辑系统"><a href="#模糊逻辑系统" class="headerlink" title="模糊逻辑系统"></a>模糊逻辑系统</h2><h3 id="模糊逻辑原理"><a href="#模糊逻辑原理" class="headerlink" title="模糊逻辑原理"></a>模糊逻辑原理</h3><p>模糊逻辑的发展，是由理论准备到理论提出再到理论应用的过程</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717cc8eb13c.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<h3 id="模糊集"><a href="#模糊集" class="headerlink" title="模糊集"></a>模糊集</h3><ul>
<li>从精确到模糊<ul>
<li>精确<ul>
<li>答案确定：要么是，要么不是</li>
<li><code>f : A → {0,1}</code></li>
<li>如：他是学生？不是学生？</li>
</ul>
</li>
<li>模糊<ul>
<li>答案不定：也许是，也许不是，也许介于之间</li>
<li><code>μA : U → [0,1]</code></li>
<li>如：他是成年人？不是成年人？大概是成年人？</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>【例子】表示“20岁左右”</p>
<ul>
<li>原集合（年龄）<ul>
<li>{…., 17, 18, 19, 20, 21, 22, 23, …}</li>
</ul>
</li>
<li>模糊集可以表示为：<ul>
<li>0.8/18 + 0.9/19 + 1/20 + 0.9/21 + 0.8/12</li>
<li>0.6/17+0.7/18+0.8/19+1/20+0.9/21+0.7/22+0.6/23</li>
</ul>
</li>
</ul>
<hr>
<h4 id="集合及其特征函数"><a href="#集合及其特征函数" class="headerlink" title="集合及其特征函数"></a>集合及其特征函数</h4><ul>
<li>在论域中，把具有某种属性的事物的全体称为集合。由于集合中的元素都具有某种属性，因此可以用集合表示某一种概念，而且可用一个函数来刻画它，该函数称为特征函数。</li>
<li>设A是论域U上的一个集合，对任意u∈U，令</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d04126ba.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>则称CA(u)为集合A的特征函数。特征函数CA(u)在u=u0处的取值CA(u0)称为u0对A的隶属度。</li>
<li>集合A与其特征函数可以认为是等价的：A={u |CA(u)=1}</li>
</ul>
<h4 id="模糊集与隶属函数"><a href="#模糊集与隶属函数" class="headerlink" title="模糊集与隶属函数"></a>模糊集与隶属函数</h4><ul>
<li>模糊集把特征函数的取值范围从{0,1}推广到[0,1]上。</li>
<li>设U是论域，μA是把任意u∈U映射为[0,1]上某个值的函数，即<br><code>μA : U→[0,1]   或   u→μA(u)</code></li>
<li>则称μA为定义在U上的一个隶属函数，由μA(u)(u∈U)所构成的集合A称为U上的一个模糊集，μA(u)称为u对A的隶属度。</li>
</ul>
<hr>
<p>【例子】<br>论域U={1,2,3,4,5}，用模糊集表示“大”和“小”。</p>
<p>解：设A、B分别表示“大”与“小”的模糊集，μA ，μB分别为相应的隶属函数。<br><code>A = {0, 0, 0.1, 0.6, 1}</code><br><code>B = {1, 0.5, 0.01, 0, 0}</code><br>其中：<br><code>μA(1)=0, μA(2)=0 , μA(3)=0.1 , μA(4)=0.6 , μA(5)=1</code><br><code>μB(1)=1, μB(2)=0.5 , μB(3)=0.01 , μB(4)=0, μB(5)=0</code></p>
<hr>
<h4 id="模糊集的表示方法"><a href="#模糊集的表示方法" class="headerlink" title="模糊集的表示方法"></a>模糊集的表示方法</h4><p>(1)、论域离散且为有限<br>若论域 U={u1, … , un}为离散论域，模糊集A表示为：<br><code>A= {μA(u1), μA(u2), … ,  μA(un)}</code><br>也可写为：<br><code>A= μA(u1)/u1 + μA(u2)/u2 + … + μA(un)/un</code><br>其中，隶属度为0的元素可以不写。</p>
<p>例如：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = <span class="number">1</span>/u1+<span class="number">0.7</span>/u2+<span class="number">0</span>/u3+<span class="number">0.4</span>/u4</span><br><span class="line">  = <span class="number">1</span>/u1+<span class="number">0.7</span>/u2+<span class="number">0.4</span>/u4</span><br></pre></td></tr></table></figure></p>
<p>(2)、论域连续<br>若论域是连续的，则模糊集可用实函数表示。</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d041d888.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>例如：以年龄为论域U=[0,100]， “年轻”和“年老”这两个概念可表示为：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d042113c.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>(3)、一般表示方法<br>不管论域 U 是有限的还是无限的，是连续的亦或是离散的，扎德（ L. A. Zadeh ）又给出了一种类似于积分的一般表示形式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d04140b2.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>这里的记号不是数学中的积分符号，也不是求和，只是表示论域中各元素与其隶属度对应关系的总括。</p>
<h4 id="模糊集的运算"><a href="#模糊集的运算" class="headerlink" title="模糊集的运算"></a>模糊集的运算</h4><ul>
<li>模糊集的包含运算<ul>
<li>设A、B分别是U 上的两个模糊集，对任意u∈U，都有 μB(u) ≤ μA(u) 成立，则称A包含B，记为B≤A。</li>
</ul>
</li>
<li>模糊集的交、并、补运算<ul>
<li>设A、B分别是U上的两个模糊集，则A和B两个集合的并集A∪B、交集A∩B和A的补集﹁A的隶属函数分别为：</li>
</ul>
</li>
</ul>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d043ac6a.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<hr>
<p>【例子】<br>设U={u1,u2,u3}，<br>A=0.3/u1+0.8/u2+0.6/u3;    B=0.6/u1+0.4/u2+0.7/u3<br>求A∩B，A∪B和¬A。<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A∩B = (<span class="number">0.3</span>∧<span class="number">0.6</span>)/u1+(<span class="number">0.8</span>∧<span class="number">0.4</span>)/u2+(<span class="number">0.6</span>∧<span class="number">0.7</span>)/u3</span><br><span class="line">    = <span class="number">0.3</span>/u1+<span class="number">0.4</span>/u2+<span class="number">0.6</span>/u3</span><br><span class="line">A∪B = (<span class="number">0.3</span>∨<span class="number">0.6</span>)/u1+(<span class="number">0.8</span>∨<span class="number">0.4</span>)/u2+(<span class="number">0.6</span>∨<span class="number">0.7</span>)/u3</span><br><span class="line">    = <span class="number">0.6</span>/u1+<span class="number">0.8</span>/u2+<span class="number">0.7</span>/u3</span><br><span class="line">¬A  = (<span class="number">1</span><span class="number">-0.3</span>)/u1+(<span class="number">1</span><span class="number">-0.8</span>)/u2+(<span class="number">1</span><span class="number">-0.6</span>)/u3</span><br><span class="line">    = <span class="number">0.7</span>/u1+<span class="number">0.2</span>/u2+<span class="number">0.4</span>/u3</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="模糊关系"><a href="#模糊关系" class="headerlink" title="模糊关系"></a>模糊关系</h3><h4 id="模糊关系的定义"><a href="#模糊关系的定义" class="headerlink" title="模糊关系的定义"></a>模糊关系的定义</h4><ul>
<li><p>设 Ai 是 Ui (i=1,2,…,n) 上的模糊集，则称<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d76c6501.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>为A1, A2, …, An的笛卡尔乘积，它是U1×U2×…×Un上的一个模糊集。</p>
</li>
<li><p>在U1×…×Un上一个n元模糊关系R是指以U1×…×Un为论域的一个模糊集，记为:</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d9ecfa10.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<p>一般地说，当U和V都是有限论域时，<code>U={u1,u2,…,um}</code>，<code>V={v1,v2,…,vn}</code>，则U×V上的模糊关系R可用一个模糊矩阵表示<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717d9ee3711.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<h5 id="模糊关系的合成"><a href="#模糊关系的合成" class="headerlink" title="模糊关系的合成"></a>模糊关系的合成</h5><ul>
<li>设R1与R2分别是U×V与V×W上的两个模糊关系，则R1与R2的合成是指从U到W的一个模糊关系，记为<code>R1°R2</code></li>
<li>其隶属函数为<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717deb739dc.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>隶属函数计算方法：取R1的第 i 行元素分别与R2的第 j 列元素相比较，两个数中取其小者，然后再在所得的一组最小数中取最大的一个，以此作为R1°R2的第 i 行第 j 列的元素。</li>
</ul>
<hr>
<p>【例子】<br>设有两个模糊关系<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717e07cc3e0.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>则R1与R2的合成是<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717e07d34eb.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<h3 id="模糊变换"><a href="#模糊变换" class="headerlink" title="模糊变换"></a>模糊变换</h3><h4 id="模糊变换的概念"><a href="#模糊变换的概念" class="headerlink" title="模糊变换的概念"></a>模糊变换的概念</h4><p>设<code>A={μA(u1),μA(u2),…,μA(un)}</code>是论域U上的模糊集，R是<code>U×V</code>上的模糊关系，则<code>A°R = B</code>称为模糊变换。</p>
<p>例如：设A={0.2,0.5,0.3}<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717e5e3cbf8.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<h4 id="用模糊变换可进行模糊推理"><a href="#用模糊变换可进行模糊推理" class="headerlink" title="用模糊变换可进行模糊推理"></a>用模糊变换可进行模糊推理</h4><p>例如：设对某厨师做的一道菜进行评判<br>评判标准是：色（u1）、香（u2） 、味（u3），它们构成论域：U= { u1, u2 , u3}。<br>评判时由评委对每一个评判因素分别进行打分，评判等级是好（v1）、较好（v2） 、一般（v3）、差（v4），它们构成论域：V= {v1, v2 , v3 , v4}。<br>仅就色而言，有60%的评委认为这道菜“好”， 20%的评委认为 “较好”，20%的评委认为 “一般”，没有评委认为 “差”，则对“色”的评价为：{0.6 , 0.2, 0.2, 0}<br>对“香”的评价为：{0.8 , 0.1, 0.1, 0}<br>对“味”的评价为：{0.3 , 0.3, 0.3, 0.1}<br>这样就可以写出矩阵R：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717e8a1c5bd.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>假设三个评判因素在评判中所占的权重分别是：“色”为0.3，“香”为0.3，“味”为0.4。这三个权重组成了U上的一个模糊向量：A={0.3 , 0.3, 0.4}<br>由此可得到评委对这道菜的综合评判为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717e8a29f94.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>在此例中，评判结果的各项和刚好为1，所以它就是最终评判结果。<br>如果不是这样，还需要对其进行归一化处理，将归一化后的结果作为最终评判结果。</p>
<h3 id="模糊推理"><a href="#模糊推理" class="headerlink" title="模糊推理"></a>模糊推理</h3><h4 id="简单模糊推理-扎德法"><a href="#简单模糊推理-扎德法" class="headerlink" title="简单模糊推理(扎德法)"></a>简单模糊推理(扎德法)</h4><blockquote>
<p>知识中只含有简单条件，且不带可信度因子的模糊推理称为简单模糊推理。<br>关于如何由已知的模糊知识和证据具体地推出模糊结论，目前已经提出了多种推理方法。其中包括扎德（ L. A. Zadeh ）等人提出的合成推理规则。</p>
</blockquote>
<p>扎德：基于模糊关系合成推理的基本思想</p>
<ul>
<li>对于知识 “<code>IF x is A  THEN y is B</code>”，首先构造出A与B之间的模糊关系R；</li>
<li>再将R与证据合成，求出结论。</li>
</ul>
<p><em>合成推理规则</em>：<br>对于知识<code>IF x is A  THEN y is B</code></p>
<ul>
<li>首先构造出A与B之间的模糊关系R，然后通过R与证据的合成求出结论。</li>
<li>如果已知证据是<code>x is A’</code>，且A与A’可以模糊匹配，则通过下述合成运算求取B’：<br><code>B&#39;= A’ ◦R</code></li>
<li>如果已知证据是<code>y is B&#39;</code>，且B与B’可以模糊匹配，则通过下述合成运算求出A’：<br><code>A’ =R◦B&#39;</code></li>
</ul>
<p>至于如何构造模糊关系R：</p>
<blockquote>
<p>条件命题的极大极小规则：记获得的模糊关系为Rm</p>
</blockquote>
<p>设<code>A∈F(U)</code>, <code>B∈F(V)</code>，其表示分别为<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717eb0efe4c.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>扎德把Rm定义为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717eb0f1776.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<p>【例子】<br>设<code>U=V={1,2,3,4,5}</code>, <code>A=1/1+0.5/2</code>, <code>B=0.4/3+0.6/4+1/5</code><br>并设模糊知识为：<code>IF  x is A  THEN  y is B</code><br>模糊证据为：<code>x is A’</code><br>其中， A’的模糊集为： <code>A’ =1/1+0.4/2+0.2/3</code></p>
<p>则：由模糊知识可得到 Rm<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717ecf19ec3.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>求B’：B’= A’ ◦R<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717ecf3a49b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>若已知证据为: y is B’<br><code>B&#39;=0.2/1+0.4/2+0.6/3+0.5/4 +0.3/5</code><br>则：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717ecf55dcc.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<h3 id="模糊计算的流程"><a href="#模糊计算的流程" class="headerlink" title="模糊计算的流程"></a>模糊计算的流程</h3><h4 id="模糊计算适用"><a href="#模糊计算适用" class="headerlink" title="模糊计算适用"></a>模糊计算适用</h4><ol>
<li>复杂且没有完整数学模型的非线性问题：可在不知晓具体模型的情况下利用经验规则求解；</li>
<li>与其它智能算法结合实现优势互补: 提供了将人类在识别、决策、理解等方面的模糊性引入机器及其控制的途径。</li>
</ol>
<h4 id="模糊计算的过程"><a href="#模糊计算的过程" class="headerlink" title="模糊计算的过程"></a>模糊计算的过程</h4><ul>
<li>模糊规则库：<ul>
<li>是专家提供的模糊规则。</li>
</ul>
</li>
<li>模糊化：<ul>
<li>是根据隶属度函数从具体的输入得到对模糊集隶属度的过程。</li>
</ul>
</li>
<li>推理方法：<ul>
<li>是从模糊规则和输入对相关模糊集的隶属度得到模糊结论的方法。</li>
</ul>
</li>
<li>去模糊化：<ul>
<li>就是将模糊结论转化为具体的、精确的输出的过程。</li>
</ul>
</li>
</ul>
<h4 id="模糊计算的一般流程："><a href="#模糊计算的一般流程：" class="headerlink" title="模糊计算的一般流程："></a>模糊计算的一般流程：</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717ecf6090f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<h2 id="神经网络系统"><a href="#神经网络系统" class="headerlink" title="神经网络系统"></a>神经网络系统</h2><h3 id="人工神经网络概述"><a href="#人工神经网络概述" class="headerlink" title="人工神经网络概述"></a>人工神经网络概述</h3><p>人工智能的各种学说</p>
<ul>
<li>符号（功能）主义：符号逻辑推理</li>
<li>联结（结构）主义：人工神经网络</li>
<li>行为主义：智能行为模拟， “模式-动作”<br>联结主义的观点：智能的寓所在大脑皮层，它由大量非线性神经元互联而成 <strong>并行处理</strong>的神经网络。<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f0d881df.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<p>对比这几种模拟方法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">神经网络：是一种对人类智能的结构模拟方法，它是通过对大量人工神经元的广泛并行互联，构造人工神经网络系统去模拟生物神经系统的智能机理的。</span><br><span class="line">进化计算：是一种对人类智能的演化模拟方法，它是通过对生物遗传和演化过程的认识，用进化算法去模拟人类智能的进化规律的。</span><br><span class="line">模糊计算：是一种对人类智能的逻辑模拟方法，它是通过对人类处理模糊现象的认知能力的认识，用模糊逻辑去模拟人类的智能行为的。</span><br></pre></td></tr></table></figure></p>
<ul>
<li>人工神经网络（ANN）是反映人脑结构及功能的一种抽象数学模型，是由大量神经元节点互连而成的复杂网络，用以模拟人类进行知识的表示与存储以及利用知识进行推理的行为。</li>
<li>简单地讲，它是一个数学模型，可以用电子线路来实现，也可以用计算机程序来模拟，是人工智能研究的一种方法。</li>
<li>人工神经网络力求从四个方面模拟人脑的智能行为： 物理结构，计算模拟，存储与操作，训练</li>
</ul>
<h3 id="人工神经网络的特性"><a href="#人工神经网络的特性" class="headerlink" title="人工神经网络的特性"></a>人工神经网络的特性</h3><ul>
<li>并行分布处理：并行结构，耐故障；</li>
<li>非线性映射：任意非线性映射能力；</li>
<li>通过训练进行学习：通过数据记录进行训练，能处理由数学模型或描述规则难以处理的问题；</li>
<li>适应与集成：自适应和信息融合能力；</li>
<li>硬件实现：快速和大规模处理能力。</li>
</ul>
<h3 id="人工神经元模型"><a href="#人工神经元模型" class="headerlink" title="人工神经元模型"></a>人工神经元模型</h3><p>MP模型是美国心理学家麦克洛奇(W.McM  ulloch)和数理逻辑学家皮茨(W.Pitts) 根据生物神经元的功能和结构，于1943年提出的一种将神经元看作二进制阈值元件的简单模型。MP模型是大多数神经网络模型的基础。<br>MP模型示意图：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f557f98f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<ul>
<li>人工神经元是仿照生物神经元提出的，神经元可以有N个输入：<br><code>x1,x2,x3,...,xN</code></li>
<li>每个输入端与神经元之间有一定的联接权值：<br><code>w1,w2,w3,...,wN</code></li>
<li>神经元总的输入为对每个输入的加权求和，同时减去阈值θ。u代表神经元的活跃值，即神经元状态：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f5581682.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>神经元的输出y是对u的映射：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f55892f1.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></li>
<li>f 称为输出函数（激励函数，激活函数），可以有很多形式：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f55bdd0b.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<p>输出函数的作用</p>
<ul>
<li>控制输入对输出的激活作用</li>
<li>对输入、输出进行函数转换</li>
<li><p>将可能无限域的输入变换成指定的有限范围内的输出</p>
</li>
<li><p>神经元的模型确定之后，一个神经网络的特性及能力主要取决于网络的拓扑结构及学习方法</p>
</li>
<li>人工神经网络（ANN）可以看成是以人工神经元为结点，用有向加权弧连接起来的有向图<ul>
<li>人工神经元就是对生物神经元的模拟</li>
<li>有向弧则是轴突—突触—树突对的模拟</li>
<li>有向弧的权值表示相互连接的两个人工神经元间相互作用的强弱。</li>
</ul>
</li>
</ul>
<h3 id="人工神经网络的分类"><a href="#人工神经网络的分类" class="headerlink" title="人工神经网络的分类"></a>人工神经网络的分类</h3><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f55c706d.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>人工神经网络模型是指对网络结构、联结权值和学习能力的总括。常用的网络模型已有数十种。例如：<br>传统的感知机模型；具有误差反向传播功能的反向传播网络模型；采用多变量插值的径向基函数网络模型；建立在统计学习理论基础上的支撑向量机网络模型；采用反馈联接方式的反馈网络模型；基于模拟退火算法的随机网络模型。</p>
<blockquote>
<p>前馈网络</p>
</blockquote>
<h4 id="单层前馈网络"><a href="#单层前馈网络" class="headerlink" title="单层前馈网络"></a>单层前馈网络</h4><p>单层前馈网络是指那种只拥有单层计算节点的前向网络。它仅含有输入层和输出层，且只有输出层的神经元是可计算节点。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717f55ced12.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>其中，输入向量为X=(x1,x2,…,xn)；输出向量为Y=(y1,y2,…,ym)；输入层各个输入到相应神经元的连接权值分别是wij，i=1,2,..,n，j=1,2,.., m。<br>若假设各神经元的阈值分别是θj，j=1,2,…,m，则各神经元的输出yj, j=1,2,..,m分别为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717fc28c821.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>其中，由所有连接权值wij构成的连接权值矩阵W为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717fc292138.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>在实际应用中，该矩阵是通过大量的训练示例学习而形成的。</p>
<h4 id="多层前馈网络"><a href="#多层前馈网络" class="headerlink" title="多层前馈网络"></a>多层前馈网络</h4><p>多层前馈网络是指那种除拥有输入、输出层外，还至少含有一个、或更多个隐含层的前馈网络。典型代表：BP网络。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c717fc2e39b5.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>隐含层作用：通过对输入层信号的加权处理，将其转移成更能被输出层接受的形式。</p>
<blockquote>
<p>反馈网络</p>
</blockquote>
<p>反馈网络是指允许采用反馈联结方式所形成的神经网络。反馈联结方式：是指一个神经元的输出可以被反馈至同层或前层的神经元。典型代表：Hopfield网络。</p>
<h4 id="前馈网络与反馈网络的区别"><a href="#前馈网络与反馈网络的区别" class="headerlink" title="前馈网络与反馈网络的区别"></a>前馈网络与反馈网络的区别</h4><ul>
<li>前馈网络：非循环连接模式，每个神经元的输入都没有包含该神经元先前的输出，因此不具有“短期记忆”的性质。</li>
<li>反馈网络：每个神经元的输入都有可能包含有该神经元先前输出的反馈信息，这就有点类似于人类的短期记忆的性质。</li>
</ul>
<h3 id="传统的感知器模型"><a href="#传统的感知器模型" class="headerlink" title="传统的感知器模型"></a>传统的感知器模型</h3><blockquote>
<p>感知器是美国学者罗森勃拉特（Rosenblatt）于1957年为研究大脑的存储、学习和认知过程而提出的一类具有自学习能力的神经网络模型，其拓扑结构是一种分层前向网络。包括：单层感知器和多层感知器。</p>
</blockquote>
<p>使用感知器的主要目的是 <strong>为了对外部输入进行分类</strong>。<br>罗森勃拉特已经证明，如果外部输入是线性可分的（指存在一个超平面可以将它们分开），则单层感知器一定能够把它划分为两类。其判别超平面由如下判别式确定：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718058bf8bc.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>作为例子，下面讨论用单个感知器实现逻辑运算的问题。</p>
<h4 id="单层感知器"><a href="#单层感知器" class="headerlink" title="单层感知器"></a>单层感知器</h4><p>单层感知器是一种只具有单层可调节连接权值神经元的前向网络，这些神经元构成了单层感知器的输出层，是感知器的可计算节点。</p>
<ul>
<li>在单层感知器中，每个可计算节点都是一个线性阈值神经元。当输入信息的加权和大于或等于阈值时，输出为1，否则输出为0或-1。</li>
<li>单层感知器的输出层的每个神经元都只有一个输出，且该输出仅与本神经元的输入及联接权值有关，而与其他神经元无关。</li>
<li>单层感知器可以很好地实现“与”、“或”、“非”运算，但却 <strong>不能解决“异或”问题</strong>。<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71811825fda.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<h4 id="多层感知器"><a href="#多层感知器" class="headerlink" title="多层感知器"></a>多层感知器</h4><p>多层感知器是通过在单层感知器的输入、输出层之间加入一层或多层处理单元所构成的。</p>
<ul>
<li>拓扑结构与多层前馈网络相似，差别在于其计算节点的连接权值是可变的。</li>
<li>输入与输出呈现高度非线性的映射关系。</li>
</ul>
<h3 id="BP网络模型"><a href="#BP网络模型" class="headerlink" title="BP网络模型"></a>BP网络模型</h3><blockquote>
<p>误差反向传播(Error Back Propagation)网络简称为BP网络，是由美国加州大学的鲁梅尔哈特和麦克莱兰于1985年提出的一种网络模型。</p>
</blockquote>
<p>在BP网络中，同层节点之间不存在相互连接，层与层之间多采用全互连方式，且各层的连接权值可调。BP网络实现了明斯基的多层网络的设想，是当今神经网络模型中使用最广泛的一种。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7181181ddcc.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>如上图所示，BP网络的网络拓扑结构是多层前向网络。</p>
<p>对BP网络需说明以下两点：<br>第一，BP网络的每个处理单元均为非线性输入/输出关系，其作用函数通常采用的是可微的Sigmoid函数，如：<br>！<a href="https://i.loli.net/2019/02/24/5c7181177b6c6.jpg" target="_blank" rel="noopener"></a><br>第二，BP网络的学习过程是由工作信号的正向传播和误差信号的反向传播组成的。所谓正向传播，是指输入模式经隐层到输出层，最后形成输出模式；所谓误差反向传播，是指从输出层开始逐层将误差传到输入层，并修改各层联接权值，使误差信号为最小的过程。</p>
<h3 id="BP网络模型-1"><a href="#BP网络模型-1" class="headerlink" title="BP网络模型"></a>BP网络模型</h3><blockquote>
<p>Hopfield网络是由美国加州工学院物理学家霍普菲尔特1982年提出来的一种单层全互连的对称反馈网络模型。分为离散Hopfield网络和连续Hopfield网络。</p>
</blockquote>
<p>离散Hopfield网络是在非线性动力学的基础上由若干基本神经元构成的一种单层全互连网络，其任意神经元之间均有连接，并且是一种对称连接结构。<br>离散Hopfield网络模型是一个离散时间系统，每个神经元只有0和1（或-1和1）两种状态，任意神经元i和j之间的连接权值为Wij。由于神经元之间为对称连接，且神经元自身无连接，因此有<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7181178fc99.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>由该连接权值所构成的连接矩阵是一个零对角的对称矩阵。</p>
<p>在 Hopfield网络中，虽然神经元自身无连接，但由于每个神经元都与其他神经元相连，即每个神经元的输出都将通过突触连接权值传递给别的神经元，同时每个神经元又都接受其他神经元传来的信息，这样对每个神经元来说，其输出经过其他神经元后又有可能反馈给自己，因此Hopfidld网络是一种反馈神经网络 。</p>
<hr>
<p>【例子】<br>已知网络结构如图所示，网络输入输出路标所示。其中，f(x)为x的符号函数<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71811791800.jpg)，bias取常数1，设初始值随机取成(0.75,0.5,-0.6" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>。利用误差传播学习算法调整神经网络权值。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c71811823ac3.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<p>解题过程：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718118556a9.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<hr>
<h2 id="机器学习系统"><a href="#机器学习系统" class="headerlink" title="机器学习系统"></a>机器学习系统</h2><h3 id="机器学习的基本概念"><a href="#机器学习的基本概念" class="headerlink" title="机器学习的基本概念"></a>机器学习的基本概念</h3><ul>
<li>机器学习是人工智能的核心，通过使机器模拟人类学习行为，智能化地从过去的经历中获得经验，从而改善其整体性能，重组内在知识结构，并对未知事件进行准确的推断。</li>
<li>机器学习在科学和工程诸多领域都有着非常广泛的应用，例如金融分析、数据挖掘、生物信息学、医学诊断等。生活中常见的一些智能系统也广泛使用机器学习算法，例如电子商务、手写输入、邮件过滤等。</li>
</ul>
<blockquote>
<p>机器学习的定义:<br>西蒙（Simon,1983）：学习就是系统中的适应性变化，这种变化使系统在重复同样工作或类似工作时，能够做得更好。</p>
<p>明斯基（Minsky,1985）：学习是在人们头脑里（心理内部）有用的变化。</p>
<p>学习是一个有特定目的知识获取和能力增长过程，其内在行为是获得知识、积累经验、发现规律等，其外部表现是改进性能、适应环境、实现自我完善等。</p>
<p>机器学习是研究如何使用机器来模拟人类学习活动的一门学科。</p>
</blockquote>
<p>进入21世纪，机器学习的阶段研究热点：<br>2000-2006年的流形学习、2006-2011年的稀疏学习、2012年-至今的深度学习、未来：迁移学习？</p>
<h3 id="机器学习与深度学习的关系"><a href="#机器学习与深度学习的关系" class="headerlink" title="机器学习与深度学习的关系"></a>机器学习与深度学习的关系</h3><ul>
<li><strong>机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术</strong>。深度学习本来并不是一种独立的学习方法，其本身也会用到监督和无监督的学习方法来训练深度神经网络。</li>
<li>最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。</li>
</ul>
<p>提一下，深度学习目前存在以下问题</p>
<ol>
<li>深度学习模型需要大量的训练数据，才能展现出神奇的效果，但现实生活中往往会遇到小样本问题，此时深度学习方法无法下手，传统的机器学习方法就可以处理。</li>
<li>有些领域，采用传统的简单的机器学习方法，可以很好地解决了，没必要非得用复杂的深度学习方法。</li>
<li>深度学习的思想，来源于人脑的启发，但绝不是人脑的模拟。举个例子，一个三岁的小孩看一辆自行车之后，再见到哪怕外观完全不同的自行车，小孩也十有八九就能做出那是一辆自行车的判断，也就是说，人类的学习过程往往不需要大规模的训练数据，而现在的深度学习方法显然不是对人脑的模拟。</li>
</ol>
<h3 id="机器学习策略与基本结构"><a href="#机器学习策略与基本结构" class="headerlink" title="机器学习策略与基本结构"></a>机器学习策略与基本结构</h3><h4 id="机器学习的主要策略"><a href="#机器学习的主要策略" class="headerlink" title="机器学习的主要策略"></a>机器学习的主要策略</h4><ul>
<li>按照学习中使用推理的多少，机器学习所采用的策略大体上可分为4种：<ul>
<li>机械学习：记忆学习方法，即把新的知识存储起来，供需要时检索调用，而不需要计算和推理。</li>
<li>示教学习：外界输入知识与内部知识的表达不完全一致，系统在接受外部知识时需要推理、翻译和转化。</li>
<li>类比学习：需要发现当前任务与已知知识的相似之处，通过类比给出完成当前任务的方案。</li>
<li>示例学习：需要从一组正例和反例中分析和总结出一般性的规律，在新的任务中推广、验证、修改规律。</li>
</ul>
</li>
</ul>
<h4 id="学习系统的基本结构"><a href="#学习系统的基本结构" class="headerlink" title="学习系统的基本结构"></a>学习系统的基本结构</h4><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197adb27.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>影响学习系统设计的要素<ul>
<li>环境：环境向系统提供信息的水平（一般化程度）和质量（正确性）</li>
<li>知识库：表达能力，易于推理，容易修改，知识表示易于扩展。</li>
</ul>
</li>
</ul>
<h3 id="归纳学习"><a href="#归纳学习" class="headerlink" title="归纳学习"></a>归纳学习</h3><ul>
<li>归纳学习是应用归纳推理进行学习的一种方法。</li>
<li><p>归纳学习的模式：</p>
<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197b3bbf.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<p>实验规划过程通过对实例空间的搜索完成实例选择，并将这些选中拿到的活跃实例提交给解释过程。解释过程对实例加以适当转换，把活跃实例变换为规则空间中的特定概念，以引导规则空间的搜索。</p>
</li>
<li><p>归纳学习是目前研究得最多的学习方法，其学习目的是为了获得新概念、构造新规则或发现新理论。</p>
</li>
<li>根据归纳学习有无教师指导，可把它分为<ul>
<li>示例学习：给学习者提供某一概念的一组正例和反例，学习者归纳出一个总的概念描述（规则），并使这个描述适合于所有的正例，排除所有的反例。</li>
<li>观察发现学习：<ul>
<li>概念聚类：按照一定的方式和准则分组，归纳概念</li>
<li>机器发现：从数据和事例中发现新知识</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="类比学习"><a href="#类比学习" class="headerlink" title="类比学习"></a>类比学习</h3><ul>
<li>类比学习（learning by analogy）就是通过类比，即通过对相似事物加以比较所进行的一种学习 。</li>
<li>类比学习是利用二个不同领域（源域、目标域）中的知识相似性，可以通过类比，从源域的知识（包括相似的特征和其它性质）推导出目标域的相应知识，从而实现学习。例如：<ul>
<li>一个从未开过truck的司机，只要他有开car的知识就可完成开truck的任务。</li>
<li>若把某个人比喻为消防车，则可通过观察消防车的行为，推断出这个人的性格。</li>
</ul>
</li>
<li>类比学习系统可以使一个已有的计算机应用系统转变为适应于新的领域，来完成原先没有设计的相类似的功能。</li>
<li><p>类比推理过程：</p>
<ul>
<li>回忆与联想：找出当前情况的相似情况</li>
<li>选择：选择最相似的情况及相关知识</li>
<li>建立对应关系：建立相似元素之间的映射</li>
<li>转换：求解问题或产生新的知识</li>
</ul>
</li>
<li><p>类比学习研究类型</p>
<ul>
<li>问题求解型的类比学习：求解一个新问题时，先回忆以前是否求解过类似问题，若是，则以此为依据求解新问题。</li>
<li>预测推理型的类比学习<ul>
<li>传统的类比法：用来推断一个不完全确定的事物可能还有的其他属性</li>
<li>因果关系型：已知因果关系<code>S1:A-&gt;B</code>，如果有<code>A&#39;≌A</code>，则可能有B’满足<code>A&#39;-&gt;B&#39;</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="解释学习"><a href="#解释学习" class="headerlink" title="解释学习"></a>解释学习</h3><blockquote>
<p>解释学习(Explanation-based learning, EBL)兴起于20世纪80年代中期，根据任务所在领域知识和正在学习的概念知识，对当前实例进行分析和求解，得出一个表征求解过程的因果解释树，以获取新的知识。<br>例如：学生根据教师提供的目标概念、该概念的一个例子、领域理论及可操作准则，首先构造一个解释来说明为什么该例子满足目标概念，然后将解释推广为目标概念的一个满足可操作准则的充分条件。</p>
</blockquote>
<p>解释学习过程和算法<br>米切尔提出了一个解释学习的统一算法EBG，建立了基于解释的概括过程，并用知识的逻辑表示和演绎推理进行问题求解。其一般性描述为：</p>
<ul>
<li>给定：领域知识DT、目标概念TC、训练实例TE、操作性准则OC</li>
<li>找出：满足OC的关于TC的充分条件<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7182479a4b4.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
</li>
</ul>
<p>EBG算法可概括为两步：</p>
<ol>
<li>构造解释：运用领域知识进行演绎，证明提供给系统的训练实例为什么是满足目标概念的一个实例。</li>
<li>获取一般性的知识：<ul>
<li>任务：对上一步得到的解释结构进行一般化的处理，从而得到关于目标概念的一般性知识。</li>
<li>方法：将常量换成变量，并把某些不重要的信息去掉，只保留求解问题必须的关键信息。</li>
</ul>
</li>
</ol>
<h3 id="神经网络学习-ppt-21-32"><a href="#神经网络学习-ppt-21-32" class="headerlink" title="神经网络学习(ppt-21-32)"></a>神经网络学习(ppt-21-32)</h3><h4 id="Hebb学习"><a href="#Hebb学习" class="headerlink" title="Hebb学习"></a>Hebb学习</h4><h4 id="纠错学习"><a href="#纠错学习" class="headerlink" title="纠错学习"></a>纠错学习</h4><h4 id="竞争学习及随机学习"><a href="#竞争学习及随机学习" class="headerlink" title="竞争学习及随机学习"></a>竞争学习及随机学习</h4><h4 id="感知器学习"><a href="#感知器学习" class="headerlink" title="感知器学习"></a>感知器学习</h4><p>单层感知器学习算法可描述如下：<br>(1)、设t=0，初始化连接权和阈值。即给wi(0)(i=1, 2, … ,n)及θ(0)分别赋予一个较小的非零随机数，作为初值。其中，wi(0)是第0次迭代时输入向量中第i个输入的连接权值；θ(0)是第0次迭代时输出节点的阈值；<br>(2)、提供新的样本输入xi(t)(i=1, 2, … , n)和期望输出d(t)；<br>(3)、计算网络的实际输出：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197b9052.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>(4)、若y(t)=d(t)，不需要调整连接权值，转(6)。否则，需要调整权值；<br>(5)、调整连接权值；<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197bbddb.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>其中，η是一个增益因子，用于控制修改速度，其值如果太大，会影响wi(t)的收敛性；如果太小，又会使wi(t)的收敛速度太慢;<br>(6)、判断是否满足结束条件，若满足，算法结束；否则，将t值加1，转(2)重新执行。这里的结束条件一般是指wi(t)对一切样本均稳定不变。<br>若输入的两类样本是线性可分的，则该算法就一定会收敛。否则，不收敛。</p>
<hr>
<p>【例子】用单层感知器实现逻辑“与”运算。</p>
<p>解：根据“与”运算的逻辑关系，可将问题转换为：<br>输入向量：<br><code>X1=[0, 0, 1, 1]</code><br><code>X2=[0, 1, 0, 1]</code><br>输出向量：<br><code>Y=[0, 0, 0, 1]</code><br>为减少算法的迭代次数，设初始连接权值和阈值取值如下：<br><code>w1(0)=0.5</code>,   <code>w2(0)=0.7</code>,   <code>θ(0)=0.6</code><br>并取增益因子<code>η=0.4</code>。</p>
<p>算法的学习过程如下：<br>设两个输入为<code>x1(0)=0</code>和<code>x2(0)=0</code>，其期望输出为<code>d(0)=0</code>，实际输出为：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y(<span class="number">0</span>)=f(w1(<span class="number">0</span>)x1(<span class="number">0</span>)+ w2(<span class="number">0</span>)x2(<span class="number">0</span>)-θ(<span class="number">0</span>))</span><br><span class="line">    =f(<span class="number">0.5</span>*<span class="number">0</span>+<span class="number">0.7</span>*<span class="number">0</span><span class="number">-0.6</span>)</span><br><span class="line">    =f(<span class="number">-0.6</span>)</span><br><span class="line">    =<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>实际输出与期望输出相同，不需要调节权值。<br>再取下一组输入：<code>x1(0)=0</code>和<code>x2(0)=1</code>，期望输出<code>d(0)=0</code>，实际输出：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y(<span class="number">0</span>)=f(w1(<span class="number">0</span>) x1(<span class="number">0</span>)+ w2(<span class="number">0</span>) x2(<span class="number">0</span>)-θ(<span class="number">0</span>))</span><br><span class="line">    =f(<span class="number">0.5</span>*<span class="number">0</span>+<span class="number">0.7</span>*<span class="number">1</span><span class="number">-0.6</span>)</span><br><span class="line">    =f(<span class="number">0.1</span>)</span><br><span class="line">    =<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<p>实际输出与期望输出不同，需要调节权值，其调整如下：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">θ(<span class="number">1</span>)=θ(<span class="number">0</span>)+η(d(<span class="number">0</span>)- y(<span class="number">0</span>))*(<span class="number">-1</span>)=<span class="number">0.6</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*(<span class="number">-1</span>)=<span class="number">1</span></span><br><span class="line">w1(<span class="number">1</span>)=w1(<span class="number">0</span>)+η(d(<span class="number">0</span>)- y(<span class="number">0</span>))x1(<span class="number">0</span>)=<span class="number">0.5</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*<span class="number">0</span>=<span class="number">0.5</span></span><br><span class="line">w2(<span class="number">1</span>)=w2(<span class="number">0</span>)+η(d(<span class="number">0</span>)- y(<span class="number">0</span>))x2(<span class="number">0</span>)=<span class="number">0.7</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*<span class="number">1</span>=<span class="number">0.3</span></span><br></pre></td></tr></table></figure></p>
<p>取下一组输入：<code>x1(1)=1</code>和<code>x2(1)=0</code>，其期望输出为<code>d(1)=0</code>，实际输出为：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y(<span class="number">1</span>)=f(w1(<span class="number">1</span>) x1(<span class="number">1</span>)+ w2(<span class="number">1</span>) x2(<span class="number">1</span>)-θ(<span class="number">1</span>))</span><br><span class="line">      =f(<span class="number">0.5</span>*<span class="number">1</span>+<span class="number">0.3</span>*<span class="number">0</span><span class="number">-1</span>)</span><br><span class="line">      =f(<span class="number">-0.51</span>)</span><br><span class="line">      =<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>实际输出与期望输出相同，不需要调节权值。<br>再取下一组输入：<code>x1(1)=1和x2(1)=1</code>，其期望输出为<code>d(1)=1</code>，实际输出为：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y(<span class="number">1</span>)=f(w1(<span class="number">1</span>) x1(<span class="number">1</span>)+ w2(<span class="number">1</span>) x2(<span class="number">1</span>)-θ(<span class="number">1</span>))</span><br><span class="line">    =f(<span class="number">0.5</span>*<span class="number">1</span>+<span class="number">0.3</span>*<span class="number">1</span><span class="number">-1</span>)</span><br><span class="line">    =f(<span class="number">-0.2</span>)</span><br><span class="line">    =<span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>实际输出与期望输出不同，需要调节权值，其调整如下：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">θ(<span class="number">2</span>)=θ(<span class="number">1</span>)+η(d(<span class="number">1</span>)- y(<span class="number">1</span>))*(<span class="number">-1</span>)=<span class="number">1</span>+<span class="number">0.4</span>*(<span class="number">1</span><span class="number">-0</span>)*(<span class="number">-1</span>)=<span class="number">0.6</span></span><br><span class="line">w1(<span class="number">2</span>)=w1(<span class="number">1</span>)+η(d(<span class="number">1</span>)- y(<span class="number">1</span>))x1(<span class="number">1</span>)=<span class="number">0.5</span>+<span class="number">0.4</span>*(<span class="number">1</span><span class="number">-0</span>)*<span class="number">1</span>=<span class="number">0.9</span></span><br><span class="line">w2(<span class="number">2</span>)=w2(<span class="number">1</span>)+η(d(<span class="number">1</span>)- y(<span class="number">1</span>))x2(<span class="number">1</span>)=<span class="number">0.3</span>+<span class="number">0.4</span>*(<span class="number">1</span><span class="number">-0</span>)*<span class="number">1</span>=<span class="number">0.7</span></span><br></pre></td></tr></table></figure></p>
<p>取下一组输入：<code>x1(2)=0</code>和<code>x2(2)=0</code>，其期望输出为<code>d(2)=0</code>，实际输出为：<br><code>y(2)=f(0.9*0+0.7*0-0.6)=f(-0.6)=0</code><br>实际输出与期望输出相同，不需要调节权值.<br>再取下一组输入：<code>x1(2)=0</code>和<code>x2(2)=1</code>，期望输出为<code>d(2)=0</code>，实际输出为：<br><code>y(2)=f(0.9*0+0.7*1-0.6)=f(0.1)=1</code><br>实际输出与期望输出不同，需要调节权值，其调整如下：<br><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">θ(<span class="number">3</span>)=θ(<span class="number">2</span>)+η(d(<span class="number">2</span>)- y(<span class="number">2</span>))*(<span class="number">-1</span>)=<span class="number">0.6</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*(<span class="number">-1</span>)=<span class="number">1</span></span><br><span class="line">w1(<span class="number">3</span>)=w1(<span class="number">2</span>)+η(d(<span class="number">2</span>)- y(<span class="number">2</span>))x1(<span class="number">2</span>)=<span class="number">0.9</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*<span class="number">0</span>=<span class="number">0.9</span></span><br><span class="line">w2(<span class="number">3</span>)=w2(<span class="number">2</span>)+η(d(<span class="number">2</span>)- y(<span class="number">2</span>))x2(<span class="number">2</span>)=<span class="number">0.7</span>+<span class="number">0.4</span>*(<span class="number">0</span><span class="number">-1</span>)*<span class="number">1</span>=<span class="number">0.3</span></span><br></pre></td></tr></table></figure></p>
<p>实际上，由与运算的阈值条件可知，此时的阈值和连接权值以满足结束条件，算法可以结束。<br>对此，可检验如下：<br>对输入：“0 0”有<code>y=f(0.9*0+0.3*0-1)=f(-1)=0</code><br>对输入：“0 1”有<code>y=f(0.9*0+0.3*0.1-1)=f(-0.7)=0</code><br>对输入：“1 0”有<code>y=f(0.9*1+0.3*0-1)=f(-0.1)=0</code><br>对输入：“1 1”有<code>y=f(0.9*1+0.3*1-1)=f(0.2)=1</code></p>
<hr>
<p>多层感知器可以解决非线性可分问题，但其隐层神经元的期望输出却不易给出。<br>而单层感知器学习是一种有导师指导的学习过程，因此其学习算法无法直接用于多层感知器。<br>由于多层感知器和BP网络都属于前向网络，并能较好解决多层前馈网络的学习问题.<br>因此，可用BP学习来解决多层感知器学习问题。</p>
<h4 id="BP网络学习"><a href="#BP网络学习" class="headerlink" title="BP网络学习"></a>BP网络学习</h4><h4 id="Hopfield网络学习"><a href="#Hopfield网络学习" class="headerlink" title="Hopfield网络学习"></a>Hopfield网络学习</h4><h3 id="其他机器学习方法"><a href="#其他机器学习方法" class="headerlink" title="其他机器学习方法"></a>其他机器学习方法</h3><ul>
<li>迁移学习：将在先前任务中学到的知识或技能应用于一个新的任务或新的领域</li>
<li>增强机器学习：从变化环境中学习蕴含在环境中的知识</li>
<li>流形机器学习：把一组在高维空间中的数据在低维空间中重新表示</li>
<li>半监督机器学习：结合标记和非标记样本</li>
<li>多实例机器学习：一个对象可能同时有多个描述</li>
<li>Ranking机器学习：获得关于检索中“喜欢”顺序的模型</li>
<li>数据流机器学习：从数据流中发现知识</li>
<li>……</li>
</ul>
<p>相关的一些概念：决策树、 随机森林、逻辑回归、朴素贝叶斯、K最近临算法、马尔可夫……</p>
<h2 id="仿生进化系统-GA"><a href="#仿生进化系统-GA" class="headerlink" title="仿生进化系统(GA)"></a>仿生进化系统(GA)</h2><blockquote>
<p>遗传算法最早由美国密西根大学的J. Holland 教授提出，起源于20世纪60年代对自然和人工自适应系统的研究。70年代，De Jong 基于遗传算法的思想在计算机上进行了大量的纯数值函数优化计算实验。在一系列研究工作的基础上，80年代由Goldberg进行归纳总结，形成了遗传算法的基本框架</p>
</blockquote>
<h3 id="遗传算法的定义"><a href="#遗传算法的定义" class="headerlink" title="遗传算法的定义"></a>遗传算法的定义</h3><p>遗传算法（Genetic Algorithm, GA）是模拟生物在自然环境种的遗传和进化过程而形成的一种自适应全局优化概率搜索算法。</p>
<h3 id="遗传算法的基本思想"><a href="#遗传算法的基本思想" class="headerlink" title="遗传算法的基本思想"></a>遗传算法的基本思想</h3><p>是从初始种群出发，采用优胜劣汰、适者生存的自然法则选择个体，并通过杂交、变异来产生新一代种群，如此逐代进化，直到满足目标为止。</p>
<h3 id="遗传算法的基本过程"><a href="#遗传算法的基本过程" class="headerlink" title="遗传算法的基本过程"></a>遗传算法的基本过程</h3><blockquote>
<p>算法主要内容和基本步骤</p>
</blockquote>
<p>(1) 选择编码策略，将问题搜索空间中每个可能的点用相应的编码策略表示出来，即形成染色体；<br>(2) 定义遗传策略，包括种群规模N，交叉、变异方法，以及选择概率Pr、交叉概率Pc、变异概率Pm等遗传参数；<br>(3) 令t=0，随机选择N个染色体初始化种群P(0)；<br>(4) 定义适应度函数f（f&gt;0）；<br>(5) 计算P(t)中每个染色体的适应值；<br>(6) t=t+1；<br>(7) 运用选择算子，从P(t-1)中得到P(t)；<br>(8) 对P(t)中的每个染色体，按概率Pc参与交叉；<br>(9) 对染色体中的基因，以概率Pm参与变异运算；<br>(10) 判断群体性能是否满足预先设定的终止标准，若不满足则返回(5)。</p>
<blockquote>
<p>常用的遗传编码算法</p>
</blockquote>
<ul>
<li>霍兰德二进制码<ul>
<li>二进制编码是将原问题的结构变换为染色体的位串结构。在二进制编码中，首先要确定二进制字符串的长度    ，该长度与变量的定义域和所求问题的计算精度有关。</li>
</ul>
</li>
<li>格雷编码（Gray Code）<ul>
<li>格雷编码是对二进制编码进行变换后所得到的一种编码方法。这种编码方法要求两个连续整数的编码之间只能有一个码位不同，其余码位都是完全相同的。它有效地解决了汉明悬崖问题。</li>
</ul>
</li>
<li>实数编码<ul>
<li>实数编码是将每个个体的染色体都用某一范围的一个实数（浮点数）来表示，其编码长度等于该问题变量的个数。</li>
<li>这种编码方法是将问题的解空间映射到实数空间上，然后在实数空间上进行遗传操作。由于实数编码使用的是变量的真实值，因此这种编码方法也叫做真值编码方法。</li>
<li>实数编码适应于那种多维、高精度要求的连续函数优化问题。</li>
</ul>
</li>
<li>字符编码</li>
</ul>
<blockquote>
<p>适应度函数</p>
</blockquote>
<p>适应度函数是一个用于对个体的适应性进行度量的函数。通常，一个个体的适应度值越大，它被遗传到下一代种群中的概率也就越大。</p>
<p>(1) 常用的适应度函数</p>
<ul>
<li>原始适应度函数：直接将待求解问题的目标函数f(x)定义为遗传算法的适应度函数。<ul>
<li>优点：能够直接反映出待求解问题的最初求解目标</li>
<li>缺点：是有可能出现适应度值为负的情况。</li>
</ul>
</li>
<li>标准适应度函数<ul>
<li>在遗传算法中，一般要求适应度函数非负，并其适应度值越大越好。这就往往需要对原始适应函数进行某种变换，将其转换为标准的度量方式，以满足进化操作的要求，这样所得到的适应度函数被称为标准适应度函数。<br>(2) 适应度函数的加速变换</li>
</ul>
</li>
<li>在某些情况下，需要对适应度函数进行加速速度。</li>
<li><p>适应度函数的加速变换有两种基本方法，即线性加速与非线性加速。</p>
</li>
<li><p><strong>线性加速</strong></p>
<ul>
<li>线性加速的适应度函数的定义如下：</li>
<li><code>f&#39;(x)=αf(x)+β</code></li>
<li>其中，f(x)是加速转换前的适应度函数；f’(x)是加速转换后的适应度函数；α和β是转换系数,它们应满足如下条件：<ul>
<li>变化后得到的新的适应度函数平均值要等于原适应度函数的平均值。即<br><img src="https://i.loli.net/2019/02/24/5c718197d1e7f.jpg" alt></li>
<li>其中，<code>xi(i=1,…,n)</code>为当前代中的染色体。</li>
<li>变换后所得到的新的种群个体所具有的最大适应度要等于其平均适应度的指数倍数。即有关系：<br><img src="https://i.loli.net/2019/02/24/5c718197cc03b.jpg" alt></li>
<li>式中，<code>xi(i=1,…,n)</code>为当前代中的染色体，M是指将当前的最大适应度放大为平均值的M倍。目的是通过M拉开不同染色体适应度值的差距。</li>
</ul>
</li>
</ul>
</li>
<li><strong>非线性加速</strong><ul>
<li>幂函数变换方法<ul>
<li><code>f&#39;(x)=f(x)k</code></li>
</ul>
</li>
<li>指数变换方法<ul>
<li><code>f&#39;(x)=exp(-βf(x))</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>基本遗传操作</p>
</blockquote>
<p>(1)、选择操作<br>选择（Selection）操作是指根据选择概率按某种策略从当前种群中挑选出一定数目的个体，使它们能够有更多的机会被遗传到下一代中。<br>常用的选择策略:</p>
<ul>
<li>比例选择</li>
<li>排序选择</li>
<li>竞技选择<ul>
<li>其中比例选择基本思想是：各个个体被选中的概率与其适应度大小成正比。</li>
<li>常用的比例选择策略:轮盘赌选择、繁殖池选择</li>
<li>轮盘赌选择<br>轮盘赌选择法又被称为转盘赌选择法或轮盘选择法。在这种方法中，个体被选中的概率取决于该个体的相对适应度。而相对适应度的定义为：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197cdd40.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
其中，P(xi)是个体xi的相对适应度，即个体xi被选中的概率；f(xi)是个体xi的原始适应度；是种群的累加适应度。</li>
</ul>
</li>
</ul>
<p>轮盘赌选择算法的基本思想是：根据每个个体的选择概率P(xi)将一个圆盘分成N个扇区，其中第i个扇区的中心角为：<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197d633d.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure><br>再设立一个移动指针，将圆盘的转动等价为指针的移动。选择时，假想转动圆盘，若静止时指针指向第i个扇区，则选择个体i。<br>从统计角度看，个体的适应度值越大，其对应的扇区的面积越大，被选中的可能性也越大。这种方法有点类似于发放奖品使用的轮盘，并带有某种赌博的意思，因此亦被称为轮盘赌选择。</p>
<p>(2)、交叉操作</p>
<ul>
<li>交配重组是自然界中生物遗传进化的一个主要环节，也是遗传算法中产生新的个体的最主要方法。</li>
<li>交叉（Crossover）操作是指按照某种方式对选择的父代个体的染色体的部分基因进行交配重组，从而形成新的个体。</li>
<li>根据个体编码方法的不同，遗传算法中的交叉操作可分为二进制交叉和实值交叉两种类型。<ul>
<li>二进制交叉</li>
<li>二进制交叉（Binary Valued Crossover）是指二进制编码情况下所采用的交叉操作，它主要包括单点交叉、两点交叉、多点交叉和均匀交叉等方法。<ul>
<li>单点交叉<ul>
<li>单点交叉也称简单交叉，它是先在两个父代个体的编码串中随机设定一个交叉点，然后对这两个父代个体交叉点前面或后面部分的基因进行交换，并生成子代中的两个新的个体。假设两个父代的个体串分别是：</li>
<li><code>X=x1 x2 … xk xk+1 … xn</code></li>
<li><code>Y=y1 y2 … yk yk+1 … yn</code></li>
<li>随机选择第k位为交叉点，若采用对交叉点后面的基因进行交换的方法，但点交叉是将X中的xk+1到xn部分与Y中的yk+1到yn部分进行交叉，交叉后生成的两个新的个体是：</li>
<li><code>X&#39;= x1 x2 … xk yk+1 … yn</code></li>
<li><code>Y&#39;= y1 y2 … yk xk+1 … xn</code></li>
</ul>
</li>
<li>两点交叉<ul>
<li>两点交叉是指先在两个父代个体的编码串中随机设定两个交叉点，然后再按这两个交叉点进行部分基因交换，生成子代中的两个新的个体。</li>
<li>假设两个父代的个体串分别是：</li>
<li><code>X=x1 x2 … xi … xj … xn</code></li>
<li><code>Y=y1 y2 … yi … yj …,yn</code></li>
<li>随机设定第i、j位为两个交叉点（其中<code>i&lt; j &lt; n</code>），两点交叉是将X中的xi+1到xj部分与Y中的yi+1到yj部分进行交换，交叉后生成的两个新的个体是：</li>
<li><code>X&#39;= x1 x2 … xi yi+1 … yj xj+1 … xn</code></li>
<li><code>Y&#39;= y1 y2 … yi xi+1 … xj yj+1 … yn</code></li>
</ul>
</li>
<li>多点交叉<ul>
<li>多点交叉是指先随机生成多个交叉点，然后再按这些交叉点分段地进行部分基因交换，生成子代中的两个新的个体。</li>
<li>假设交叉点个数为m，则可将个体串划分为m+1个分段，其划分方法是：<ul>
<li>当m为偶数时，对全部交叉点依次进行两两配对，构成m/2个交叉段。</li>
<li>当m为奇数时，对前(m-1)个交叉点依次进行两两配对，构成(m-1)/2个交叉段，而第m个交叉点则按单点交叉方法构成一个交叉段。</li>
</ul>
</li>
</ul>
</li>
<li>均匀交叉<ul>
<li>均匀交叉（Uniform Crossover）是先随机生成一个与父串具有相同长度，并被称为交叉模版（或交叉掩码）的二进制串，然后再利用该模版对两个父串进行交叉，即将模版中1对应的位进行交换，而0对应的位不交换，依此生成子代中的两个新的个体。事实上，这种方法对父串中的每一位都是以相同的概率随机进行交叉的。</li>
</ul>
</li>
</ul>
</li>
<li>实值交叉<ul>
<li>实值交叉是在实数编码情况下所采用的交叉操作，主要包括离散交叉和算术交叉，下面主要讨论离散交叉（部分离散交叉和整体离散交叉） 。</li>
<li>部分离散交叉是先在两个父代个体的编码向量中随机选择一部分分量，然后对这部分分量进行交换，生成子代中的两个新的个体。</li>
<li>整体交叉则是对两个父代个体的编码向量中的所有分量，都以1/2的概率进行交换，从而生成子代中的两个新的个体。</li>
<li>以部分离散交叉为例，假设两个父代个体的n维实向量分别是 <code>X=x1x2… xi…xk…xn</code>和<code>Y=y1y2…yi…yk…yn</code>，若随机选择对第k个分量以后的所有分量进行交换，则生成的两个新的个体向量是：<ul>
<li><code>X&#39;= x1 x2 … xk yk+1 … yn</code></li>
<li><code>Y&#39;= y1 y2 … yk xk+1 … xn</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>(3)、变异操作<br>变异（Mutation）是指对选中个体的染色体中的某些基因进行变动，以形成新的个体。变异也是生物遗传和自然进化中的一种基本现象，它可增强种群的多样性。遗传算法中的变异操作增加了算法的局部随机搜索能力，从而可以维持种群的多样性。根据个体编码方式的不同，变异操作可分为二进制变异和实值变异两种类型。</p>
<ul>
<li>二进制变异<ul>
<li>当个体的染色体采用二进制编码表示时，其变异操作应采用二进制变异方法。该变异方法是先随机地产生一个变异位，然后将该变异位置上的基因值由“0”变为“1”，或由“1”变为“0”，产生一个新的个体。</li>
</ul>
</li>
<li>实值变异<ul>
<li>当个体的染色体采用实数编码表示时，其变异操作应采用实值变异方法。该方法是用另外一个在规定范围内的随机实数去替换原变异位置上的基因值，产生一个新的个体。<ul>
<li>基于位置的变异方法<ul>
<li>该方法是先随机地产生两个变异位置，然后将第二个变异位置上的基因移动到第一个变异位置的前面。</li>
</ul>
</li>
<li>基于次序的变异<ul>
<li>该方法是先随机地产生两个变异位置，然后交换这两个变异位置上的基因。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="遗传算法的优势"><a href="#遗传算法的优势" class="headerlink" title="遗传算法的优势"></a>遗传算法的优势</h3><ol>
<li>适应度函数不受连续、可微等条件的约束，适用范围很广。</li>
<li>不容易陷入局部极值，能以很大的概率找到全局最优解。</li>
<li>由于其固有的并行性，适合于大规模并行计算。</li>
<li>不是盲目穷举，而是启发式搜索。</li>
</ol>
<h2 id="群智能系统"><a href="#群智能系统" class="headerlink" title="群智能系统"></a>群智能系统</h2><blockquote>
<p>由James Kenney（社会心理学博士）和<a href="http://www.engr.iupui.edu/~eberhart/" target="_blank" rel="noopener">Russ Eberhart</a>（电子工程学博士）于1995年提出粒子群算法（Particle Swarm Optimization, PSO）。</p>
</blockquote>
<h3 id="蚁群算法-Ant-Colony-Optimization-ACO"><a href="#蚁群算法-Ant-Colony-Optimization-ACO" class="headerlink" title="蚁群算法(Ant Colony Optimization,ACO)"></a>蚁群算法(Ant Colony Optimization,ACO)</h3><p>通过遗留在来往路径上的信息素（Pheromone）的挥发性化学物质来进行通信和协调。<br><figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197e8aa7.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure></p>
<ul>
<li>蚂蚁在寻找食物的过程中往往是随机选择路径的，但它们能感知当前地面上的信息素浓度，并倾向于往信息素浓度高的方向行进。信息素由蚂蚁自身释放，是实现蚁群内间接通信的物质。</li>
<li>由于较短路径上蚂蚁的往返时间比较短，单位时间内经过该路径的蚂蚁多，所以信息素的积累速度比较长路径快。因此，当后续蚂蚁在路口时，就能感知先前蚂蚁留下的信息，并倾向于选择一条较短的路径前行。</li>
<li>这种正反馈机制使得越来越多的蚂蚁在巢穴与食物之间的最短路径上行进。由于其他路径上的信息素会随着时间蒸发，最终所有的蚂蚁都在最优路径上行进。</li>
</ul>
<h4 id="ACO基本要素"><a href="#ACO基本要素" class="headerlink" title="ACO基本要素"></a>ACO基本要素</h4><ul>
<li>路径构建<br>伪随机比例选择规则<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c718197f149f.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li>对于每只蚂蚁k，路径记忆向量Rk按照访问顺序记录了所有k已经经过的城市序号。</li>
<li>设蚂蚁k当前所在城市为i，则其选择城市j作为下一个访问对象的概率如上式。Jk(i) 表示从城市i 可以直接到达的、且又不在蚂蚁访问过的城市序列Rk中的城市集合。</li>
<li><code>η(i, j)</code>是一个启发式信息，通常由<code>η (i, j)=1/dij</code> 直接计算。</li>
<li><code>τ (i, j)</code> 表示边<code>(i, j)</code>上的信息素量。</li>
<li>长度越短、信息素浓度越大的路径被蚂蚁选择的概率越大。</li>
<li>α和β是两个预先设置的参数，用来控制启发式信息与信息素浓度作用的权重关系。</li>
<li>当<code>α =0</code>时，算法演变成传统的随机贪心算法，最邻近城市被选中的概率最大。当<code>β =0</code>时，蚂蚁完全只根据信息素浓度确定路径，算法将快速收敛，这样构建出的最优路径与实际目标差异较大，算法性能较差。</li>
</ul>
</li>
<li>信息素更新：<br>(1) 在算法初始化时，问题空间中所有的边上的信息素都被初始化为τ0。<br>(2) 算法迭代每一轮，问题空间中的所有路径上的信息素都会发生蒸发，我们为所有边上的信息素乘上一个小于1的常数( ρ: 信息素的蒸发率)。信息素蒸发是自然界本身固有的特征，在算法中能够帮助避免信息素的无限积累，使得算法可以快速丢弃之前构建过的较差的路径。<br>(3) 蚂蚁根据自己构建的路径长度在它们本轮经过的边上释放信息素。蚂蚁构建的路径越短、释放的信息素就越多。一条边被蚂蚁爬过的次数越多、它所获得的信息素也越多。<br>(4) 迭代 (2)，直至算法终止。<br>信息素的更新公式：<figure class="image-bubble">
                <div class="img-lightbox">
                    <div class="overlay"></div>
                    <img src="https://i.loli.net/2019/02/24/5c7182479d604.jpg" alt title>
                </div>
                <div class="image-caption"></div>
            </figure>
<ul>
<li><code>m</code>：蚂蚁个数；</li>
<li><code>ρ</code>：信息素的蒸发率，规定<code>0&lt;r≤1</code>。</li>
<li><code>Δτ (i, j)</code>：第k只蚂蚁在它经过的边上释放的信息素量，它等于蚂蚁k本轮构建路径长度的倒数。</li>
<li><code>Ck</code>：路径长度，它是Rk中所有边的长度和。</li>
</ul>
</li>
</ul>
<h3 id="粒子群优化算法"><a href="#粒子群优化算法" class="headerlink" title="粒子群优化算法"></a>粒子群优化算法</h3><ul>
<li>源于对鸟群捕食行为的研究，是基于迭代的方法</li>
<li>简单易于实现，需要调整的参数相对较少</li>
<li>在函数优化、神经网络训练、工业系统优化和模糊系统控制等领域得到了广泛的应用。<ul>
<li>鸟群：假设一个区域，所有的鸟都不知道食物的位置，但是它们知道当前位置离食物还有多远。</li>
<li>PSO算法：每个解看作一只鸟，称为“粒子(particle)”，所有的粒子都有一个适应值，每个粒子都有一个速度决定它们的飞翔方向和距离，粒子们追随当前最优粒子在解空间中搜索。</li>
</ul>
</li>
</ul>
<h4 id="粒子群算法的特点"><a href="#粒子群算法的特点" class="headerlink" title="粒子群算法的特点"></a>粒子群算法的特点</h4><ul>
<li>PSO算法收敛速度快，特别是在算法的早期，但也存在着精度较低，易发散等缺点。</li>
<li>若加速系数、最大速度等参数太大，粒子群可能错过最优解，算法不收敛；</li>
<li>而在收敛的情况下，由于所有的粒子都向最优解的方向飞去，所以粒子趋向同一化（失去了多样性），使得后期收敛速度明显变慢，同时算法收敛到一定精度时，无法继续优化，所能达到的精度也不高。</li>
</ul>
<h3 id="其他计算智能方法"><a href="#其他计算智能方法" class="headerlink" title="其他计算智能方法"></a>其他计算智能方法</h3><p>模拟退火、工免疫系统、粗集理论、EDA算法、文化进化计算、量子计算、DNA计算、智能Agent、……</p>
<h2 id="多真体及自然语言理解"><a href="#多真体及自然语言理解" class="headerlink" title="多真体及自然语言理解 *"></a>多真体及自然语言理解 *</h2><h3 id="Agent的定义和译法"><a href="#Agent的定义和译法" class="headerlink" title="Agent的定义和译法"></a>Agent的定义和译法</h3><blockquote>
<p>Agent的定义</p>
</blockquote>
<ul>
<li>定义1.社会中某个个体经过协商后可求得问题的解，这个个体就是agent.（明斯基，1986年）</li>
<li>定义2.是一种通过传感器知其环境，并通过执行器作用于该环境的实体，因此，可以把真体定义为一种从感知序列到实体动作的映射。（Russell and Norving,1995）</li>
<li>定义3.是一种具有智能的实体。</li>
</ul>
<blockquote>
<p>Agent的译法</p>
</blockquote>
<p>建议把agent译为“(艾)真体”的理由：</p>
<ul>
<li>Agent是一种通过传感器感知其环境，并通过执行器作用于该环境的实体。 这个“实体”也可叫做“真体”。因此，可以把真体定义为一种从感知序列到实体动作的映射。</li>
<li>译为“主体”可能是考虑到agent具有自主性。但交互性、协调性、社会性、适应性和分布性等不可能在译名上全部反映出来，因而是片面的。</li>
<li>译为“代理”是受到社会科学和管理科学的影响。也不能表示出agent的原义。</li>
<li>音译不失为一种可取方法。</li>
<li>有一定的物理意义。</li>
</ul>
<h3 id="真体的要素和特性"><a href="#真体的要素和特性" class="headerlink" title="真体的要素和特性"></a>真体的要素和特性</h3><blockquote>
<p>真体的要素</p>
</blockquote>
<p>真体必须利用知识修改其内部状态（心理状态），以适应环境变化和协作求解的需要。真体的行动受其心理状态驱动。人类心理状态的要素有认知（信念、知识、学习等）、情感（愿望、兴趣、爱好等）和意向（意图、目标、规划和承诺等）三种。着重研究信念（belief）、愿望（desire）和意图（intention）的关系及其形式化描述，力图建立真体的BDI（信念、愿望和意图）模型，已成为真体理论模型研究的主要方向。</p>
<blockquote>
<p>真体的特性</p>
</blockquote>
<ul>
<li>行为自主性：能够控制自身行为，其行为是主动的、自发的/有目标和意图的，并能根据目标和环境要求对短期行为做出规划。</li>
<li>作用交互性：能够与环境交互作用，能够感知其所处环境，并借助自己的行为结果，对环境做出适当反应。</li>
<li>环境协调性：真体存在于一定的环境中，感知环境的状态、事件和特征，并通过其动作和行为影响环境，与环境保持协调。环境和真体互相依存，互相作用。</li>
<li>面向目标性：真体能够表现出某种目标指导下的行为，为实现其内在目标而采取主动行为。</li>
<li>存在社会性：真体存在于由多个真体构成的社会环境中，与其它真体交换信息、交互作用和通讯。各真体通过社会承诺，进行社会推理，实现社会意向和目标。</li>
<li>工作协调性：各真体合作和协调工作，求解单个真体无法处理的问题，提高处理问题的能力。</li>
<li>运行持续性：真体的程序在起动后，能够在相当长的一段时间内维持运行状态，不随运算的停止而立即结束运行。</li>
<li>系统适应性：真体不仅能够感知环境，对环境做出反应，而且能够把新建立的真体集成到系统中而无需对原有的多真体系统进行重新设计，因而具有很强的适应性和可扩展性。</li>
<li>结构分布性：在物理上或逻辑上分布和异构的实体（或真体），如主动数据库、知识库、控制器和执行器等，在多真体系统中具有分布式结构，便于技术集成、资源共享、性能优化和系统整合。</li>
<li>功能智能性：真体强调理性作用，可作为描述机器智能、动物智能和人类智能的统一模型。</li>
</ul>
<h3 id="自然语言理解"><a href="#自然语言理解" class="headerlink" title="自然语言理解"></a>自然语言理解</h3><blockquote>
<p>Natural Language Understanding 俗称人机对话。研究用电子计算机模拟人的语言交际过程，使计算机能理解和运用人类社会的自然语言如汉语、英语等，实现人机之间的自然语言通信，以代替人的部分脑力劳动，包括查询资料、解答问题、摘录文献、汇编资料以及一切有关自然语言信息的加工处理。<br>这一领域的研究涉及自然语言，即人们日常使用的语言，包括中文、英文、俄文、日文、德文、法文等等，所以它与语言学的研究有着密切的联系，但又有重要的区别。<br>自然语言处理并不是一般地研究自然语言，而在于研制能有效地实现自然语言通信的计算机系统，特别是其中的软件系统。</p>
</blockquote>
<p>实现人机间自然语言通信意味着要使计算机既能理解自然语言文本的意义(即自然语言理解)，也能以自然语言文本来表达给定的意图、思想等(即自然语言生成)。<br>无论实现自然语言理解，还是自然语言生成，都远不如人们原来想象的那么简单，而是十分困难的。从目前的理论和技术现状看，通用的、高质量的自然语言处理系统，仍然是较长期的努力目标，但是针对一定应用，具有相当自然语言处理能力的实用系统已经出现，有些已商品化，甚至开始产业化。典型的例子有：各种数据库和专家系统的自然语言接口、各种机器翻译系统、全文信息检索系统、自动文摘系统等。</p>
<p>自然语言处理，即实现人机间自然语言通信，或实现 <strong>自然语言理解</strong>和 <strong>自然语言生成</strong>是十分困难的。<br>造成困难的 <strong>根本原因</strong>是自然语言文本和对话的各个层次上广泛存在的各种各样的歧义性或多义性（ambiguity）。<br>一般情况下，它们中的大多数都是可以根据相应的语境和场景的规定而得到解决的。也就是说，从总体上说，并不存在歧义。这也就是我们平时并不感到自然语言歧义，和能用自然语言进行正确交流的原因。但是一方面，我们也看到，为了消解歧义，是需要极其大量的知识和进行推理的。如何将这些知识较完整地加以收集和整理出来；又如何找到合适的形式，将它们存入计算机系统中去；以及如何有效地利用它们来消除歧义，都是工作量极大且十分困难的工作。这不是少数人短时期内可以完成的，还有待长期的、系统的工作。</p>
<h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><ul>
<li>一方面，迄今为止的语法都限于分析一个孤立的句子，上下文关系和谈话环境对本句的约束和影响还缺乏系统的研究，因此分析歧义、词语省略、代词所指、同一句话在不同场合或由不同的人说出来所具有的不同含义等问题，尚无明确规律可循。</li>
<li>另一方面，人理解一个句子不是单凭语法，还运用了大量的有关知识，包括生活知识和专门知识，这些知识无法全部贮存在计算机里。因此一个书面理解系统只能建立在有限的词汇、句型和特定的主题范围内。</li>
</ul>
<h3 id="如何看待语言理解"><a href="#如何看待语言理解" class="headerlink" title="如何看待语言理解"></a>如何看待语言理解</h3><ul>
<li>从微观上讲，语言理解是指从自然语言到机器(计算机系统)内部之间的一种映射。</li>
<li>从宏观上看，语言理解是指机器能够执行人类所期望的某些语言功能。这些功能包括：<ul>
<li>回答有关提问；</li>
<li>提取材料摘要；</li>
<li>不同词语叙述；</li>
<li>不同语言翻译。</li>
</ul>
</li>
</ul>
<h3 id="自然语言理解的研究领域和方向"><a href="#自然语言理解的研究领域和方向" class="headerlink" title="自然语言理解的研究领域和方向"></a>自然语言理解的研究领域和方向</h3><p>文字识别、语音识别、机器翻译、自动文摘、句法分析、文本分类、信息检索、信息获取、信息过滤、自然语言生成、中文自动分词、语音合成、问答系统</p>
<p>用自然语言与计算机进行通信，这是人们长期以来所追求的。因为它既有明显的实际意义，同时也有重要的理论意义：人们可以用自己最习惯的语言来使用计算机，而无需再花大量的时间和精力去学习不很自然和习惯的各种计算机语言；人们也可通过它进一步了解人类的语言能力和智能的机制。</p>

        </div>

        <blockquote class="post-copyright">
    <div class="content">
        <p><b>版权声明：</b>
        <a href="https://zhouie.cn/posts/201806111/" rel="external">【复习】 人工智能入门复习总结</a>
        ，由&nbsp;<a href="/about" target="_blank" rel="external">zhouie</a>&nbsp;
        首次发表于&nbsp;<a href="/" target="_blank" rel="external">北岛向南的小屋</a>&nbsp;
        ，本文地址为：<a href="https://zhouie.cn/posts/201806111/" target="_blank" rel="external">https://zhouie.cn/posts/201806111/</a>
        ，转载请注明<b>作者</b>和<b>出处</b>。</p>
    </div>
    <footer>
        <a href="https://zhouie.cn">
            <img src="https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg" alt="zhouie">
            zhouie
        </a>
    </footer>
</blockquote>

        
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;" class="page-reward-btn waves-effect waves-circle waves-light">赏</a>
</div>



        <div class="post-footer">
            
	<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/XTU/">XTU</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&source=北岛向南的小屋" data-title="QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      
      <li>
        <a class="qzone share-sns" target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://zhouie.cn/posts/201806111/&sharesource=qzone&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pics=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&summary=北岛向南的小屋" data-title="QQ空间">
          <img src="https://i.loli.net/2019/03/31/5ca088358a49d.png"/>
        </a>
      </li>
      <li>
        <a class="renren share-sns" target="_blank" href="http://widget.renren.com/dialog/share?resourceUrl=https://zhouie.cn/posts/201806111/&srcUrl=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&description=北岛向南的小屋" data-title="人人网">
          <img src="https://i.loli.net/2019/03/31/5ca0883589571.png"/>
        </a>
      </li>
      <li>
        <a class="youdao share-sns" target="_blank" href="http://note.youdao.com/signIn/index.html?&callback=http://note.youdao.com/memory/?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&sumary=北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&product=北岛向南" data-title="有道云笔记">
          <img src="https://i.loli.net/2019/03/31/5ca088358a321.png"/>
        </a>
      </li>
      <li>
        <a class="douban share-sns" target="_blank" href="https://www.douban.com/share/service?name=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&href=https://zhouie.cn/posts/201806111/&utm_content=note&utm_medium=reader_share&utm_source=douban&image=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&updated=&bm=1&url==https://zhouie.cn/posts/201806111/&utm_content=note&utm_medium=reader_share&utm_source=douban&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&sel=&v=1" data-title="豆瓣">
          <img src="https://i.loli.net/2019/03/31/5ca0ace235d5d.png"/>
        </a>
      </li>

      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zhouie.cn/posts/201806111/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&url=https://zhouie.cn/posts/201806111/&via=https://zhouie.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zhouie.cn/posts/201806111/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/posts/201806171/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">【分享】 图片处理相关工具</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/posts/201806071/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">【收藏】 用Excel设计的RPG游戏</h4>
      </a>
    </div>
  
</nav>



    

<section class="comments" id="comments">
    <div id="disqus_thread"></div>
    <script>
    var disqus_shortname = 'zhouie';
    lazyScripts.push('//' + disqus_shortname + '.disqus.com/embed.js')
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>












</article>

<div id="reward" class="page-modal reward-lay">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <h3 class="reward-title">
        <i class="icon icon-quote-left"></i>
        赠人玫瑰，手有余香
        <i class="icon icon-quote-right"></i>
    </h3>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="https://i.loli.net/2019/02/22/5c6f8f5534b72.jpg" alt="打赏二维码">
        </div>
        
        <label class="reward-toggle">
            <input id="rewardToggle" type="checkbox" class="reward-toggle-check"
                data-wechat="https://i.loli.net/2019/02/22/5c6f8f5534b72.jpg" data-alipay="https://i.loli.net/2019/02/22/5c6f9576d940e.jpg">
            <div class="reward-toggle-ctrol">
                <span class="reward-toggle-item wechat">微信</span>
                <span class="reward-toggle-label"></span>
                <span class="reward-toggle-item alipay">支付宝</span>
            </div>
        </label>
        
    </div>
</div>


</div>

        <footer class="footer">
    <div class="top">
        
<p>	
    <span id="busuanzi_container_site_uv" style='display:none'>
        站点总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        站点总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span>
                    <a href="/atom.xml" target="_blank" class="rss" title="RSS" rel="external nofollow">
                        <i class="icon icon-rss"></i>
                    </a>
                </span>
            
            <span>
                <a href="http://weibo.com/u/5736541528" rel="external nofollow" title="Weibo" target="_blank">
                    <i class="icon icon-weibo"></i>
                </a>
            </span>
            <span>
                <a href="https://github.com/zhouie/zhouie.github.io" rel="external nofollow" title="GitHub" target="_blank" >
                    <i class="icon icon-github"></i>
                </a>
            </span>
            <span>
                <a href="http://www.linkedin.com/in/zhouie/" rel="external nofollow" title="LinkedIn" target="_blank">
                    <i class="icon icon-linkedin"></i>
                </a>
            </span>
            <span>
                <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" title="License" target="_blank">
                    <i class="icon icon-cc"></i>
                </a>
            </span>
        </p>
    </div>
    <div class="bottom">
        <p>
            <span>
                zhouie · Copyright &copy; 2017 -  2019 · Powered by <a href="http://hexo.io/" target="_blank" rel="external nofollow">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank" rel="external nofollow">indigo</a><br>

                
                
                
                <img src="http://zhouie.oss-cn-beijing.aliyuncs.com/zhouie/file/img/1.png" /><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=43030202001139">湘公网安备43030202001139号</a>
                

                
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1273664769'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s22.cnzz.com/z_stat.php%3Fid%3D1273664769%26online%3D2' type='text/javascript'%3E%3C/script%3E"));</script>
                
            </span>
        </p>
    </div>
</footer>
    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&source=北岛向南的小屋" data-title="QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      
      <li>
        <a class="qzone share-sns" target="_blank" href="https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?url=https://zhouie.cn/posts/201806111/&sharesource=qzone&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pics=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&summary=北岛向南的小屋" data-title="QQ空间">
          <img src="https://i.loli.net/2019/03/31/5ca088358a49d.png"/>
        </a>
      </li>
      <li>
        <a class="renren share-sns" target="_blank" href="http://widget.renren.com/dialog/share?resourceUrl=https://zhouie.cn/posts/201806111/&srcUrl=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&description=北岛向南的小屋" data-title="人人网">
          <img src="https://i.loli.net/2019/03/31/5ca0883589571.png"/>
        </a>
      </li>
      <li>
        <a class="youdao share-sns" target="_blank" href="http://note.youdao.com/signIn/index.html?&callback=http://note.youdao.com/memory/?url=https://zhouie.cn/posts/201806111/&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&sumary=北岛向南的小屋&pic=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&product=北岛向南" data-title="有道云笔记">
          <img src="https://i.loli.net/2019/03/31/5ca088358a321.png"/>
        </a>
      </li>
      <li>
        <a class="douban share-sns" target="_blank" href="https://www.douban.com/share/service?name=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&href=https://zhouie.cn/posts/201806111/&utm_content=note&utm_medium=reader_share&utm_source=douban&image=https://i.loli.net/2019/02/22/5c6f8812ce1c0.jpg&updated=&bm=1&url==https://zhouie.cn/posts/201806111/&utm_content=note&utm_medium=reader_share&utm_source=douban&title=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&sel=&v=1" data-title="豆瓣">
          <img src="https://i.loli.net/2019/03/31/5ca0ace235d5d.png"/>
        </a>
      </li>

      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zhouie.cn/posts/201806111/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《【复习】 人工智能入门复习总结》 — 北岛向南的小屋&url=https://zhouie.cn/posts/201806111/&via=https://zhouie.cn" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zhouie.cn/posts/201806111/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://zhouie.cn/posts/201806111/" alt="微信分享二维码">
</div>



    <script src="//cdn.bootcss.com/node-waves/0.7.6/waves.min.js"></script>

<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: true };


</script>

<script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>
<script src="/assets/scripts/hxjzg.min.js?v=1.7.1"></script>
<script src="/assets/scripts/love.min.js?v=1.7.1"></script>

<script src="/assets/scripts/main.min.js?v=1.7.1"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/assets/scripts/search.min.js?v=1.7.1" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</body>
</html>
